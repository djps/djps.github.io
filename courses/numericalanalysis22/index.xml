<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Numerical Analysis 2022 | David Sinden</title>
    <link>https://djps.github.io/courses/numericalanalysis22/</link>
      <atom:link href="https://djps.github.io/courses/numericalanalysis22/index.xml" rel="self" type="application/rss+xml" />
    <description>Numerical Analysis 2022</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 08 Feb 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://djps.github.io/media/logo_hud7cfbe45e4df55bd5c7c181ea654d8f2_56982_300x300_fit_lanczos_3.png</url>
      <title>Numerical Analysis 2022</title>
      <link>https://djps.github.io/courses/numericalanalysis22/</link>
    </image>
    
    <item>
      <title>Principles of Numerical Mathematics</title>
      <link>https://djps.github.io/courses/numericalanalysis22/part-1/principles/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalanalysis22/part-1/principles/</guid>
      <description>&lt;p&gt;Find $x$ such that $F(x,d)=0$ for a set of data, $d$ and $F$, a functional relationship between $x$ and $d$.&lt;/p&gt;
&lt;h2 id=&#34;well-posed-problems&#34;&gt;Well Posed Problems&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Well-Posed Problems&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;A problem is said to be &lt;strong&gt;well-posed&lt;/strong&gt; if&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a solution exists,&lt;/li&gt;
&lt;li&gt;the solution is unique,&lt;/li&gt;
&lt;li&gt;the solution&amp;rsquo;s behaviour changes continuously with the initial conditions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A problem which does not have these properties is said to be &lt;strong&gt;ill-posed&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h2 id=&#34;condition-number&#34;&gt;Condition Number&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Relative and Absolute Condition Numbers&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;The &lt;strong&gt;relative condition number&lt;/strong&gt; of a problem is given by:
\begin{equation}
K ( d ) = \sup\limits_{\delta d \in \mathcal{D}} \dfrac{ \left\Vert \delta x \right\Vert / \left\Vert x \right\Vert}{\left\Vert \delta d \right\Vert / \left\Vert d \right\Vert}
\end{equation}
if either $x=0$ or $d=0$, the &lt;strong&gt;absolute condition number&lt;/strong&gt; is then
\begin{equation}
K_{\mathrm{abs}} ( d ) = \sup\limits_{\delta d  \in \mathcal{D}}\dfrac{ \left\Vert \delta x \right\Vert }{\left\Vert \delta d \right\Vert}
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h2 id=&#34;stability&#34;&gt;Stability&lt;/h2&gt;
&lt;p&gt;Consider a well-posed problem, a construct a sequence of approximate solutions via a sequence of approximate solutions and data, i.e. $F_n (x_n, d_n)=0$&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Consistency&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;If the $d$ is admissible for $F_n$ then a numerical method $F_n (x_n, d_n)=0$ is &lt;strong&gt;consistent&lt;/strong&gt; if
\begin{equation}
\lim\limits_{n\rightarrow\infty} F_n (x,d)  \rightarrow F(x,d).
\end{equation}
The method is strongly consistent if $F_n(x,d)=0$ for all $n\ge0$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Asymptotic and Relative Condition Numbers&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;If the sets of functions for $F_n (x_n, d_n)=0$ and $F(x,d)=0$ coincide, that is
\begin{equation}
K_n \left( d_n \right) = \sup\limits_{\delta d_n \in \mathcal{D}_n }  \dfrac{ \left\Vert \delta x_n \right\Vert /  \left\Vert x_n \right\Vert }{\left\Vert \delta d_n \right\Vert / \left\Vert d_n \right\Vert}
\end{equation}
and&lt;/p&gt;
&lt;p&gt;$$K_{n,\mathrm{abs}} \left( d_n \right) = \sup \limits_{\delta d_n \in \mathcal{D}_n}  \dfrac{ \left\Vert \delta x_n \right\Vert }{ \left\Vert \delta d_n \right\Vert }$$&lt;/p&gt;
&lt;p&gt;then the &lt;strong&gt;asymptotic condition number&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$K^{\mathrm{num}} (d) = \lim \limits_{k\rightarrow \infty} \sup\limits_{n \le k} K_n \left( d_n \right).$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;relative condition number&lt;/strong&gt; is:&lt;/p&gt;
&lt;p&gt;\begin{equation}
K_{\mathrm{abs}}^{\mathrm{num}} \left( d \right) = \lim \limits_{k\rightarrow \infty} \sup \limits_{n \le k} K_{n, \mathrm{abs}} \left( d_n \right).
\end{equation}&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Convergence&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A method is &lt;strong&gt;convergent&lt;/strong&gt; if and only if:
\begin{equation}
\forall \varepsilon &amp;gt; 0, \quad \exists ,  n \quad \mbox{such that} \quad \left\Vert x(d) - x_n\left( d+ \delta d_n \right) \right\Vert \le \varepsilon
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Lax-Ritchmyer&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A numerical algorithm converges if and only if it is consistent and stable.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h2 id=&#34;matrix-analysis&#34;&gt;Matrix Analysis&lt;/h2&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Let $A \in \mathbb{R}^{n \times n}$, then&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\lim\limits_{k \rightarrow \infty}A^k =0 \Leftrightarrow \rho \left( A \right) &amp;lt; 1$. Where $\rho\left(A \right)$ is the largest absolute value of the eigenvalues of $A$.  This is called the &lt;em&gt;spectral radius&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The geometric series, $\sum\limits_{k=0}^{\infty}A^k$ is convergent if and only if $\rho \left( A \right) &amp;lt; 1$. Then in this case, the sum is given by
\begin{equation}
\sum\limits_{k=0}^{\infty}A^k = \left( I - A \right)^{-1}
\end{equation}&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Thus, if $\rho \left( A \right) &amp;lt; 1$, the matrix $I-A$ is invertible and
\begin{equation}
\dfrac{1}{1+\left\Vert A \right\Vert} \le \left\Vert\left( I - A \right)^{-1}\right\Vert \le \dfrac{1}{1-\left\Vert A \right\Vert}
\end{equation}
where $\left\Vert \cdot \right\Vert$ is an &lt;em&gt;induced matrix norm&lt;/em&gt; such that $\left\Vert A \right\Vert &amp;lt;1$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Let $A \in \mathbb{R}^{n \times n}$ be non-singular and let $\delta A \in \mathbb{R}^{n \times n}$ be such that $\left\Vert A^{-1} \right\Vert \bigl\Vert \delta A \bigr\Vert &amp;lt; 1$. Furthermore, if $x \in \mathbb{R}^n$ is a solution to $Ax=b$, where $b \in \mathbb{R}^n$ and $b \neq 0$ and $\delta x$ is such that
\begin{equation}
\left( A + \delta A \right)\left( x + \delta x \right) = b + \delta b
\end{equation}
for a $\delta b \in \mathbb{R}^n$, then
\begin{equation}
\left( A + \delta A \right)\left( x + \delta x \right) \le \dfrac{K(A)}{1- K(A) \left\Vert \delta A \right\Vert_2 / \left\Vert A \right\Vert_2} \left(\dfrac{\left\Vert \delta b \right\Vert_2}{\left\Vert b \right\Vert_2} + \dfrac{\left\Vert \delta A \right\Vert_2}{\left\Vert A \right\Vert_2} \right)
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Let $A \in \mathbb{R}^{n \times n}$ be non-singular and if $x \in \mathbb{R}^n$ is a solution to $Ax=b$, where $b \in \mathbb{R}^n$ and $b \neq 0$ and $\delta x$ is such that
\begin{equation}
A \left( x + \delta x \right) = b + \delta b
\end{equation}
then
\begin{equation}
\dfrac{1}{K(A)} \dfrac{\left\Vert \delta b \right\Vert}{\left\Vert b \right\Vert} \le \dfrac{\left\Vert \delta x \right\Vert}{\left\Vert x \right\Vert} \le K(A) \dfrac{\left\Vert \delta b \right\Vert}{\left\Vert b \right\Vert}
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;For $A \in \mathbb{R}^{n \times n}$ and $b \in \mathbb{R}^{n}$, assume $\left\Vert \delta A \right\Vert \le \gamma \left\Vert A \right\Vert$ and $\left\Vert \delta b \right\Vert \le \gamma \left\Vert b \right\Vert$ for some $\gamma \in \mathbb{R}^{+}$. Then, if $\gamma K(A) &amp;lt;1$, then the following holds
\begin{equation}
\dfrac{\left\Vert x+ \delta x \right\Vert}{\left\Vert x \right\Vert} \le \dfrac{1+\gamma K(A)}{1-\gamma K(A)}
\end{equation}
and
\begin{equation}
\dfrac{\left\Vert \delta x \right\Vert}{\left\Vert x \right\Vert} \le \dfrac{2\gamma K(A)}{1-\gamma K(A)}
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;For $A, C \in \mathbb{R}^{n \times n}$, let $R = AC − I$.  If $\left\Vert R \right\Vert_2 &amp;lt; 1$ and
\begin{equation}
\bigl\Vert A^{-1} \bigr\Vert \le \dfrac{\left\Vert C \right\Vert}{1 - \left\Vert R \right\Vert}
\end{equation}
and
\begin{equation}
\dfrac{\left\Vert R \right\Vert}{\left\Vert A \right\Vert} \le {\left\Vert {C - A^{-1}} \right\Vert} \le \dfrac{\left\Vert C \right\Vert \left\Vert R \right\Vert }{1 - \left\Vert R \right\Vert}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the frame of backwards a priori analysis we can interpret $C$ as being the inverse of $A + \delta A$ (for a suitable unknown $\delta A$). We are thus assuming that
$C(A + \delta A) = I$. This yields
\begin{equation}
\delta A = C^{−1} − A = −(AC − I)C^{−1} = −R C^{−1}
\end{equation}
and, as a consequence, if $\left\Vert R \right\Vert &amp;lt;1$ it turns out that
\begin{equation}
\begin{aligned}
\left\Vert \delta A \right\Vert &amp;amp; \le  \left\Vert  R  \right\Vert  \left\Vert C^{-1} \right\Vert \newline
&amp;amp; \le \dfrac{ \left\Vert R \right\Vert \left\Vert A \right\Vert }{1 - \left\Vert R \right\Vert}
\end{aligned}
\end{equation}&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Iterative Methods</title>
      <link>https://djps.github.io/courses/numericalanalysis22/part-1/iterative-methods/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalanalysis22/part-1/iterative-methods/</guid>
      <description>&lt;p&gt;{{ myReadingTime }}&lt;/p&gt;
&lt;p&gt;Construct a scheme which solves the linear system $Ax=b$ by generating a sequence ${ x^{(n)} }$ which approximates the solution, $x$, that is
\begin{equation}
\lim_{n\rightarrow \infty} x^{(n)} = x. \nonumber
\end{equation}
So that $x = A^{-1} b$. Split the matrix $A=P-N$ and solve
\begin{equation}
P x^{(n+1)} = B x^{(n)} + f, \nonumber
\end{equation}&lt;/p&gt;
&lt;p&gt;where $P$ is called a &lt;em&gt;preconditioner&lt;/em&gt; and $B=P^{-1}N$ is the &lt;em&gt;iteration matrix&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;An equivalent way is to write
\begin{equation}
x^{(k+1)} = x^{(k)} + P^{-1}r^{(k)} \nonumber
\end{equation}
where
\begin{equation}
r^{(k)} = b - A x^{(k)}
\end{equation}
is the &lt;em&gt;residual&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Consistency&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;An iterative method is consistent if $x=Bx +f$, or equivalently,
\begin{equation}
f = (I-B) A^{-1} b
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;If an iterative scheme is &lt;strong&gt;consistent&lt;/strong&gt;, then if and only if $\rho \left( B \right) &amp;lt; 1$ the method will converge for any initial guess $x^{(0)}$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Stationary Methods&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;The formulation can be written as
\begin{equation}
x^{(0)} =  F^{(0)} \left( A, b \right) \quad \mbox{and} \quad x^{(k+1)} = F^{(k+1)} \left( x^{(k)}, x^{(k-1)}, \ldots, x^{(0)}, A, b \right) .
\end{equation}&lt;/p&gt;
&lt;p&gt;If the functions $F^{(k)}$ are independent of the number of iterations, then it is said to be &lt;strong&gt;stationary&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h2 id=&#34;jacobi-method&#34;&gt;Jacobi Method&lt;/h2&gt;
&lt;p&gt;The Jacobi method decomposes the matrix $A$ into diagonal, lower and upper triangular matrices $A=D+L+U$, and solves
\begin{equation}
D x^{(n+1)} = -(L + U) x^{(n)} + b . \label{eq:jacobi_method}
\end{equation}&lt;/p&gt;
&lt;p&gt;Element-wise this is
\begin{equation}
x_i^{(k+1)} = \dfrac{1}{a_{ii}} \left( b_i - \sum\limits_{\substack{j=1,\newline{}j \neq i}}^n a_{ij} x_{j}^{(k)} \right) . \nonumber
\end{equation}&lt;/p&gt;
&lt;p&gt;Thus, the iterative scheme is
\begin{equation}
x^{(n+1)} = -D^{-1}(L + U) x^{(n)} + D^{-1} b. \nonumber
\end{equation}&lt;/p&gt;
&lt;p&gt;As $L+U = A-D$, so the iteration matrix can be written as $B=I - D^{-1}A$.&lt;/p&gt;
&lt;h2 id=&#34;over-relaxation-of-jacobi-method&#34;&gt;Over-Relaxation of Jacobi Method&lt;/h2&gt;
&lt;p&gt;Also called the weighted Jacobi method. Introduce $\omega$ to solve
\begin{equation}
x_i^{(k+1)} = \dfrac{ \omega }{ a_{ii} } \left( b_i - \sum\limits_{\substack{j=1,\newline{}j \neq i}}^n a_{ij} x_{j}^{(k)} \right) + \left( 1 - \omega \right) x^{(k)}. \label{eq:JOR}
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;successive-over-relaxation&#34;&gt;Successive Over-Relaxation&lt;/h2&gt;
&lt;p&gt;Introduce $\omega$ to solve
\begin{equation}
\left(D + \omega L \right) x^{(n+1)} = -( (\omega-1)D + \omega U) x^{(n)} + \omega b. \label{eq:SOR}
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;gauss-seidel&#34;&gt;Gauss-Seidel&lt;/h2&gt;
&lt;p&gt;The Gauss-Seidel method decomposes the matrix $A$ into diagonal, lower and upper triangular matrices $A=D+L+U$, and solves
\begin{equation}
(D + L) x^{(n+1)} = -U x^{(n)} + b \label{eq:GS}
\end{equation}&lt;/p&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;If $A$ is strictly diagonally dominant by rows, i.e. $|a_{ii}| &amp;gt; \sum_{j \ne i} |a_{ij}|$, the &lt;a href=&#34;#jacobi-method&#34;&gt;Jacobi&lt;/a&gt; and &lt;a href=&#34;#gauss-seidel&#34;&gt;Gauss-Seidel&lt;/a&gt; methods are convergent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If $A$ and $2D-A$ are symmetric and positive definite, then the &lt;a href=&#34;#jacobi-method&#34;&gt;Jacobi method&lt;/a&gt; is convergent and the spectral radius of the iteration matrix $B$ is equal to
\begin{equation}
\rho \left( B \right) = \left\Vert B \right\Vert_{A} = \left\Vert B \right\Vert_{D} \nonumber
\end{equation}
where $\left\Vert \cdot \right\Vert_{A}$ is an &lt;em&gt;energy norm&lt;/em&gt; which is induced by the vector norm $\left| x \right|_{A} = \sqrt{ x \cdot A x}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If and only if $A$ is symmetric and positive definite, the &lt;a href=&#34;#over-relaxation-of-jacobi-method&#34;&gt;Jacobi over-relaxation method&lt;/a&gt; is convergent if
\begin{equation}
0 &amp;lt; \omega &amp;lt; \dfrac{2}{\rho\left( D^{-1}A \right) }. \nonumber
\end{equation}&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If and only if $A$ is symmetric and positive definite, the &lt;a href=&#34;#gauss-seidel&#34;&gt;Gauss-Seidel&lt;/a&gt; method is monotonically convergent with respect to the energy norm $\left\Vert \cdot \right\Vert_{A}$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;For any $\omega \in \mathbb{R}$ we have $\rho \left( B \left( \omega \right) \right) \ge |\omega - 1|$. Thus, &lt;a href=&#34;#successive-over-relaxation&#34;&gt;SOR&lt;/a&gt; does not converge if either $\omega \le 0$ or $\omega \ge 2$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Ostrowski&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;IF $A$ is symmetric and positive definite, then the &lt;a href=&#34;#successive-over-relaxation&#34;&gt;SOR&lt;/a&gt; method is convergent if and only if  $0 &amp;lt; \omega &amp;lt; 2$. Furthermore, the convergence is monotonic with respect to the energy norm $\left\Vert \cdot \right\Vert_{A}$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h2 id=&#34;gradient-descent&#34;&gt;Gradient Descent&lt;/h2&gt;
&lt;p&gt;Consider the function $\Phi \left( y \right) , : , \mathbb{R}^{n} \mapsto \mathbb{R}$ which takes the form:
\begin{equation}
\Phi \left( y\right) = \dfrac{1}{2} y \cdot A y - y \cdot b.
\end{equation}
It can be shown that solving $Ax=b$ is equivalent to minimizing $\Phi$.&lt;/p&gt;
&lt;p&gt;If $x$ is a solution to the linear system and minimizes $\Phi(x)$ then $\nabla \Phi(x) = 0$, so that $Ax-b = \nabla \Phi(x) =0$.&lt;/p&gt;
&lt;p&gt;Now
\begin{align}
\Phi(y) &amp;amp; = \Phi(x + (y-x) ) \newline
&amp;amp; = \Phi(x) + \dfrac{1}{2} \left\Vert y - x\right\Vert_{A}^{2}.
\end{align}&lt;/p&gt;
&lt;p&gt;Gradient descent seeks to construct a scheme which updates the vector $x^{(k)}$ according to
\begin{equation}
x^{(k+1)} = x^{(k)} + \alpha^{(k)} d^{(k)}
\end{equation}
where $d^{(k)}$ is the update direction and $\alpha^{(k)}$ is the step size at the $k$-th iterate.&lt;/p&gt;
&lt;p&gt;Note that in contrast to the methods above, the gradient descent method is non-stationary as values $d$ and $\alpha$ change at every iterate.&lt;/p&gt;
&lt;p&gt;The idea is to let the search direction be the gradient of the function $\Phi$
\begin{align}
d^{(k)} &amp;amp; = -\nabla \Phi \left( x^{(k)} \right) \newline
&amp;amp; = - \left( A x^{(k)} - b \right) \newline
&amp;amp; = b - A x^{(k)} \newline
&amp;amp; = r^{(k)}
\end{align}
The ideal step size is found by differentiating $\Phi$ with respect to $\alpha$ and setting this to zero, so that
\begin{equation}
\alpha^{(k)} = \dfrac{ r^{(k)} \cdot r^{(k)} }{ r^{(k)} \cdot A r^{(k)} }
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;If $A$ is symmetric and positive definite, then the &lt;a href=&#34;#gradient-descent&#34;&gt;gradient method&lt;/a&gt; method is convergent for any $x^{(0)}$ and
\begin{equation}
\left\Vert e^{(k+1)} \right\Vert_A \le \dfrac{K(A)-1}{K(A)+1} \left\Vert e^{(k)} \right\Vert_A.
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
If we apply a preconditioner, i.e. multiplying both sides of the linear system from the left by $P^{-1}$,&lt;/p&gt;
&lt;h2 id=&#34;conjugate-gradient&#34;&gt;Conjugate Gradient&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Conjugate Vectors&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;If $A$ is symmetric and positive definite, let the vectors $u$ and $v$ be &lt;strong&gt;$A$-orthogonal&lt;/strong&gt; or &lt;strong&gt;conjugate&lt;/strong&gt; if $u \cdot Av=0$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Lemma open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Lemma:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Choosing $p^{(k+1)}$ such that
\begin{equation}
p^{(k+1)} \cdot A p^{(j)} = 0
\end{equation}
for $j=0, \ldots, k$ leads to
\begin{equation}
p^{(j)} \cdot r^{(k+1)} = 0.
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Lemma open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Lemma:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Setting
\begin{equation}
\beta^{(k)} = \dfrac{r^{(k+1)} \cdot A p^{(k)} }{ p^{(k)} \cdot A p^{(k)} }
\end{equation}
and
\begin{equation}
p^{(k+1)} = r^{(k+1)} - \beta^{(k)} p^{(k)}
\end{equation}
then, for $j=0, \ldots, k,$ yields
\begin{equation}
p^{(k+1)} \cdot A p^{(j)} = 0.
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;If $A \in \mathbb{R}^{n \times n}$ is a symmetric and positive definite matrix, and $b \in \mathbb{R}^{n}$, then the &lt;a href=&#34;#conjugate-gradient&#34;&gt;conjugate gradient method&lt;/a&gt; yields the exact solution of $Ax=b$ after $n$ steps.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Interpolation</title>
      <link>https://djps.github.io/courses/numericalanalysis22/part-2/interpolation/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalanalysis22/part-2/interpolation/</guid>
      <description>&lt;p&gt;Numerical treatment of problems often involves the process of &lt;em&gt;discretization&lt;/em&gt; - i.e. going from a continuous function to set of discrete points.&lt;/p&gt;
&lt;div class=&#34;details quotemonition MyQuote open&#34;&gt;
        &lt;div class=&#34;details-summary quotemonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-quote-left fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;quotemonition-content&#34;&gt;Interpolation provides a way of approximating continuous functions by discrete data.&lt;/div&gt;
        &lt;/div&gt;

        &lt;div class=&#34;details-summary quotemonition-title&#34; align=&#34;right&#34;&gt;
            &lt;i class=&#34;icon fas fa-quote-right fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			
        &lt;/div&gt;

    &lt;/div&gt;
&lt;p&gt;Types of functions which can be used are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Polynomial interpolation&lt;/strong&gt; : using a polynomial to approximate the data,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Trigonometric interpolation&lt;/strong&gt;: using polynomials of trigonometric functions,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spline interpolation&lt;/strong&gt;: using a set of piecewise polynomials over subintervals of the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Given $n+1$ distinct points $x_0, x_1, \ldots, x_n$ and $n+1$ corresponding values $y_0, y_1, \ldots, y_n$ there exists a &lt;em&gt;unique&lt;/em&gt; polynomial $\Pi_n \in \mathbb{P}_n$ such that for all $i=0, \ldots, n$
\begin{equation}
\Pi_n\left( x_i \right) = y_i.
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h2 id=&#34;lagrange-interpolation&#34;&gt;Lagrange Interpolation&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Lagrange Polynomials&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;The &lt;strong&gt;Lagrange form of an interpolating polynomial&lt;/strong&gt; is given by
\begin{equation}
\Pi_n \left( x\right) = \sum\limits_{i=0}^{n} y_i l_i\left(x\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;where  $l_{i} \in \mathbb{P}_{n}$
such that $l_{i}\left( x_{j} \right) = \delta_{ij}$. The polynomials $l_i\left(x\right) \in \mathbb{P}_n$ for $i=0, \ldots, n$, are called &lt;strong&gt;characteristic polynomials&lt;/strong&gt; and are given by
\begin{equation}
l_{i} \left( x \right) = \prod \limits_{\substack{j = 0,\newline{}j \ne i}}^{n} \dfrac{x - x_j}{x_i - x_j}. &lt;br&gt;
\end{equation}&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition example open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ol fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			example:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;An &lt;code&gt;.ipynb&lt;/code&gt; notebook illustrating the Lagrange polynomials can be accessed &lt;a href=&#34;https://djps.github.io/courses/numericalanalysis22/notebooks/characteristicpolynomial&#34;&gt;here&lt;/a&gt;. It can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/characteristicPolynomial.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;An &lt;code&gt;.ipynb&lt;/code&gt; notebook illustrating Runge phenomenon can be accessed &lt;a href=&#34;https://djps.github.io/courses/numericalanalysis22/notebooks/lagrangeinterpolation&#34;&gt;here&lt;/a&gt;. It can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/LagrangeInterpolation.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Let $x_0, x_1, \ldots x_n$ be $n+1$ distinct nodes and let $x$ be a point belonging to the domain of a given function $f$.&lt;/p&gt;
&lt;p&gt;Let $I_x$ be the smallest interval containing the nodes $x_0, x_1, \ldots x_n$ and $x$ and assume that $f \in C^{n+1}\left( I_x \right)$. Then the interpolation error at the point $x$ is defined and given by
\begin{equation}
\begin{aligned}
E_n(x) &amp;amp;= f(x) − \Pi_n f (x) \newline
&amp;amp;= \dfrac{f^{(n+1)}( \xi ) }{ (n + 1)!} \omega_{n+1}(x)
\end{aligned}
\tag{1} \label{eq:basic_interpolation_error}
\end{equation}&lt;/p&gt;
&lt;p&gt;where $\xi \in I_x$ and $\omega_{n+1}$ is the nodal polynomial of degree $n + 1$, which is defined as
\begin{equation}
\omega_{n+1}(x) = \prod\limits_{i=0}^{n} \left(x - x_i \right).
\end{equation}&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h2 id=&#34;piecewise-lagrange-interpolation&#34;&gt;Piecewise Lagrange Interpolation&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;$\mathrm{L}^2$ Space&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Define the following space
\begin{equation}
\mathrm{L}^{2}(a, b) = \bigl \lbrace f : (a, b) \rightarrow \mathbb{R}, \int_{a}^{b} | f(x) |^{2} , \mathrm{d}x &amp;lt; \infty \bigr \rbrace
\end{equation}&lt;/p&gt;
&lt;p&gt;with&lt;/p&gt;
&lt;p&gt;\begin{equation}
| f |^{}_{\mathrm{L}^{2}(a, b)} = \left( \int_{a}^{b} | f(x) |^{2} \mathrm{d} x \right)^{1 / 2}.
\end{equation}&lt;/p&gt;
&lt;p&gt;This defines a &lt;em&gt;norm&lt;/em&gt; for $\mathrm{L}^{2}(a, b)$.  Note that integral of the function $|f|^{2}$ is in the Lebesgue sense - in particular, $f$ needs not be continuous everywhere.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;p&gt;Partition $\mathcal{T_h}$ of $[a, b]$ into $K$ subintervals $I_j = \left[ x_j , x_{j+1} \right]$ of length $h_j$ such that $[a, b] = \bigcup\limits_{j=0}^{K−1}I_j$. Let ${h = \max\limits_{0 \le j \le K−1} h_j}$.&lt;/p&gt;
&lt;p&gt;For $k \geq 1$, introduce on $\mathcal{T}_h$ the piecewise polynomial space&lt;/p&gt;
&lt;p&gt;\begin{equation}
X_h^k = \bigl\lbrace v \in C^0([a, b]) \, : \,   v |_{I_j} \in \mathbb{P}_k \left( I_j \right), \quad \forall \, I_j \in \mathcal{T}_h \bigr\rbrace \tag{2} \label{eq:piecewise}
\end{equation}&lt;/p&gt;
&lt;p&gt;which is the space of the continuous functions over the interval $[a, b]$ whose restrictions on each $I_j$ are polynomials of degree less than or equal to $k$.&lt;/p&gt;
&lt;p&gt;Then, for any continuous function $f$ in $[a, b]$, the piecewise interpolation polynomial $\Pi^k_h f$
coincides on each $I_j$ with the interpolating polynomial of $\left. f \right|_{I_j}$ at the $n + 1$ nodes
$\lbrace x_j^{(i)}, 0 \leq i \leq n \rbrace$.&lt;/p&gt;
&lt;p&gt;As a consequence, if $f \in C^{k+1}([a, b])$, then from \eqref{eq:basic_interpolation_error} within
each interval the following error estimate holds
\begin{equation}
\left\Vert f − \Pi_h^k f \right\Vert_\infty \leq C h^{k+1} \left\Vert f^{(k+1} \right\Vert_\infty .
\end{equation}&lt;/p&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Partition $\mathcal{T_h}$ of $[a, b]$ into $K$ subintervals
$I_j = [x_j , x_{j+1}]$ of length $h_j$, with $h = \max\limits_{0 \le j \le K−1} h_j$, such that $[a, b] = \bigcup\limits_{j=0}^{K−1}I_j$. Now using Lagrange interpolation on each subinterval $I_j$ using $n + 1$ equally spaced nodes
$\lbrace{ x^{(i)}_{j} , 0 \le i \le n \rbrace}$
with a small $n$. Then $\Pi_n^k$ is the &lt;em&gt;piecewise interpolation polynomial&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Let $0 \leq m \leq k+1$, with $k \geq 1$ and assume that $f^{(m)} \in$ $\mathrm{L}^{2}(a, b)$ for $0 \leq m \leq k+1$ then there exists a positive constant $C$, independent of $h$, such that&lt;/p&gt;
&lt;p&gt;\begin{equation}
\left| \left( f - \Pi_{h}^{k} f\right)^{(m)} \right|_{\mathrm{L}^{2}(a, b)} \leq C h^{k+1-m} \left| f^{(k+1)} \right|_{\mathrm{L}^{2}(a, b)} .
\end{equation}&lt;/p&gt;
&lt;p&gt;In particular, for $k=1$ and $m=0$, or $m=1$&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{aligned}
\left| f-\Pi_{h}^{1} f \right|_{\mathrm{L}^{2}(a, b)} &amp;amp; \leq C_{1} h^{2} \left| f^{\prime \prime} \right|_{\mathrm{L}^{2}(a, b)} \newline
\left| \left( f - \Pi_{h}^{1} f \right)^{\prime}\right|_{\mathrm{L}^{2}(a, b)} &amp;amp; \leq C_{2} h\left| f^{\prime \prime} \right|_{\mathrm{L}^{2}(a, b)}&lt;br&gt;
\end{aligned}
\end{equation}&lt;/p&gt;
&lt;p&gt;for two suitable positive constants $C_{1}$ and $C_{2}$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Integration</title>
      <link>https://djps.github.io/courses/numericalanalysis22/part-2/integration/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalanalysis22/part-2/integration/</guid>
      <description>&lt;p&gt;If $f \in C^{0} \left([a, b]\right)$, the quadrature error $E_{n}(f) = I(f) - I_{n}(f)$ satisfies
\begin{equation}
\left| E_{n}(f) \right| \leq \int_{a}^{b} \left| f(x) - f_{n}(x) \right| \mathrm{d} x \leq (b-a) \left| f - f_{n} \right|_{\infty}
\end{equation}&lt;/p&gt;
&lt;p&gt;Therefore, if for some $n$,  $\left\| f - f_{n}\right\|_{\infty} &lt; \varepsilon$ , then $\left|E_{n}(f)\right| \leq \varepsilon(b-a)$.&lt;/p&gt;
&lt;p&gt;The approximation of the function $f_{n}$ must be easily integrable, which is the case if, for example, $f_{n} \in \mathbb{P}_{n}$.&lt;/p&gt;
&lt;p&gt;In this respect, a natural approach consists of using $f_{n}=\Pi_{n} f$, the interpolating &lt;a href=&#34;https://djps.github.io/courses/numericalanalysis22/part-2/interpolation/#lagrange-interpolation&#34;&gt;Lagrange polynomial&lt;/a&gt; of $f$ over a set of $n+1$ distinct nodes $\lbrace x_{i} \rbrace$, with $i=0, \ldots, n$. It follows that the approximation to the integral is
\begin{equation}
I_{n}(f) = \sum_{i=0}^{n} f \left( x_{i} \right) \int_{a}^{b} l_{i}(x) \mathrm{d} x
\end{equation}&lt;/p&gt;
&lt;p&gt;where $l_{i}$ is the characteristic &lt;a href=&#34;https://djps.github.io/courses/numericalanalysis22/part-2/interpolation/#lagrange-interpolation&#34;&gt;Lagrange polynomial&lt;/a&gt; of degree $n$ associated with node $x_{i}$. It is called the &lt;strong&gt;Lagrange quadrature formula&lt;/strong&gt;, and is a special instance of the following, generalised, quadrature formula
\begin{equation}
I_{n}(f) = \sum \limits_{i=0}^{n} \alpha_{i} f\left( x_{i} \right)
\end{equation}&lt;/p&gt;
&lt;p&gt;where the coefficients $\alpha_{i}$ of the linear combination are given by $\int_{a}^{b} l_{i} \left( x \right) \mathrm{d} x$. The above equation is a weighted sum of the values of $f$ at the points $x_{i}$, for $i=0, \ldots, n$. These points are said to be the nodes of the quadrature formula, while the  $\alpha_{i} \in \mathbb{R}$ are its &lt;em&gt;coefficients&lt;/em&gt; or &lt;em&gt;weights&lt;/em&gt;. Both weights and nodes depend in general on $n$.&lt;/p&gt;
&lt;p&gt;Another approximation of the function $f$ leads to the &lt;strong&gt;Hermite quadrature formula&lt;/strong&gt;
\begin{equation}
I_{n}(f)=\sum_{k=0}^{1} \sum_{i=0}^{n} \alpha_{i k} f^{(k)}\left(x_{i}\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;where the weights are now denoted by $\alpha_{i k}$. This depends on an evaluation of the function and its derivative.&lt;/p&gt;
&lt;p&gt;Both the above are &lt;em&gt;interpolatory quadrature formula&lt;/em&gt;, since the function $f$ has been replaced by its interpolating polynomial (Lagrange and Hermite polynomials, respectively).&lt;/p&gt;
&lt;p&gt;Define the &lt;strong&gt;degree of exactness&lt;/strong&gt; of a quadrature formula as the maximum integer $r \geq 0$ for which
\begin{equation}
I_{n}(f)=I(f), \quad \forall f \in \mathbb{P}_{r} .
\end{equation}&lt;/p&gt;
&lt;p&gt;Any interpolatory quadrature formula that makes use of $n+1$ distinct nodes has degree of exactness equal to at least $n$. Indeed, if $f \in \mathbb{P}_{n}$, then $\Pi_n f = f$ and thus $I_n\left( \Pi_n f \right) = I \left( \Pi_n f \right)$.&lt;/p&gt;
&lt;!-- The converse statement is also true, that is, a quadrature formula using $n + 1$ distinct nodes and having degree of exactness equal at least to $n$ is necessarily of interpolatory type. --&gt;
&lt;h2 id=&#34;midpoint-rule&#34;&gt;Midpoint Rule&lt;/h2&gt;
&lt;p&gt;The zero-th order approximation is given by
\begin{equation}
I_0 = (b-a) f \left( \dfrac{a+b}{2} \right).
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;trapezoidal-rule&#34;&gt;Trapezoidal Rule&lt;/h2&gt;
&lt;p&gt;The trapezoidal rule is given by
\begin{equation}
I_1 = \dfrac{b-a}{2} \left( f \left( a \right) + f \left(b\right) \right).
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;simpsons-rule&#34;&gt;Simpson&amp;rsquo;s Rule&lt;/h2&gt;
&lt;p&gt;Simpson&amp;rsquo;s rule is
\begin{equation}
I_2 = \dfrac{b-a}{6} \left( f \left( a \right) + 4f\left(\dfrac{a+b}{2} \right)+ f \left(b\right) \right).
\end{equation}&lt;/p&gt;
&lt;!-- !!! warning &#34;Example:&#34; --&gt;
&lt;div class=&#34;details admonition warning open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-exclamation-triangle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			warning:	&lt;em&gt;Examples&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;An &lt;code&gt;.ipynb&lt;/code&gt; notebook, detailing examples of the &lt;a href=&#34;#midpoint-rule&#34;&gt;Midpoint rule&lt;/a&gt;, &lt;a href=&#34;#trapezoidal-rule&#34;&gt;Trapezoidal rule&lt;/a&gt;, &lt;a href=&#34;#simpsons-rule&#34;&gt;Simpson’s rule&lt;/a&gt; can be accessed &lt;a href=&#34;https://djps.github.io/courses/numericalanalysis22/notebooks/integrals/&#34;&gt;here&lt;/a&gt;.  It can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/integrals.py&#34;&gt;here&lt;/a&gt; as a python file or as ipyth It can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/integrals.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;!-- Changed links to pages for notebook --&gt;
&lt;h2 id=&#34;gaussian-integration&#34;&gt;Gaussian Integration&lt;/h2&gt;
&lt;p&gt;Gaussian quadrature integrates a function by a suitable choice of &lt;em&gt;nodes&lt;/em&gt; and &lt;em&gt;weights&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;With the exact integral of $f$
\begin{equation}
I_g (f) = \int\limits_{−1}^{1} f (x) g(x) , \mathrm{d}x,
\end{equation}
being $f \in C^0 \left( [−1, 1] \right)$, consider quadrature rules of the type
\begin{equation}
I_{n,g} (f) = \sum\limits_{i=0}^{n} \alpha_i f(x_i)
\end{equation}
where $\alpha_i$ are to be determined.&lt;/p&gt;
&lt;p&gt;For a given $m &amp;gt; 0$, the quadrature $I_{n,g}$ has degree of exactness $d=n + m$ if and only if it is of interpolatory type and the nodal polynomial $\omega_{n+1}$ associated with the set of nodes $\lbrace x_i \rbrace$, is such that
\begin{equation}
\int_{-1}^{1} \omega_{n+1}(x) p(x) g(x) , \mathrm{d}x = 0, \quad \forall , p \in \mathbb{P}_{m-1}.
\end{equation}&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Finite Difference Methods</title>
      <link>https://djps.github.io/courses/numericalanalysis22/part-3/fdm/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalanalysis22/part-3/fdm/</guid>
      <description>&lt;h2 id=&#34;greens-functions&#34;&gt;Green&amp;rsquo;s functions&lt;/h2&gt;
&lt;p&gt;For a linear differential operator acting on $u$, that is $\mathcal{L} \left[ u \left( x \right) \right]$, which has a differential equation of the form
\begin{equation}
\mathcal{L}\left[ u \left( x \right) \right] = f \left(x \right),
\end{equation}&lt;/p&gt;
&lt;p&gt;then the &lt;strong&gt;Green&amp;rsquo;s function&lt;/strong&gt; for the operator $\mathcal{L}$, denoted by $G\left( x,s \right)$, can be used to solved the differential equation as
\begin{equation}
u(x) = \int^x G\left(x,s \right) f\left( s \right) \, \mathrm{d} s.
\end{equation}&lt;/p&gt;
&lt;!--
## Properties

Typically the Green&#39;s function has the following properties

1. Continuous
2. Symmetric, i.e. $G(s,x) = G(x,s)$
3. Piecewise linear for a fixed $s$ or $x$
4. Non-negative

!!! info &#34;Definition: _Discrete Maximum Principle_&#34;

    Something here.


!!! info &#34;Definition: _Monotonicity Principle_&#34;

    Something.

--&gt;
&lt;h2 id=&#34;finite-difference-methods&#34;&gt;Finite Difference Methods&lt;/h2&gt;
&lt;div class=&#34;details quotemonition MyQuote open&#34;&gt;
        &lt;div class=&#34;details-summary quotemonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-quote-left fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;quotemonition-content&#34;&gt;First discretize the domain and then approximate the governing equation to produce a linear system.&lt;/div&gt;
        &lt;/div&gt;

        &lt;div class=&#34;details-summary quotemonition-title&#34; align=&#34;right&#34;&gt;
            &lt;i class=&#34;icon fas fa-quote-right fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			
        &lt;/div&gt;

    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Finite-Difference Quotients&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Consider the approximations to the first-order derivative:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Forward Difference Quotient:&lt;/strong&gt;
\begin{equation}
D_{j}^{+} u = \dfrac{u_{j+1} - u_j}{h}
\end{equation}&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Backwards Difference Quotient:&lt;/strong&gt;
\begin{equation}
D_{j}^{-} u = \dfrac{u_{j} - u_{j-1}}{h}
\end{equation}&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Central Difference Quotient:&lt;/strong&gt;
\begin{equation}
D_{j}^{0} u = \dfrac{u_{j+1} - u_{j-1}}{2h}
\end{equation}&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With these, approximations to second-order derivatives can be constructed, for example:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{aligned}
D_{j}^{\pm} u  &amp;amp; = \dfrac{ D_j^{+} u - D_j^{-} u }{h} \newline
&amp;amp; = \dfrac{ \dfrac{u_{j+1} - u_j}{h} - \dfrac{u_j - u_{j-1}}{h} }{h} \newline
&amp;amp; = \dfrac{ u_{j+1} - 2 u_j + u_{j-1}  }{h^2}.
\end{aligned}
\end{equation}&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Errors for Finite-Difference Quotients&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;The errors for the approximation of the derivatives are given by&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$u\left( x_j \right) - D_{j}^{+}u = -\dfrac{h}{2} u^{\prime\prime}\left( \xi\right)$ where $\xi \in \left( x_j, x_{j+1} \right)$&lt;/li&gt;
&lt;li&gt;$u\left( x_j \right) - D_{j}^{-}u = \dfrac{h}{2} u^{\prime\prime}\left( \xi\right)$ where $\xi \in \left( x_{j-1}, x_{j} \right)$&lt;/li&gt;
&lt;li&gt;$u\left( x_j \right) - D_{j}^{0}u = -\dfrac{h^2}{6} u^{\prime\prime\prime}\left( \xi\right)$ where $\xi \in \left( x_{j-1}, x_{j+1} \right)$&lt;/li&gt;
&lt;li&gt;$u\left( x_j \right) - D_{j}^{\pm}u = -\dfrac{h^2}{24}\left( u^{(4)}\left( \xi_1\right) + u^{(4)}\left( \xi_2\right) \right)$ where $\xi_1 \in \left( x_{j-1}, x_{j} \right)$ and $\xi_2 \in \left( x_{j}, x_{j+1} \right)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;stability-analysis&#34;&gt;Stability Analysis&lt;/h3&gt;
&lt;p&gt;Let $V_h$ be the set of discrete functions defined on the nodal points $x_j$ and $V_h^0 \subset V_h$ contain the discrete functions $v_h \in V_h$ which vanish at $x_0$ and $x_n$, i.e. $v_0 =0$ and $v_n=0$.&lt;/p&gt;
&lt;div class=&#34;details admonition Lemma open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Lemma:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Let $\mathcal{L}_h$ be the discretization of a linear differential operator which acts on $u_h \in V_h$, i.e. $\mathcal{L}_h \left[ u_h \right]$.  If the &lt;strong&gt;discrete inner product&lt;/strong&gt; for both $v_h$ and $w_h \in V_h$ is defined as&lt;/p&gt;
&lt;p&gt;\begin{align}
\left( v_h, w_h \right)_h^{} &amp;amp; = h \sum\limits_{j=0}^{n} c_j v_j w_j
\end{align}&lt;/p&gt;
&lt;p&gt;where &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; $c_j = 1$ for $j=1, \ldots n-1$ and $c_0 = c_n = \frac{1}{2}$ and a &lt;strong&gt;norm&lt;/strong&gt; is defined as&lt;/p&gt;
&lt;p&gt;\begin{equation}
\left\Vert v_h \right\Vert_h = \sqrt{ \left( v_h, v_h \right)_h }
\end{equation}&lt;/p&gt;
&lt;p&gt;for a $v_h \in V_h$.&lt;/p&gt;
&lt;p&gt;Then the operator $\mathcal{L}_h$ is &lt;strong&gt;symmetric&lt;/strong&gt;, i.e.
\begin{equation}
\left( \mathcal{L}_h \left[ v_h \right], w_h \right)_h = \left( v_h, \mathcal{L}_h \left[ w_h \right]\right)_h \quad \forall \, w_h , \, v_h \in V^0_h
\end{equation}&lt;/p&gt;
&lt;p&gt;and &lt;strong&gt;positive definite&lt;/strong&gt;, that is
\begin{equation}
\left( \mathcal{L}_h \left[ v_h \right], v_h \right)_h \ge 0 \quad \forall \,  v_h \in V^0_h
\end{equation}&lt;/p&gt;
&lt;p&gt;and
\begin{equation}
\left( \mathcal{L}_h \left[ v_h \right], v_h \right)_h = 0 \Longleftrightarrow v_h = 0 .
\end{equation}&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;This is just the &lt;a href=&#34;https://djps.github.io/courses/numericalanalysis22/part-2/integration/#trapezoidal-rule&#34;&gt;composite trapezium rule&lt;/a&gt;, so that the discrete inner product is the discrete analogue to
\begin{equation}
\left( w, v \right) = \int w(x) v(x) \, \mathrm{d}x
\end{equation}
i.e. it approximates an integral.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Lemma open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Lemma:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;For any $v_h \in V_h$
\begin{equation}
\left\Vert v_h \right\Vert_h \le \dfrac{1}{\sqrt{2}} \Bigg( h \sum \limits_{j=0}^{n-1} \left( \dfrac{v_{j+1} - v_j}{h} \right)^2 \Bigg)^{1/2}.
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;convergence&#34;&gt;Convergence&lt;/h3&gt;
&lt;p&gt;The finite difference solution $u_h$ can be characterised by a discrete Green&amp;rsquo;s function. Define $G^k\left(x\right)  \in V_h^0$ such that
\begin{equation}
\mathcal{L}_h \left[ G^k\left(x\right) \right] = e^k\left(x\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;where $e^k \in V_h^0$ satisfies $e^k\left( x_j \right) = \delta_{kj}$. Then
\begin{equation}
G^k \left( x_j \right) = h G\left( x_j, x_k \right) .
\end{equation}&lt;/p&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Let $\left\Vert v_h	\right\Vert_{h,\infty} = \max \limits_{0 \le j \le n}\left| v_h\left( x_j \right) \right|$ be the &lt;em&gt;discrete maximum norm&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Assume that $f \in C^2 \left( \left[ 0,1 \right] \right)$, then the nodal error, given by $e\left( x_j \right) = u\left(x_j\right) - u_h\left(x_j\right)$ satisfies:
\begin{equation}
\left\Vert u - u_h \right\Vert_{h,\infty} \le \dfrac{h^2}{96}  \left\Vert f^{\prime\prime} \right\Vert_\infty .
\end{equation}&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h2 id=&#34;galerkin-method&#34;&gt;Galerkin Method&lt;/h2&gt;
&lt;p&gt;Consider the elementary problem:
\begin{equation}
\left( \alpha u^{\prime} \right)^{\prime} + \beta u^{\prime} + \gamma u = f\left( x \right) \quad \mbox{on} \quad (0,1) \quad \mbox{with}  \quad u(0) = u(1)=0
\end{equation}&lt;/p&gt;
&lt;p&gt;where $\alpha$, $\beta$, $\gamma \in C^0 \left( \left[ 0, 1 \right] \right)$ and $\alpha(x) \ge \alpha_0 &amp;gt;0$ for all $x \in  \left[ 0, 1 \right]$.&lt;/p&gt;
&lt;p&gt;Next, on $L^2$, define the &lt;em&gt;scalar product&lt;/em&gt;
\begin{equation}
\left( f, v \right) = \int \limits_0^1 f v \, \mathrm{d}x
\end{equation}&lt;/p&gt;
&lt;p&gt;and a &lt;em&gt;bilinear form&lt;/em&gt; $a : \left( \cdot, \cdot \right)$ which maps $H_0^1 \times H^1_0 \rightarrow \mathbb{R}$
\begin{equation}
a\left( u, v \right) = \int \limits_0^1 \left( \alpha u^\prime v^\prime + \beta u^\prime v + \gamma u v \right) \, \mathrm{d}x
\end{equation}&lt;/p&gt;
&lt;p&gt;and consider the &lt;strong&gt;weak form&lt;/strong&gt;
\begin{equation}
\mbox{Find} \quad u \in H^1_0 \quad \mbox{such that} \quad a\left(u,v \right) =\left( f, v\right) \quad \forall \, v \in H^1_0 \left(0, 1 \right).
\end{equation}&lt;/p&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;ol&gt;
&lt;li&gt;Let $u$ be a $C^2$ be a solution of the elementary problem, then $u \in H^1_0$ also solves the weak form problem.&lt;/li&gt;
&lt;li&gt;Let $u \in H^1_0$ be a solution of the weak problem. If and only if $u \in C^2 \left( [ 0, 1 ] \right)$ then $u$ also solves the elementary problem.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Fundamental Theorem of the Calculus of Variations&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Suppose that $f(x)$ is integrable on $(0,1)$ and
\begin{equation}
\int \limits_0^1 \phi(x) f(x) \, \mathrm{d}x = 0 \quad \forall \, \phi(x) \in C^\infty_0\left(\left[ 0,1 \right]\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;then $f(x)=0$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Poincaré-Friedrich Inequality&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Let $\Omega \subset \mathbb{R}^n$ be contained in $n$-dimensional cube of length $s$, then
\begin{equation}
\bigl\Vert v \bigr\Vert_{L^2\left(\Omega\right)} \le s \left| v \right|_{H_0^1\left( \Omega \right)}.
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Let
\begin{equation}
C = \dfrac{1}{\alpha_0} \left( \bigl\Vert \alpha \bigr\Vert_\infty + C_p^2 \left\Vert \gamma \right\Vert_\infty \right)
\end{equation}
then
\begin{equation}
\left| u - u_h \right| \le C \min\limits_{ w_h \in V_h} \left| u - w_h \right|_{H_0^1\left(0,1\right)}.
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Finite Element Methods</title>
      <link>https://djps.github.io/courses/numericalanalysis22/part-3/fem/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalanalysis22/part-3/fem/</guid>
      <description>&lt;div class=&#34;details quotemonition MyQuote open&#34;&gt;
        &lt;div class=&#34;details-summary quotemonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-quote-left fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;quotemonition-content&#34;&gt;&lt;em&gt;Finite element methods approximate the solution on a small element.&lt;/em&gt;&lt;/div&gt;
        &lt;/div&gt;

        &lt;div class=&#34;details-summary quotemonition-title&#34; align=&#34;right&#34;&gt;
            &lt;i class=&#34;icon fas fa-quote-right fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			
        &lt;/div&gt;

    &lt;/div&gt;
&lt;h2 id=&#34;distributions&#34;&gt;Distributions&lt;/h2&gt;
&lt;p&gt;Denote by $\mathrm{H}^s(a, b)$, for $s \geq 1$, the space of the functions $f \in C^{s-1}(a, b)$ such that $f^{(s-1)}$ is continuous and piecewise differentiable, so that $f^{(s)}$ exists unless for a finite number of points and belongs to $\mathrm{L}^2(a, b)$. The space $\mathrm{H}^s(a, b)$ is known as the Sobolev function space of order $s$ and is endowed with the norm $\left\Vert \cdot \right\Vert_{H^s(a,b)}$ defined as
\begin{equation}
\left\Vert f \right\Vert_s = \left( \sum_{k=0}^s \left\Vert f^{\left(k\right)} \right\Vert_{\mathrm{L}^2\left(a,b\right)}^2 \right)^{1/2}.
\end{equation}&lt;/p&gt;
&lt;p&gt;Let
\begin{align}
C_0^\infty &amp;amp; = \lbrace \varphi \in C^\infty \, | \, \exists \, a, b \in (0,1) \quad \mbox{such that} \quad \varphi(x)=0 \quad \mbox{for} \quad 0 \leq x &amp;lt; a \quad \mbox{or} \quad b &amp;lt; x \leq 1 \rbrace .
\end{align}&lt;/p&gt;
&lt;p&gt;Then for a function $v \in \mathrm{L}^2(0,1)$ we say $v^\prime$ is the &lt;strong&gt;weak derivative&lt;/strong&gt; (or &lt;strong&gt;distributional derivative&lt;/strong&gt;) if
\begin{equation}
\int \limits_0^1 v^\prime \varphi \, \mathrm{d}x = - \int \limits_0^1 v \varphi^\prime \, \mathrm{d}x \quad \forall \, \varphi \in C^\infty_0 \left(0,1\right).
\end{equation}&lt;/p&gt;
&lt;p&gt;Of interest is
\begin{equation}
H^1 (0, 1) = \lbrace v \in \mathrm{L}^2(0, 1) \, : \, v^\prime \in \mathrm{L}^2(0, 1) \rbrace
\end{equation}&lt;/p&gt;
&lt;p&gt;where $v^\prime$ is the distributional derivative of $v$, and
\begin{equation}
H^1_0 (0, 1) = \lbrace v \in \mathrm{L}^2(0, 1) \, : \, v^\prime \in \mathrm{L}^2(0, 1), \, v(0) = v(1) = 0 \rbrace
\end{equation}&lt;/p&gt;
&lt;p&gt;On $H^1$ there is the semi-norm:&lt;/p&gt;

$$
\left| v \right|_{\mathrm{H}^1(0,1)} = \left( \int_0^1 \left\| v^\prime \left( x \right) \right\|^2 \mathrm{d}x \right)^{1/2} = \left\Vert v^\prime \right\Vert_{\mathrm{L}^2(0,1)}.
$$

&lt;p&gt;To see that it is a semi-norm and not a norm, consider $v$ a constant, so ${v^{\prime}=0}$ thus ${\left| v \right|_{\mathrm{H}^1(0,1)} =0}$ for ${v \neq 0}$ and thus by definition is a semi-norm, rather than a norm. Now consider the integral on functions in $H_0^1$, it is the case that if the integral is zero so the function is constant, but as it must be zero on the boundaries, so the function is zero and hence a norm.&lt;/p&gt;
&lt;h2 id=&#34;galerkin-method&#34;&gt;Galerkin Method&lt;/h2&gt;
&lt;p&gt;Consider the elementary problem:&lt;/p&gt;
&lt;p&gt;\begin{equation}
-\left( \alpha u^{\prime} \right)^{\prime} + \beta u^{\prime} + \gamma u = f\left( x \right) \quad \mbox{on} \quad (0,1) \quad \mbox{with} \quad u(0) = u(1) = 0
\end{equation}&lt;/p&gt;
&lt;p&gt;where $\alpha$, $\beta$, $\gamma \in C^0 \left( \left[ 0, 1 \right] \right)$ and $\alpha(x) \ge \alpha_0 &amp;gt;0$ for all $x \in  \left[ 0, 1 \right]$.&lt;/p&gt;
&lt;p&gt;Next, on $\mathrm{L}^2(0,1)$, define the &lt;strong&gt;scalar product&lt;/strong&gt;
\begin{equation}
\left( f, v \right) = \int \limits_0^1 f v \mathrm{d}x
\end{equation}&lt;/p&gt;
&lt;p&gt;and a &lt;strong&gt;bilinear form&lt;/strong&gt; $a : \left( \cdot, \cdot \right)$ which maps $H_0^1 \times H^1_0 \rightarrow \mathbb{R}$
\begin{equation}
a\left( u, v \right) = \int_0^1 \left( \alpha u^\prime v^\prime + \beta u^\prime v + \gamma u v \right) \mathrm{d}x
\end{equation}&lt;/p&gt;
&lt;p&gt;and consider the &lt;strong&gt;weak form&lt;/strong&gt;
\begin{equation}
\mbox{Find} \quad u \in H^1_0 \quad \mbox{such that} \quad a\left(u,v \right) =\left( f, v\right) \quad \forall \, v \in H^1_0\left(0,1\right).
\end{equation}&lt;/p&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;The following hold:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Let $u$ be a $C^2$ be a solution of the elementary problem, then ${u \in H^1_0}$ also solves the weak form&lt;/li&gt;
&lt;li&gt;Let $u \in H^1_0$ be a solution of the weak problem. If and only if ${u \in C^2\left( \left[0,1 \right] \right)}$ then $u$ also solves the elementary problem.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Fundamental Theorem of the Calculus of Variations&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Suppose that $f$ is integrable on $(0,1)$ and
\begin{equation}
\int_0^1 \phi f \, \mathrm{d}x = 0 \quad \forall \, \phi \in C^\infty_0\left(\left[ 0,1 \right]\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;then $f=0$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;p&gt;Approximate $H_0^1$ by $V_h$. The &lt;strong&gt;discrete weak problem&lt;/strong&gt; is then:
\begin{equation}
\mbox{Find a} \quad u_h \in V_h \quad \mbox{such that} \quad a\left(u_h, v_h\right) = \left(f, v_h \right) \quad \forall \, v_h \in V_h
\end{equation}&lt;/p&gt;
&lt;p&gt;Let $\lbrace \varphi_1, \varphi_2, \ldots, \varphi_N \rbrace$ be a basis of $V_h$, then, with $N=\mbox{dim}V_h$, so that
\begin{equation}
u_h \left(x \right) = \sum\limits_{j=1}^N u_j \varphi_j\left(x\right).
\end{equation}&lt;/p&gt;
&lt;p&gt;So the problem can be written as: Find ${\left(u_1, \ldots u_N \right) \in \mathbb{R}^N}$ such that
\begin{equation}
\sum\limits_{j=1}^N u_j a\left( \varphi_j, \varphi_i \right) = \left( f, \varphi_i \right) \quad i=1, \ldots, N.
\end{equation}&lt;/p&gt;
&lt;p&gt;Denote ${a_{ij}=a\left( \varphi_j, \varphi_i \right)}$ as the elements of the matrix $A$, let ${u=\left(u_1, \ldots, u_N\right)}$ and ${f=\left(f_1, \ldots, f_N\right)}$ be vectors where each entry is given by ${f_i = f \varphi_i}$, so that the problem is equivalent to solving the linear problem ${Au=f}$&lt;/p&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Poincaré-Friedrich Inequality&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Let $\Omega \subset \mathbb{R}^n$ be contained in $n$-dimensional cube of length $s$, then
\begin{equation}
\bigl\Vert v \bigr\Vert_{L^2\left(\Omega\right)} \le s \left| v \right|_{H_0^1\left( \Omega \right)}.
\end{equation}&lt;/p&gt;
&lt;p&gt;For functions which are zero on the boundary a simplified form is
\begin{equation}
\int_{a}^b \left| v(x) \right|^2 \mathrm{d}x \leq C_p \int_a^b \bigl| v^\prime \left(x\right) \bigr|^2 \mathrm{d}x \quad \forall \, v \in V_0
\end{equation}&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Let&lt;/p&gt;
&lt;p&gt;\begin{align}
C &amp;amp; = \dfrac{1}{\alpha_0} \Bigl( \bigl\Vert \alpha \bigr\Vert_\infty + C_p^2 \bigl\Vert \gamma \bigr\Vert_\infty \Bigr)
\end{align}
then&lt;/p&gt;
&lt;p&gt;$$
\left| u - u_h \right|^{}_{H^1\left(0,1\right)} \le C \min\limits_{ w_h \in V_h} \left| u - w_h \right|_{H^1\left(0,1\right)}.
$$&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Coercivity and Continuity of Bilinear Forms&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;A bilinear form $a\left(\cdot,\cdot\right)$ on $V$, with a norm $\bigl\Vert \cdot \bigr\Vert_V$, then a bilinear form is &lt;strong&gt;coercive&lt;/strong&gt; if there exists an ${\alpha_0 &amp;gt; 0}$ such that
\begin{equation}
a(v, v) \geq \alpha_0	\bigl\Vert v \bigr\Vert^2 \quad \forall \, v \in V.
\end{equation}&lt;/p&gt;
&lt;p&gt;A bilinear form is said to be &lt;strong&gt;continuous&lt;/strong&gt; if there exists an ${M &amp;gt; 0}$ such that
\begin{equation}
\left| a\left(u, v\right) \right| \leq M \bigl\Vert u \bigr\Vert_V  \bigl\Vert u \bigr\Vert_V \quad  \forall \, u, v \in V.
\end{equation}&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Lax-Milgram&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;If coercive and continuous, and the right hand side $(f, v)$ satisfies the following inequality
\begin{equation}
\left| \left( f, v \right) \right| \leq K \bigl\Vert v\bigr\Vert_V \quad \forall \, v \in V.
\end{equation}&lt;/p&gt;
&lt;p&gt;Then the weak and discrete weak form problems admit unique solutions which satisfy
\begin{equation}
\bigl\Vert u \bigr\Vert_V  \leq \dfrac{K}{\alpha_0} \quad \mbox{and} \quad  \bigl\Vert u_h \bigr\Vert_V \leq \dfrac{K}{\alpha_0}.
\end{equation}&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Lemma open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Lemma:	&lt;em&gt;Céa&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;It can be shown that
\begin{equation}
\left\Vert u - u_h \right\Vert_V \le \dfrac{M}{\alpha_0} \min_{\substack{w_h \in V_h}} \left\Vert u - w_h \right\Vert_V . \tag{3}\label{eq:cea}
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h2 id=&#34;finite-element-method&#34;&gt;Finite Element Method&lt;/h2&gt;
&lt;p&gt;The finite element method (FEM) is a special technique for constructing a subspace $V_h$ based on piecewise polynomial interpolation.&lt;/p&gt;
&lt;p&gt;Introduce a partition
 $\mathcal{T}_h$ 
of $[0,1]$ into $n$ subintervals $I_j = [x_j, x_{j+1}]$, $n \geq 2$, of width $h_j = x_{j+1} − x_j$, $j = 0, \ldots, n − 1$, with $0 = x_0 &amp;lt; x_1 &amp;lt; \ldots &amp;lt; x_{n−1} &amp;lt; x_n = 1$ and let  $h=\max\limits_{\mathcal{T}_{h}}\left(h_{j}\right)$  .&lt;/p&gt;
&lt;p&gt;Since functions in  $\mathrm{H}_{0}^{1}(0,1)$  are continuous it makes sense to consider for ${k \geq 1}$ the family of piecewise polynomials $X_{h}^{k}$ introduced in &lt;a href=&#34;https://djps.github.io/courses/numericalanalysis22/part-2/interpolation/#piecewise-lagrange-interpolation&#34;&gt;$(2)$&lt;/a&gt;, but where now $[a, b]$ must be replaced by $[0,1]$. Any function $v_{h} \in X_{h}^{k}$ is a continuous piecewise polynomial over $[0,1]$ and its restriction over each interval $I_{j} \in \mathcal{T}_{h}$ is a polynomial of degree less than or equal to $k$.&lt;/p&gt;
&lt;p&gt;Considering the cases $k=1$ and $k=2$, set
\begin{equation}
V_{h} = X_{h}^{k, 0}=\lbrace v_{h} \in X_{h}^{k} \, : \, v_{h}(0) = v_{h}(1) = 0 \rbrace .
\end{equation}&lt;/p&gt;
&lt;p&gt;The dimension $N$ of the finite element space $V_{h}$ is equal to ${n k -1}$. In the following the two cases $k=1$ and $k=2$ will be examined.&lt;/p&gt;
&lt;p&gt;To assess the accuracy of the Galerkin FEM, first notice that, due to Céa&amp;rsquo;s lemma&lt;/p&gt;

\begin{equation}
\min_{w_{h} \in V_{h}}\left\|u-w_{h}\right\|_{\mathrm{H}_{0}^{1}(0,1)} \leq\left\|u-\Pi_{h}^{k} u\right\|_{\mathrm{H}_{0}^{1}(0,1)} \tag{4}\label{eq:cea_estimate}
\end{equation}

&lt;p&gt;where $\Pi_{h}^{k} u$ is the interpolant of the exact solution $u \in V$ from the weak form of the governing equation. From inequality \eqref{eq:cea_estimate} estimating the Galerkin approximation error  $\left\| u - u_{h} \right\|_{\mathrm{H}_{0}^{1}(0,1)}$  is then equivalent to estimating the interpolation error  $\left\| u -\Pi_{h}^{k} u \right\|_{H_{0}^{1}(0,1)}$  . When ${k=1}$, using \eqref{eq:cea} and the bounds on the interpolation errors &lt;a href=&#34;https://djps.github.io/courses/numericalanalysis22/part-2/interpolation/#lagrange-interpolation/&#34;&gt;$(1)$&lt;/a&gt;&lt;/p&gt;

$$
\left\| u - u_{h} \right\|^{}_{\mathrm{H}_{0}^{1}(0,1)} \leq \frac{M}{\alpha_{0}} C h \|u\|_{\mathrm{H}^{2}(0,1)}
$$

&lt;p&gt;provided that $u \in \mathrm{H}^{2}(0,1)$. This estimate can be extended to the case ${k&amp;gt;1}$ as stated in the following convergence result.&lt;/p&gt;

\begin{equation}
\left\| u - u_{h} \right\|_{ \mathrm{H}_{0}^{1}(0,1) }
\end{equation}

&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Lax-Milgram&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Let $u \in H_0^1\left( 0,1 \right)$ be the exact solution of
\begin{equation}
a\left( u, v \right) = f\left(v\right) \quad \forall \, v \in H_0^1\left(0\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;and let $u_h \in V_h$ be it finite element approximation using a continuous piecewise polynomial of degree less than or equal to $k$, where $k \ge 1$. Furthermore, assume that $u \in \mathrm{H}^{s}(0,1)$ for some $s \geq 2$. Then the error is bounded as&lt;/p&gt;
&lt;p&gt;\begin{equation}
\bigl\Vert u - u_{h}\bigr\Vert_{\mathrm{H}_{0}^{1}(0,1)} \le \dfrac{M}{\alpha_0} C h^l \bigl\Vert u \bigr\Vert_{H^{l+1}(0,1)}
\end{equation}&lt;/p&gt;
&lt;p&gt;where $l = \min\left( k, s-1 \right)$. Additionally, under the same assumptions it is possible to show that&lt;/p&gt;
&lt;p&gt;\begin{equation}
\bigl\Vert u - u_{h}\bigr\Vert_{\mathrm{L}^{2}(0,1)} \leq C h^{l+1} \bigl\Vert u \bigr\Vert_{\mathrm{H}^{l+1}(0,1)} .
\end{equation}&lt;/p&gt;
&lt;p&gt;The error estimate shows that the Galerkin method is convergent, that is the approximation error tends to zero as $h \rightarrow 0$. The order of convergence is $k$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition example open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ol fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			example:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;An &lt;code&gt;.ipynb&lt;/code&gt; notebook, with an example of the finite element method in one-dimension can be viewed &lt;a href=&#34;https://djps.github.io/courses/numericalanalysis22/notebooks/1dfem/&#34;&gt;here&lt;/a&gt;, or can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/1dfem.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;An  example of the finite element method in two-dimensions can be viewed &lt;a href=&#34;https://djps.github.io/courses/numericalanalysis22/notebooks/2dfem/&#34;&gt;here&lt;/a&gt;, or can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/2dfem.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;!--
In the space $H_0^1$ derivatives are used instead of functions $v \in L^2\left( 0,1 \right)$.

Let

\begin{equation}
C_0^\infty = \lbrace \varphi \in C^\infty \, | \, \exists \, a, b \in (0,1) \quad \mbox{such that} \quad \varphi ( x ) = 0 \quad \mbox{for} \quad 0 \leq x &lt; a \quad \mbox{or} \quad b &lt; x \leq 1 \rbrace
\end{equation}

Then for a function $v \in L^2 (0,1)$ we say $v^\prime$ is the _weak derivative_ (or distributional derivative) if

\begin{equation}
\int \limits_0^1 v^\prime \varphi \, \mathrm{d}x = - \int \limits_0^1 v \varphi^\prime \, \mathrm{d}x \quad \forall \, \varphi \in C^\infty_0 \left(0,1\right).
\end{equation}

Approximate $H_0^1$ by $V_h$. The **discrete weak problem** is then:

Find a $u_h \in V_h$ such that
\begin{equation}
a\left(u_h, v_h\right) = \left(f, v_h \right) \quad \forall \, v_h \in V_h .
\end{equation}

Let $\lbrace \varphi_1, \varphi_2, \ldots, \varphi_N \rbrace$ be a basis of $V_h$, then, with $N=\mbox{dim}V_h$, so that
\begin{equation}
u_h \left(x \right) = \sum\limits_{j=1}^N u_j \varphi_j\left(x\right).
\end{equation}
So the problem can be written as:

Find ${\left(u_1, \ldots u_N \right) \in \mathbb{R}^N}$ such that
\begin{equation}
\sum \limits_{j=1}^N u_j a\left( \varphi_j, \varphi_i \right) = \left( f, \varphi_i \right) \quad i=1, \ldots, N.
\end{equation}
Denote ${a_{ij}=a\left( \varphi_j, \varphi_i \right)}$ as elements of the matrix $A$, ${u=\left(u_1, \ldots, u_N\right)}$ and ${f = \left( f_1, \ldots, f_N \right)}$ where each entry in the vector is given by ${f_i = f \varphi_i}$, so that the problem is equivalent to solving the linear problem ${Au=f}$


!!! note &#34;Theorem:&#34;

    Let $u \in H_0^1\left( 0,1 \right)$ be the exact solution of

    \begin{equation}
    a\left( u, v\right) = f\left(v\right) \quad \forall \, v \in H_0^1 \left(0, 1 \right)
    \end{equation}

    and let $u_h \in V_h$ be it finite element approximation using a continuous piecewise polynomial of degree less than or equal to $k$, where $k \ge 1$.

    Furthermore, assume that $u \in H^{s}(0,1)$ for some $s \geq 2$. Then the error is bounded as

    \begin{equation}
    \left\| u - u_{h}\right\|_{H_{0}^{1}(0,1)} \leq \frac{M}{\alpha_{0}} C h^{l}\|u\|_{H^{l+1}(0,1)}
    \end{equation}

    where $l = \min\left( k, s-1 \right)$. Additionally, under the same assumptions it is possible to show that

    \begin{equation}
    \left\| u - u_{h} \right\|_{L^{2}(0,1)} \leq C h^{l+1} \|u \|_{H^{l+1}(0,1)} .
    \end{equation}


The error estimate shows that the Galerkin method is _convergent_, that is the approximation error tends to zero as $h \rightarrow 0$. Additionally, the order of convergence is $k$.
--&gt;
</description>
    </item>
    
  </channel>
</rss>
