<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>JTMS-MAT-13: Numerical Methods | David Sinden</title>
    <link>https://djps.github.io/courses/numericalmethods24/</link>
      <atom:link href="https://djps.github.io/courses/numericalmethods24/index.xml" rel="self" type="application/rss+xml" />
    <description>JTMS-MAT-13: Numerical Methods</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 14 Nov 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://djps.github.io/media/logo_hud7cfbe45e4df55bd5c7c181ea654d8f2_56982_300x300_fit_lanczos_3.png</url>
      <title>JTMS-MAT-13: Numerical Methods</title>
      <link>https://djps.github.io/courses/numericalmethods24/</link>
    </image>
    
    <item>
      <title>Taylor Series</title>
      <link>https://djps.github.io/courses/numericalmethods24/part-1/taylor/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalmethods24/part-1/taylor/</guid>
      <description>&lt;!--
&lt;p style=&#34;font-size: 14px; letter-spacing: 0.03em; color: rgb(0,0,0,0.54); border-top: 1px solid #e8e8e8; margin-top: 30px; padding-top: 10px; border-bottom: 1px solid #e8e8e8; margin-bottom: 30px; padding-bottom: 10px;&#34;&gt; &lt;span style=&#34;font-weight: 700;&#34;&gt; David Sinden &lt;/span&gt; &amp;middot;
&lt;i class=&#34;far fa-calendar&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; March 11, 2022 &amp;middot;
&lt;i class=&#34;far fa-clock&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; 45 minute read &lt;/p&gt;
--&gt;
&lt;p style=&#34;font-size: 14px; letter-spacing: 0.03em; color: rgb(0,0,0,0.54);&#34;&gt; &lt;span style=&#34;font-weight: 700;&#34;&gt; David Sinden &lt;/span&gt; &amp;middot;
&lt;i class=&#34;far fa-calendar&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; December 07, 2023 &amp;middot;
&lt;i class=&#34;far fa-clock&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; 15 minute read &lt;/p&gt;
&lt;h2 id=&#34;taylor-series&#34;&gt;Taylor Series&lt;/h2&gt;
&lt;p&gt;The Taylor series, or Taylor expansion of a function, is defined as&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Taylor Series&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;For a function $f : \mathbb{R} \mapsto \mathbb{R}$ which is infinitely differentiable at a point $c$, the Taylor series of $f(c)$ is given by
\begin{equation}
\sum\limits_{k=0}^{\infty} \dfrac{ f^{(k)} \left( c \right) }{k!} \left( x - c \right)^{k}.
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;p&gt;This is a power series, which is convergent for some radius.&lt;/p&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Taylor&amp;#39;s Theorem&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;For a function $f \in C^{n+1}\left([a, b]\right)$, i.e. $f$ is $(n+1)$-times continuously differentiable in the interval $[a, b]$, then for some $c$ in the interval, the function can be written as
\begin{equation}
f\left( x \right) = \sum\limits_{k=0}^{n} \dfrac{f^{(k)} \left(c\right) }{k!} \left( x- c \right)^{k} + \dfrac{f^{(n+1)} \left( \xi \right) }{\left( n + 1 \right)!} \left( x - c \right)^{n+1}
\end{equation}
for some value $\xi \in \left[ a, b \right]$ where
\begin{equation}
\lim\limits_{\xi \rightarrow c} \dfrac{ f^{(n+1)} \left( \xi \right) }{ \left( n + 1 \right)!} \left( x - c \right)^{n+1} = 0.
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Example open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-calculator fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Example:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;An &lt;code&gt;.ipynb&lt;/code&gt; notebook with an example of the Taylor series for $\sin\left(x\right)$ can be accessed online &lt;a href=&#34;https://djps.github.io/courses/numericalmethods24/notebooks/taylor_series/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/taylor_series.py&#34;&gt;here&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href=&#34;https://djps.github.io/ipyth/taylor_series.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Number Representations</title>
      <link>https://djps.github.io/courses/numericalmethods24/part-1/number-representations/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalmethods24/part-1/number-representations/</guid>
      <description>&lt;style&gt;
mjx-container {
  display: inline-block;
}
mjx-assistive-mml {
  right: 0px;
  bottom: 0px;
}
&lt;/style&gt;
&lt;!--
&lt;p style=&#34;font-size: 14px; letter-spacing: 0.03em; color: rgb(0,0,0,0.54); border-top: 1px solid #e8e8e8; margin-top: 30px; padding-top: 10px; border-bottom: 1px solid #e8e8e8; margin-bottom: 30px; padding-bottom: 10px;&#34;&gt; &lt;span style=&#34;font-weight: 700;&#34;&gt; David Sinden &lt;/span&gt; &amp;middot;
&lt;i class=&#34;far fa-calendar&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; March 11, 2022 &amp;middot;
&lt;i class=&#34;far fa-clock&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; 45 minute read &lt;/p&gt;
--&gt;
&lt;p style=&#34;font-size: 14px; letter-spacing: 0.03em; color: rgb(0,0,0,0.54);&#34;&gt; &lt;span style=&#34;font-weight: 700;&#34;&gt; David Sinden &lt;/span&gt; &amp;middot;
&lt;i class=&#34;far fa-calendar&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; January 05, 2024 &amp;middot;
&lt;i class=&#34;far fa-clock&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; 25 minute read &lt;/p&gt;
&lt;h2 id=&#34;errors&#34;&gt;Errors&lt;/h2&gt;
&lt;p&gt;There are many different sources of errors which can affect numerical methods, such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Errors in data&lt;/li&gt;
&lt;li&gt;Round-off errors&lt;/li&gt;
&lt;li&gt;Truncation errors&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Absolute and Relative Errors&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Let $\tilde{a}$ be an approximation to $a$, then the &lt;strong&gt;absolute error&lt;/strong&gt; is given by $\left| {\tilde{a}-a} \right|$.&lt;/p&gt;
&lt;p&gt;If ${\left| a \ne 0\right|}$, the &lt;strong&gt;relative error&lt;/strong&gt; is given by $\left| \dfrac{\tilde{a}-a}{a} \right|$ and the error bound is the magnitude of the admissible error.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;For both addition and subtraction the bounds for the absolute error are added.&lt;/p&gt;
&lt;p&gt;In division and multiplication the bounds for the relative errors are added.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Linear Sensitivity to Uncertainties&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;If $y(x)$ is a smooth function, i.e. is differentiable, then $\left| y^{\prime} \right|$ can be interpreted as the &lt;strong&gt;linear sensitivity&lt;/strong&gt; of $y(x)$ to uncertainties in $x$.&lt;/p&gt;
&lt;p&gt;For functions of several variables, i.e. $f : \mathbb{R}^n \rightarrow \mathbb{R}$, then
$$
\left| \Delta y \right| \le \sum\limits_{i=1}^{n}\left| \dfrac{\partial y}{\partial x_i} \right|\left| \Delta x_i \right|
$$
where
$\left| \Delta x_i \right| = \tilde{x}_i - x_i$ for an approximation $\tilde{x}_i$, thus  ${\left| \Delta y_i \right| = \tilde{y}_i - y_i = f \left( \tilde{x}_i \right) - \left( x_i \right)}$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Example open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-calculator fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Example:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;An &lt;code&gt;.ipynb&lt;/code&gt; notebook with an example of errors and number representations can be accessed online &lt;a href=&#34;https://djps.github.io/courses/numericalmethods24/notebooks/number_representations/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/number_representations.py&#34;&gt;here&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href=&#34;https://djps.github.io/ipyth/number_representations.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h2 id=&#34;number-representations&#34;&gt;Number Representations&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Base Representation&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Let $b \in \mathbb{N} \backslash \lbrace 1 \rbrace$. Every integer $x \in \mathbb{N}_0$
can be written as a unique expansion with respect to base $b$ as&lt;/p&gt;
&lt;p&gt;\begin{equation}
\left(x\right)_b = a_0 b^0 + a_1 b^1 + \ldots + a_n b^n = \sum \limits_{i=0}^{n} a_i b^i
\end{equation}&lt;/p&gt;
&lt;p&gt;A number can be written in a nested form:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\left(x\right)_b &amp;amp; = a_0 b^0 + a_1 b^1 + \ldots + a_n b^n \\
&amp;amp; = a_0 + b\left( a_1 + b\left( a_2 + b\left( a_3 + \ldots + b a_n\right) \right. \ldots  \right)
\end{align}
$$&lt;/p&gt;
&lt;p&gt;with $a_i \in \mathbb{N}_0$ and $a_i &amp;lt; b$, i.e. ${a_i \in \lbrace 0, \ldots, b-1 \rbrace }$.&lt;/p&gt;
&lt;p&gt;For a real number, ${x \in \mathbb{R}}$, write&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
x &amp;amp; = \sum \limits_{i=0}^{n} a_i b^i + \sum \limits_{i=1}^{\infty} \alpha_i b^{-i} \\
&amp;amp; = a_n  \ldots a_0 \cdot \alpha_1 \alpha_2 \ldots
\end{align}
$$&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Algorithm open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-laptop-code fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Algorithm:	&lt;em&gt;Euclid&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Euclid&amp;rsquo;s algorithm can convert number $x$ in base 10, i.e.
$\left(x \right)_{10}$
into another base, $b$, i.e. $\left(x \right)_{b}$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Input $\left(x \right)_{10}$&lt;/li&gt;
&lt;li&gt;Determine the smallest integer $n$ such that ${x &amp;lt; b^{n+1}}$&lt;/li&gt;
&lt;li&gt;Let $y=x$. Then for $i=n, \ldots, 0$
$$
\begin{array}{rcl}
a_i &amp;amp; = &amp;amp; y \mbox{ div } b^i \\
y &amp;amp; = &amp;amp; y \mbox{ mod } b^i
\end{array}
$$
which at each steps provides an $a_i$ and updates $y$.&lt;/li&gt;
&lt;li&gt;Output as $\left(x \right)_{b} = a_n a_{n-1} \cdots a_0$&lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Algorithm open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-laptop-code fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Algorithm:	&lt;em&gt;Horner&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;ol&gt;
&lt;li&gt;Input $\left(x \right)_{10}$&lt;/li&gt;
&lt;li&gt;Set $i=0$&lt;/li&gt;
&lt;li&gt;Let $y=x$. Then while $y&amp;gt;0$
$$
\begin{array}{rcl}
a_i &amp;amp; = &amp;amp; y \mbox{ mod } b \\
y &amp;amp; = &amp;amp; y \mbox{ div } b \\
i &amp;amp; = &amp;amp; i+1
\end{array}
$$
which at each steps provides an $a_i$ and updates $y$.&lt;/li&gt;
&lt;li&gt;Output as $\left(x \right)_{b} = a_n a_{n-1} \cdots a_0$&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Floating Point Representations&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;A number may be represented as
$$
x = \sigma \times f \times b^{t-p}
$$
where $\sigma$ is the sign of the number, i.e. $\pm 1$, $f$ is the mantissa, $b$ is the base, $t$ is the shifted exponent (where $p$ is the shift required to determine the actual exponent.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Normalized&lt;/em&gt; floating point representations with respect to some base $b$, may store a number $x$ as
$$x= \pm 0 \cdot a_1 \ldots a_k \times b^n$$
where the $a_i \in \lbrace 0, 1, \ldots b-1 \rbrace$ are called the &lt;strong&gt;digits&lt;/strong&gt;, $k$ is the &lt;strong&gt;precision&lt;/strong&gt; and $n$ is the &lt;strong&gt;exponent&lt;/strong&gt;. The set $a_1, \ldots, a_k$ is called the &lt;strong&gt;mantissa&lt;/strong&gt;. Impose that $a_1 \ne 0$, it makes the representation unique.&lt;/p&gt;
&lt;p&gt;Alternative conventions may express $x= \pm  a_1 \cdot a_2 \ldots a_{k-1} \times b^n$, with $a_1 \ne 0$. Normalization refers to specifying the digits before the decimal place.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Let $x$ and $y$ be two normalized floating point numbers with ${x &amp;gt; y &amp;gt; 0}$ and base ${b=2}$. If there exists integers $p$ and ${q \in \mathbb{N}_0}$ such that
$$2^{-p} \leq 1 - \dfrac{y}{x} \leq 2^{-q}$$
then, at most $p$ and at least $q$ significant bits (i.e. significant figures written in base 2) are lost during subtraction.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Equations</title>
      <link>https://djps.github.io/courses/numericalmethods24/part-1/linear-equations/</link>
      <pubDate>Mon, 20 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalmethods24/part-1/linear-equations/</guid>
      <description>&lt;h2 id=&#34;linear-equations&#34;&gt;Linear Equations&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Systems of Linear Equations&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;A system of linear equations (or linear system) is a collection of one or more linear equations involving the same variables. If there are $m$ equations with $n$ unknown variables to solve for&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
a_{1,1} x_1 + a_{1,2} x_2 + \ldots + a_{1,n} x_n &amp;amp; = b_1 \\
a_{2,1} x_1 + a_{2,2} x_2 + \cdots + a_{2,n} x_n &amp;amp; = b_2 \\
\vdots &amp;amp;  \\
a_{m,1} x_1 + a_{m,2} x_2 + \cdots + a_{m,n} x_n &amp;amp; = b_m
\end{align}
$$&lt;/p&gt;
&lt;p&gt;The system of linear equations can be written in matrix form ${Ax=b}$, where
$$
A = \left(
\begin{array}{cccc}
a_{1,1} &amp;amp; a_{1,2} &amp;amp; \cdots &amp;amp; a_{1,n} \\
a_{2,1} &amp;amp; a_{2,2} &amp;amp; \cdots &amp;amp; a_{2,n} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
a_{m,1} &amp;amp; a_{m,2} &amp;amp; \cdots &amp;amp; a_{m,n}
\end{array}
\right), \quad x = \left(
\begin{array}{c}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{array}
\right), \quad b = \left(
\begin{array}{c}
b_1 \\
b_2 \\
\vdots \\
b_m
\end{array}
\right),
$$
so that $A \in \mathbb{R}^{m \times n}$, $x \in \mathbb{R}^n$ and $b \in \mathbb{R}^m$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Banded Systems&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A &lt;strong&gt;banded&lt;/strong&gt; matrix is a matrix whose non-zero entries are confined to a diagonal band, comprising the main diagonal and zero or more diagonals on either side.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Symmetric Matrices&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;A square matrix $A$ is &lt;strong&gt;symmetric&lt;/strong&gt; if ${A=A^T}$, that is, $a_{i,j}=a_{j,i}$ for all indices $i$ and $j$.&lt;/p&gt;
&lt;p&gt;A square matrix is said to be &lt;strong&gt;Hermitian&lt;/strong&gt; if the matrix is equal to its conjugate transpose, i.e. $a_{i,j}=\overline{a_{j,i}}$ for all indices $i$ and $j$. A Hermitian matrix is written as $A^H$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Positive Definite Matrices&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A matrix, $M$, is said to be &lt;strong&gt;positive definite&lt;/strong&gt; if it is symmetric (or Hermitian) and all its eigenvalues are real and positive.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Nonsingular Matrices&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A matrix is &lt;strong&gt;non-singular&lt;/strong&gt; or &lt;strong&gt;invertible&lt;/strong&gt; if there exists a matrix $A^{-1}$ such that ${A^{-1}A = A A^{-1} = I,}$ where $I$ is the identity matrix.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Note open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-pencil-alt fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Note:	&lt;em&gt;Properties of Nonsingular Matrices&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;For a nonsingular matrix, the following all hold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nonsingular matrix has full rank&lt;/li&gt;
&lt;li&gt;A square matrix is nonsingular if and only if the determinant of the matrix is non-zero.&lt;/li&gt;
&lt;li&gt;If a matrix is singular, both versions of Gaussian elimination will fail due to division by zero, yielding a floating exception error.&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;If $\tilde{x}$ is an approximate solution to the linear problem ${Ax=b}$, then the &lt;strong&gt;residual&lt;/strong&gt; is defined as ${r = A \tilde{x}-b}$.&lt;/p&gt;
&lt;p&gt;If $\left| r \right|$ is large due to rounding, the matrix is said to be &lt;strong&gt;ill-conditioned&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;direct-methods&#34;&gt;Direct Methods&lt;/h3&gt;
&lt;div class=&#34;details admonition Algorithm open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-laptop-code fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Algorithm:	&lt;em&gt;Gaussian Elimination&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Gaussian elimination is a method to solve systems of linear equations based on forward elimination (a series of row-wise operations) to convert the matrix, $A$, to upper triangular form (echelon form), and then back-substitution to solve the system. The row operations are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;row swapping&lt;/li&gt;
&lt;li&gt;row scaling, i.e. multiplying by a non-zero scalar&lt;/li&gt;
&lt;li&gt;row addition, i.e. adding a multiple of one row to another&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;{\raggedright
\begin{array}{llll}
{\footnotesize \textsf{1}} &amp;amp; \textsf{For} \, k=1 \, \textsf{to} \, n-1 &amp;amp; &amp;amp; \\
\footnotesize{\textsf{2}} &amp;amp;                                               &amp;amp; \textsf{For } i = k + 1 \textsf{ to } n &amp;amp; \\
&lt;small&gt;3&lt;/small&gt; &amp;amp;                                               &amp;amp;                                         &amp;amp; \textsf{For } j = k \text{ to } n
\end{array}
}&lt;/p&gt;
&lt;p&gt;For $k$=1 to $n-1$&lt;/p&gt;
&lt;p&gt;$\quad$ For $i$ = $k$ + 1 to $n$&lt;/p&gt;
&lt;p&gt;$\quad \quad$ For $j$ = k to $n$&lt;/p&gt;
&lt;p&gt;$\quad \quad \quad a_{i,j} = a_{i,j} - \dfrac{a_{i,k}}{a_{k,k}} a_{k,j}$&lt;/p&gt;
&lt;p&gt;$\quad \quad b_{i} = b_{i} - \dfrac{a_{i,k}}{a_{k,k}} b_{k}$&lt;/p&gt;
&lt;p&gt;Then back substitute,&lt;/p&gt;
&lt;p&gt;$x_n = \dfrac{b_n}{a_{n,n}}$&lt;/p&gt;
&lt;p&gt;For $i$ =$n$-1 to 1&lt;/p&gt;
&lt;p&gt;$\quad y = b_i$&lt;/p&gt;
&lt;p&gt;$\quad$ For $j$ = $n$ to $i+1$&lt;/p&gt;
&lt;p&gt;$\quad \quad y = y - a_{i,j} x_y$&lt;/p&gt;
&lt;p&gt;$\quad x_i = \dfrac{y}{a_{i,i}}$&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Algorithm open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-laptop-code fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Algorithm:	&lt;em&gt;Gaussian Elimination with Scaled Partial Pivoting&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A pivot element is the element of a matrix which is selected first to do certain calculations. Pivoting helps reduce errors due to rounding.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Upper and Lower Triangular Matrices&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A square matrix is said to be &lt;strong&gt;lower triangular matrix&lt;/strong&gt; if all the elements above the main diagonal are zero and &lt;strong&gt;upper triangular&lt;/strong&gt; if all the entries below the main diagonal are zero.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;$LU$-Decomposition&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Let ${A \in \mathbb{R}^{n \times n}}$ be invertible. Then there exists a decomposition of $A$ such that ${A=LU}$, where $L$ is a lower triangular matrix and $U$ is an upper triangular matrix, And
$$
L = U_1^{-1} U_2^{-1} \cdots U_{n-1}^{-1}
$$
where each matrix $U_i$ is a matrix which describes the $i^{\text{th}}$ step in forward elimination.
$$
U = U_{n-1} \cdots U_2 U_1 A
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Cholesky-Decomposition&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A symmetric, positive definite matrix can be decomposed as $A=LL^T$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Algorithm open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-laptop-code fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Algorithm:	&lt;em&gt;Cholesky&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;small&gt;1&lt;/small&gt;  For $i$=1 to $n$ &lt;br&gt;
&lt;small&gt;2&lt;/small&gt;  $\hspace{3cm}$ For $j$ = 1 to $i-1$ &lt;br&gt;
&lt;small&gt;3&lt;/small&gt;  \hspace{6cm} $y = a_{i,j}$ &lt;br&gt;
&lt;small&gt;4&lt;/small&gt;  &lt;div class=&#34;horizontalgap&#34; style=&#34;width:6cm&#34;&gt;&lt;/div&gt; For $k$ = 1 to $j$-1 &lt;br&gt;
&lt;small&gt;5&lt;/small&gt;  $\quad \quad \quad y = y - l_{i,k} l_{j,k}$ &lt;br&gt;
&lt;small&gt;6&lt;/small&gt;  $\quad \quad l_{i,j} = y / l_{j,j}$ &lt;br&gt;
&lt;small&gt;7&lt;/small&gt;  $\quad y= a_{i,i}$ &lt;br&gt;
&lt;small&gt;8&lt;/small&gt;  $\quad$ For $k$= 1 to $i-1$ &lt;br&gt;
&lt;small&gt;9&lt;/small&gt;  $\quad \quad y = y - l_{i,k} l_{i,k}$ &lt;br&gt;
&lt;small&gt;10&lt;/small&gt; $\quad$ if $y \le 0$ there is no solution &lt;br&gt;
&lt;small&gt;11&lt;/small&gt; $\quad$ else $l_{i,i} = \sqrt{y}$ &lt;br&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;indirect-methods&#34;&gt;Indirect Methods&lt;/h3&gt;
&lt;p&gt;For a non-singular matrix $A$, consider the iterative scheme
$$
Q x_{k +1}= \left( Q - A \right) x_{k} + b.
$$&lt;/p&gt;
&lt;p&gt;This is equivalent to&lt;/p&gt;
&lt;p&gt;$$
x_{k +1}= \left( I - Q^{-1} A \right) x_{k} + Q^{-1} b.
$$&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Spectral Radius of a Matrix&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;The &lt;strong&gt;spectral radius&lt;/strong&gt; of a matrix $A$ is defined as
$$
\rho \left( A \right) = \max \left\{  \left|  \lambda_1 \right|,  \left|  \lambda_2 \right|, \ldots   \left|  \lambda_n \right| \right\}
$$&lt;/p&gt;
&lt;p&gt;where the $\lambda_i$ are the eigenvalues of the matrix.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Convergence&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;The iterative scheme converges if and only if the spectral radius of the matrix $I - Q^{-1} A$ is less than one, i.e. ${\rho\left( I - Q^{-1} A \right) \lt 1}$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Richardson Iteration&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Let $Q=I$. Then &lt;strong&gt;Richardson iteration&lt;/strong&gt; computes the sequence of vectors&lt;/p&gt;
&lt;p&gt;$$
x_{k +1} = \left(I - A \right) x_{k} + b
$$&lt;/p&gt;
&lt;p&gt;This may converge to $x = A^{-1} b$, depending on $A$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;modified Richardson iteration&lt;/strong&gt; scales ${Q= \omega I}$, so that
$$
x_{k +1}=  x_{k} + \omega \left( b - A x_{k} \right).
$$&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Jacobi Iteration&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;The &lt;strong&gt;Jacobi iteration&lt;/strong&gt; scheme has $Q=D$, so
$$
x_{k +1} = \left(I - D^{-1} A \right) x_{k} + D^{-1} b.
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Diagonally Dominant Matrices&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A matrix $A \in \mathbb{R}^{n \times n}$ is said to be &lt;strong&gt;diagonally dominant&lt;/strong&gt; if, for every row, the absolute value of the diagonal element is greater or equal to the sum of the magnitudes of all other elements, i.e.
$$
\left| a_{i,i} \right| \ge \sum\limits_{\substack{j=1,\newline{}j \neq i}}^n \left| a_{i,j} \right| \quad \mbox{for all } i \in (1,n)
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Convergence of Jacobi Scheme&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;If the matrix $A$ is diagonally dominant, then the Jacobi scheme converges for any initial guess $x_0$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Gauss-Seidel Scheme&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Let $Q=L + D$, then the &lt;strong&gt;Gauss-Seidel&lt;/strong&gt; scheme is given by
$$
\begin{equation}
(D + L) x^{(n+1)} = -U x^{(n)} + b. \label{eq:GS}
\end{equation}
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Convergence of Gauss-Seidel&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;If the matrix $A$ is diagonally dominant, then the Gauss-Seidel scheme converges for any initial guess $x_0$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Successive Over Relaxation&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Let $Q=L + \dfrac{1}{\omega}D$, thus
$$
\begin{equation}
(D + \omega L) x^{(n+1)} = -\left( \left(\omega - 1 \right)D + \omega U \right) x^{(n)} + \omega b
\end{equation}
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Convergence of Successive Over Relaxation&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Let $A$ be a symmetric matrix with positive entries on the diagonal and let ${\omega \in \left(0,2\right)}$. Then, if and only if $A$ is positive definite will the method of successive over relaxation converge.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Example open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-calculator fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Example:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;An &lt;code&gt;.ipynb&lt;/code&gt; notebook with an example of iterative solvers for linear systems can be accessed online &lt;a href=&#34;https://djps.github.io/courses/numericalmethods24/notebooks/iterative/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/iterative.py&#34;&gt;here&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href=&#34;https://djps.github.io/ipyth/iterative.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nonlinear Equations</title>
      <link>https://djps.github.io/courses/numericalmethods24/part-1/nonlinear-equations/</link>
      <pubDate>Fri, 17 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalmethods24/part-1/nonlinear-equations/</guid>
      <description>&lt;p style=&#34;font-size: 14px; letter-spacing: 0.03em; color: rgb(0,0,0,0.54);&#34;&gt; &lt;span style=&#34;font-weight: 700;&#34;&gt; David Sinden &lt;/span&gt; &amp;middot;
&lt;i class=&#34;far fa-calendar&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; March 11, 2024 &amp;middot;
&lt;i class=&#34;far fa-clock&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; 45 minute read &lt;/p&gt;
&lt;h2 id=&#34;root-finding&#34;&gt;Root Finding&lt;/h2&gt;
&lt;h3 id=&#34;bisection-method&#34;&gt;Bisection Method&lt;/h3&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Bisection Method&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;The bisection method, when applied in the interval $\left[a, b\right]$ to a function $f \in C^0 \left( \left[a, b\right] \right)$ with ${f(a)f(b) &amp;lt; 0}$&lt;/p&gt;
&lt;p&gt;Bisect the interval into two subintervals $\left[a, c\right]$ and $\left[c, b\right]$ such that ${a &amp;lt; c &amp;lt; b}$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $f(c) = 0$ or is sufficiently close, then $c$ is a root&lt;/li&gt;
&lt;li&gt;Else, if $f(c)f(a) &amp;lt; 0$ continue in the interval $[a, c]$&lt;/li&gt;
&lt;li&gt;Else, if $f(c)f(b) &amp;lt; 0$ continue in the interval $[c, b]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Bisection Method&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;The bisection method, when applied in the interval $[a,b]$ to a function $f \in C^0 \left( \left[a, b\right] \right)$ with ${f(a)f(b) &amp;lt; 0}$ will compute, after $n$ steps, an approximation $c_n$ of the root $r$ with error
$$
\left| r - c_n \right| \lt \dfrac{b-a}{2n}
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;newtons-method&#34;&gt;Newton&amp;rsquo;s Method&lt;/h3&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Let a function $f \in C^1 \left( \left[a, b \right] \right)$, then for an initial guess $x_0$, Newton’s method is
$$
x_{n+1} = x_n - \dfrac{f\left( x_n \right)}{f\left( x_n \right)}
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;When Newton’s method converges, it converges to a root, $r$, of $f$, i.e. ${f\left(r\right)=0}$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Let $f \in C^1\left( \left[ a, b\right] \right)$, with&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$f(a)f(b) &amp;lt; 0$&lt;/li&gt;
&lt;li&gt;$f^\prime\left(x\right) \ne 0$ for all $x \in \left( a, b \right)$&lt;/li&gt;
&lt;li&gt;$f^{\prime\prime}\left(x\right) $ exists, is continuous and either ${f^{\prime\prime}\left(x\right)  &amp;gt; 0}$ or ${f^{\prime\prime}\left(x\right) &amp;lt; 0}$ for all ${x \in \left( a, b \right)}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then $f(x)=0$ has exactly one root, $r$, in the interval and the sequence generated by Newton iterations converges to the root when the initial guess is chosen according to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if $f(a) &amp;lt; 0$ and $f^{\prime\prime}(a) &amp;lt; 0$ &lt;em&gt;or&lt;/em&gt; $f(a) &amp;gt; 0$ and $f^{\prime\prime}(a) &amp;gt; 0$ then $x \in \left[ a, r \right]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if $f(a) &amp;lt; 0$ and $f^{\prime\prime}(a) &amp;gt; 0$ &lt;em&gt;or&lt;/em&gt; $f(a) &amp;gt; 0$ and $f^{\prime\prime}(a) &amp;lt; 0$ then $x \in \left[ r, b \right]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The iterate in the sequence satisfies
$$
\left| x_n - r\right| \lt \dfrac{f\left( x_n \right)}{\min \limits_{x \in [a,b]}\left| f^{\prime}\left( x \right) \right|}
$$&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Let $f \in C^1\left( \left[ a, b\right] \right)$, with&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$f(a)f(b) &amp;lt; 0$&lt;/li&gt;
&lt;li&gt;$f^\prime\left( x \right) \ne 0$ for all $x \in \left( a, b \right)$&lt;/li&gt;
&lt;li&gt;$f^{\prime\prime}\left( x \right)$ exists and is continuous, i.e. ${f\left( x \right) \in C^2\left( \left[ a, b \right]\right)}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then, if $x_0$ is close enough to the root $r$, Newton&amp;rsquo;s method converges quadratically.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;secant-methods&#34;&gt;Secant Methods&lt;/h3&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;The secant method is defined as
$$
x_{n+1} = x_n - f \left( x_n \right) \dfrac{x_{n-1} - x_n}{f \left( x_{n-1} \right) - f \left( x_n \right)}
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Let $f \in C^2\left( \left[a, b\right] \right)$, and $r \in \left(a,b\right)$ such that ${f\left(r\right) = 0}$ and ${f^{\prime}\left(r\right) \ne 0}$. Furthermore, let
$$
x_{n+1} = x_n - f \left( x_n \right) \dfrac{x_{n-1} - x_n}{f \left( x_{n-1} \right) - f \left( x_n \right)}
$$
Then there exists a $\delta &amp;gt; 0$ such that when ${\left|  r - x_0 \right| &amp;lt; \delta}$ and ${\left|  r - x_1 \right| &amp;lt; \delta}$, then the following holds:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\lim\limits_{n\rightarrow\infty} \left|  r - x_n \right| = 0 \Leftrightarrow \lim\limits_{n\rightarrow\infty} x_n = r$&lt;/li&gt;
&lt;li&gt;$\left|  r - x_{n+1} \right| \le \mu \left|  r - x_{n} \right|^{\alpha}$ with ${\alpha =\dfrac{1+\sqrt{5}}{2} }$&lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Example open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-calculator fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Example:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;An &lt;code&gt;.ipynb&lt;/code&gt; notebook with an example of iterative solvers for nonlinear systems can be accessed online &lt;a href=&#34;https://djps.github.io/courses/numericalmethods24/notebooks/nonlinear/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/nonlinear.py&#34;&gt;here&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href=&#34;https://djps.github.io/ipyth/nonlinear.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Example open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-calculator fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Example:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;An &lt;code&gt;.ipynb&lt;/code&gt; notebook with an example using the secant method to find a local maxima can be accessed online &lt;a href=&#34;https://djps.github.io/courses/numericalmethods24/notebooks/max_finder/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/max_finder.py&#34;&gt;here&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href=&#34;https://djps.github.io/ipyth/max_finder.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h2 id=&#34;convergence&#34;&gt;Convergence&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;If a sequence $x_n$ converges to $r$ as $n \rightarrow \infty$, then it is said to &lt;strong&gt;converge linearly&lt;/strong&gt; if there exists a $\mu \in \left( 0,1\right)$ such that
$$
\lim\limits_{n\rightarrow\infty} \dfrac{\left| x_{n+1} - r \right|}{\left| x_{n} - r \right|} = \mu
$$
The sequences converges &lt;strong&gt;super-linearly&lt;/strong&gt; if
$$
\lim\limits_{n\rightarrow\infty} \dfrac{\left| x_{n+1} - r \right|}{\left| x_{n} - r \right|} = 0
$$
and &lt;strong&gt;sub-linearly&lt;/strong&gt; if
$$
\lim\limits_{n\rightarrow\infty} \dfrac{\left| x_{n+1} - r \right|}{\left| x_{n} - r \right|} = 1
$$
More generally, a sequence converges with order $q$ if there exists a ${\mu \gt 0}$ such that
$$
\lim\limits_{n\rightarrow\infty} \dfrac{\left| x_{n+1} - r \right|}{\left| x_{n} - r \right|^q } = \mu
$$
Thus a sequence is said to converge quadratically when ${q=2}$ and exhibit cubic convergence when $q=3$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Regularity&lt;/th&gt;
&lt;th&gt;Proximity to $r$&lt;/th&gt;
&lt;th&gt;Initial points&lt;/th&gt;
&lt;th&gt;Function calls&lt;/th&gt;
&lt;th&gt;Convergence&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Bisection&lt;/td&gt;
&lt;td&gt;$\mathcal{C}^{0}$&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Linear&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Newton&lt;/td&gt;
&lt;td&gt;$\mathcal{C}^{2}$&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;Quadratic&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Secant&lt;/td&gt;
&lt;td&gt;$\mathcal{C}^{2}$&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Superlinear&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;systems-of-nonlinear-equation&#34;&gt;Systems of Nonlinear Equation&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Multi-dimensional Newton Method&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;For a vector-valued function $f : \mathbb{R}^n \rightarrow \mathbb{R}^n$, which takes as an argument the vector
$$
x= \left( x_1, x_2 \ldots x_n \right)
$$
the &lt;strong&gt;Jacobian&lt;/strong&gt; matrix is defined as&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
J = \left(
\begin{array}{cccc}
\dfrac{\partial f_1 }{\partial x_1} &amp;amp; \dfrac{\partial f_1 }{\partial x_2} &amp;amp; \cdots &amp;amp; \dfrac{\partial f_1 }{\partial x_n} \\
\dfrac{\partial f_2 }{\partial x_1} &amp;amp; &amp;amp; &amp;amp; \\
\vdots &amp;amp; &amp;amp; &amp;amp; \vdots \\
\dfrac{\partial f_n }{\partial x_1} &amp;amp; \dfrac{\partial f_n }{\partial x_2} &amp;amp; \cdots &amp;amp; \dfrac{\partial f_n }{\partial x_n}
\end{array}
\right)
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;If the derivatives are evaluated at the vector $x$, the Jacobian matrix can be parameterised as $J\left(x\right)$. Newton&amp;rsquo;s method can be written as
$$
x_{n+1} = x_n - J^{-1} \left( x_n \right) f\left( x_n \right)
$$
where $J^{-1} \left( x_n \right)$ is the inverse of the Jacobian matrix evaluated at the vector $x_n$. In practice, as matrix inversion can be computationally expensive, the system
$$
J \left( x_n \right) \left( x_{n+1} - x_n \right) = -f \left( x_n \right)
$$
is solved for the unknown vector $x_{n+1} - x_n$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Interpolation</title>
      <link>https://djps.github.io/courses/numericalmethods24/part-2/interpolation/</link>
      <pubDate>Fri, 17 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalmethods24/part-2/interpolation/</guid>
      <description>&lt;style&gt;
mjx-container {
  display: inline-block;
}
mjx-assistive-mml {
  right: 0px;
  bottom: 0px;
}
&lt;/style&gt;
&lt;p style=&#34;font-size: 14px; letter-spacing: 0.03em; color: rgb(0,0,0,0.54);&#34;&gt; &lt;span style=&#34;font-weight: 700;&#34;&gt; David Sinden &lt;/span&gt; &amp;middot;
&lt;i class=&#34;far fa-calendar&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; March 11, 2024 &amp;middot;
&lt;i class=&#34;far fa-clock&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; 45 minute read &lt;/p&gt;
&lt;h2 id=&#34;interpolation&#34;&gt;Interpolation&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Given as set of points $p_0$, $\ldots$, ${p_n \in \mathbb{R}}$ and corresponding nodes $u_0$, $\ldots$, ${u_n \in \mathbb{R}}$, a function ${f : \mathbb{R} \rightarrow \mathbb{R}}$ with ${f(u_i) = p_i}$ is an &lt;strong&gt;interpolating&lt;/strong&gt; function.&lt;/p&gt;
&lt;p&gt;This can be generalised to higher dimensions, i.e. ${f : \mathbb{R} \rightarrow \mathbb{R}^N }$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;If the interpolating function is a polynomial, write can be written as
$$
p\left( u \right) = \sum\limits_{i=0}^{n} \alpha_i \varphi_i \left( u \right)
$$
So that for every $j$, the polynomial satisfies $p\left( u_j \right) = \sum\limits_{i=0}^{n} \alpha_i \varphi_i \left( u_j \right)$, thus the $\alpha_i$ lead to a linear system of the form
$$\Phi \alpha = p$$
where $p$ is the vector defined the polynomial evaluated at the node points, i.e. ${p=p\left( u_j \right)}$ and $\Phi$ is the &lt;strong&gt;collocation matrix&lt;/strong&gt;, given by
$$
\Phi = \left( \begin{array}{cccc}
\varphi_0\left(u_0\right) &amp;amp; \varphi_1\left(u_1\right) &amp;amp; \cdots &amp;amp; \varphi_n\left(u_n\right) \\
\vdots &amp;amp; &amp;amp; &amp;amp; \vdots \\
\varphi_0\left(u_n\right) &amp;amp; \cdots &amp;amp; \cdots &amp;amp; \varphi_n\left(u_n\right) \end{array} \right)
$$
Thus $ \alpha = \Phi^{-1} p$,&lt;/p&gt;
&lt;p&gt;The collocation matrix is invertible if and only if the set of functions $\varphi$ are linearly independent.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;!--
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;If $$p\left( u \right) = \sum\limits_{i=0}^{n} \alpha_i \varphi_i \left( u \right)$$
So that for every $j$, $p\left( u_j \right) = \sum\limits_{i=0}^{n} \alpha_i \varphi_i \left( u_j \right)$, thus the $\alpha_i$ lead to a linear system of the form
$$\Phi \alpha = p$$
where $\Phi$ is the &lt;strong&gt;Vandermonde matrix&lt;/strong&gt;.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
--&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Lagrange Polynomials&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;The &lt;strong&gt;Lagrange form of an interpolating polynomial&lt;/strong&gt; is given by
\begin{equation}
p \left( x\right) = \sum\limits_{i=0}^{n} \alpha_i l_i\left(x\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$l_{i} \in \mathbb{P}_{n}$
such that $l_{i}\left( x_{j} \right) = \delta_{ij}$. The polynomials $l_i\left(x\right) \in \mathbb{P}_n$ for $i=0, \ldots, n$, are called &lt;strong&gt;characteristic polynomials&lt;/strong&gt; and are given by
\begin{equation}
l_{i} \left( x \right) = \prod \limits_{\substack{j = 0,\newline{}j \ne i}}^{n} \dfrac{x - x_j}{x_i - x_j}. &lt;br&gt;
\end{equation}&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Example open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-calculator fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Example:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;An &lt;code&gt;.ipynb&lt;/code&gt; notebook with an example of Runge&amp;rsquo;s phenomenon can be accessed online &lt;a href=&#34;https://djps.github.io/courses/numericalmethods24/notebooks/runge/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/runge.py&#34;&gt;here&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href=&#34;https://djps.github.io/ipyth/runge.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;&lt;strong&gt;Aitken&amp;rsquo;s&lt;/strong&gt; algorithm is an iterative process for evaluating Lagrange interpolation polynomials without explicitly constructing them.&lt;/p&gt;
&lt;p&gt;$$
p\left( u \right) = \sum\limits_{i=0}^{n} p_{i}^{n} l_i^{n}\left( u \right)
$$&lt;/p&gt;
&lt;p&gt;this can be written as&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{array}{lllll}
l\left( u \right) = p_{0} &amp;amp;                     &amp;amp;                     &amp;amp;  \\
&amp;amp; p_0^1\left(u\right) &amp;amp;                     &amp;amp;  \\
l\left( u \right) = p_{1} &amp;amp;                     &amp;amp; l_0^1\left(u\right) &amp;amp;  \\
&amp;amp; p\left( u \right)   &amp;amp;                     &amp;amp; l_0^1\left(u\right) \\
l\left( u \right) = p_{2} &amp;amp;                     &amp;amp; l\left( u \right)   &amp;amp;  \\
&amp;amp; p\left( u \right)   &amp;amp;                     &amp;amp;  \\
l\left( u \right) = p_{3} &amp;amp;                     &amp;amp;                     &amp;amp;
\end{array}
\end{equation}
$$
where the coefficients are evaluated from left to right.&lt;/p&gt;
&lt;p&gt;The corresponding method for Newton&amp;rsquo;s form of the interpolating polynomial is called Neville&amp;rsquo;s algorithm.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;!-- &lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;polynomial’s convergence.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
--&gt;
&lt;h2 id=&#34;piecewise-polynomial-interpolation&#34;&gt;Piecewise Polynomial Interpolation&lt;/h2&gt;
&lt;h2 id=&#34;spline-interpolation&#34;&gt;Spline Interpolation&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A function $s\left(u\right)$ is called a &lt;strong&gt;spline&lt;/strong&gt; of degree $k$ on the domain ${[a, b]}$ if ${s \in C^{k-1}\left( [a, b] \right)}$ and there exists nodes ${a = u_0 \lt u_1 \lt \ldots \lt u_m = b}$ such that $s$ is a polynomial of degree $k$ for ${i = 0, \ldots m-1}$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;B-Splines&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;A spline is said to be a &lt;strong&gt;b-spline&lt;/strong&gt; if it is of the form&lt;/p&gt;
&lt;p&gt;$$
s\left(u\right) = \sum\limits_{i=0}^{m} \alpha_{i} \mathcal{N}_{i}^{n} \left(u \right)
$$&lt;/p&gt;
&lt;p&gt;where $\mathcal{N}^n$ are the &lt;strong&gt;basis spline functions&lt;/strong&gt; of degree $n$ with minimal support. (That is they are positive in the domain and zero outside). The functions are defined recursively. Let $u_i$ be the set of nodes ${u_0, u_1, \ldots, u_m}$, then&lt;/p&gt;
&lt;p&gt;$$
\mathcal{N}_{i}^{0} \left(u \right) =
\left\{
\begin{array}{ll}
1 &amp;amp; \quad \mbox{for} \quad u_i \le u \le u_{i+1} \\
0 &amp;amp; \quad \mbox{else.}
\end{array}
\right.
$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$
\mathcal{N}_{i}^{n} \left(u \right) = \alpha_i^{n-1}\left(u \right)  \mathcal{N}_{i}^{n-1} \left(u \right) + \left( 1 - \alpha_{i+1}^{n-1}\left(u \right)\right)  \mathcal{N}_{i+1}^{n-1} \left(u \right)
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
\alpha_{i}^{n-1}\left(u \right) = \dfrac{u - u_i}{u_{i+n} - u_i}
$$
is a local parameter.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;p&gt;Given data with nodes $u_i$ and values $p_i$, to interpolate with splines, of order $n$, requires solving&lt;/p&gt;
&lt;p&gt;$$
\mbox{Find} \quad s = \sum\limits_{i=0}^{m} \alpha_i \mathcal{N}_{i}^{n} \left( u\right) \quad \mbox{such that} \quad s\left(u_i \right) = p_i \quad \mbox{for} \quad i=0, \ldots, m
$$&lt;/p&gt;
&lt;p&gt;which is matrix form is ${\Phi \alpha = p}$, where the collocation matrix, $\Phi \in \mathbb{R}^{\left(m+1\right) \times \left(m+1\right)}$ is given by
$$
\Phi = \left(
\begin{array}{ccc}
\mathcal{N}_{0}^{n} \left(u_0\right) &amp;amp; \cdots &amp;amp; \mathcal{N}_{m}^{n} \left(u_0\right) \\
\vdots &amp;amp; &amp;amp; \vdots \\
\mathcal{N}_{0}^{n} \left(u_m\right) &amp;amp; \cdots &amp;amp; \mathcal{N}_{m}^{n} \left(u_m\right)
\end{array}
\right)
$$&lt;/p&gt;
&lt;h2 id=&#34;least-squares-approximation&#34;&gt;Least-Squares Approximation&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Least-Squares Approximation&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Given a set of points $y=\left( y_0, y_1, \ldots y_n \right)$ at nodes $x_0$, seek a continuous function of $x$, with a given form characterized by $m$ parameters $\beta=\left( \beta_0, \beta_1, \ldots, \beta_m \right)$, i.e. $f\left( x, \beta \right)$, which approximates the points while minimizing the error, defined by the sum of the squares
$$
E = \sum\limits_{i=0}^{n} \left( y - f\left(x_i, \beta \right) \right)^2.
$$
The minimum is found when
$$
\dfrac{\partial E}{\partial \beta_j} = 0 \quad \mbox{for all } \quad j=1, \ldots m
$$
i.e.
$$
-2 \sum\limits_{i=0}^{n} \left(y_i - f\left(x_i, \beta_j \right) \right) \dfrac{\partial f\left( x_i, \beta \right)}{\partial \beta_j} = 0 \quad \mbox{for all } \quad j=1, \ldots m.
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Linear Least-Squares Approximation&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;If the function $f$ is a function of the form
$$
y = \sum\limits_{j=1}^{m} \beta_j \varphi_j \left( x \right)
$$
then the least squares problem can be expressed as
$$
\dfrac{\partial E}{\partial \beta_j} = \sum\limits_{j=1}^m \left( \sum\limits_{i=1}^n \varphi_j\left( x_i \right) \varphi_k \left( x_i \right) \right).
$$
Thus, the weights $\beta$ can be determined by solving the &lt;em&gt;linear system&lt;/em&gt;,
$$
\Phi \Phi^T \beta = \Phi y,
$$
i.e. $\beta = \left( \Phi \Phi^T \right)^{-1} \Phi y$, where $\Phi$ is the collocation matrix.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Integration</title>
      <link>https://djps.github.io/courses/numericalmethods24/part-2/integration/</link>
      <pubDate>Fri, 17 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalmethods24/part-2/integration/</guid>
      <description>&lt;p style=&#34;font-size: 14px; letter-spacing: 0.03em; color: rgb(0,0,0,0.54);&#34;&gt; &lt;span style=&#34;font-weight: 700;&#34;&gt; David Sinden &lt;/span&gt; &amp;middot;
&lt;i class=&#34;far fa-calendar&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; March 11, 2024 &amp;middot;
&lt;i class=&#34;far fa-clock&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; 45 minute read &lt;/p&gt;
&lt;h2 id=&#34;numerical-differentiation&#34;&gt;Numerical Differentiation&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Finite-Difference Quotients&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Consider the approximations to the first-order derivative:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Forward Difference Quotient:&lt;/strong&gt;
\begin{equation}
D_{j}^{+} u = \dfrac{u_{j+1} - u_j}{h}
\end{equation}&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Backwards Difference Quotient:&lt;/strong&gt;
\begin{equation}
D_{j}^{-} u = \dfrac{u_{j} - u_{j-1}}{h}
\end{equation}&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Central Difference Quotient:&lt;/strong&gt;
\begin{equation}
D_{j}^{0} u = \dfrac{u_{j+1} - u_{j-1}}{2h}
\end{equation}&lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Richardson Extrapolation&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Here&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Higher Order Derivatives&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;with $f(x+h)$ and $f(x-h)$, so
$$
f^{\prime\prime}(x) = \dfrac{f(x-h) - 2f(x) + f(x+h)}{h^2} +\mathcal{O}\left(h^2\right)
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h2 id=&#34;numerical-integration&#34;&gt;Numerical Integration&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Riemann Sum&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Partition
Approximate the area under the curve by summing the rectangles
If $f$ is continuous, the value of $x_{i}^{\ast}$ may be chosen arbitrarily in the interval $\left[ x_i, x_{i+1} \right]$
Then the &lt;strong&gt;Lower&lt;/strong&gt; and &lt;strong&gt;Upper&lt;/strong&gt; sums are given by
$$
L\left(f, p\right) \le \int\limits_{a}^{b} f\left(x \right) , \mathrm{d}x \le U\left(f, p\right).
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Trapezoidal Rule&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Here&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Error for Trapezoidal Rule&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Here&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Algorithm open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-laptop-code fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Algorithm:	&lt;em&gt;Romberg Algorithm&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Here&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Example open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-calculator fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Example:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;An &lt;code&gt;.ipynb&lt;/code&gt; notebook with an example of numerical integration using the Trapezium rule can be accessed online &lt;a href=&#34;https://djps.github.io/courses/numericalmethods24/notebooks/integrals/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/integrals.py&#34;&gt;here&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href=&#34;https://djps.github.io/ipyth/integrals.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Finite Difference Methods for Differential Equations</title>
      <link>https://djps.github.io/courses/numericalmethods24/part-3/odes/</link>
      <pubDate>Fri, 17 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalmethods24/part-3/odes/</guid>
      <description>&lt;p style=&#34;font-size: 14px; letter-spacing: 0.03em; color: rgb(0,0,0,0.54);&#34;&gt; &lt;span style=&#34;font-weight: 700;&#34;&gt; David Sinden &lt;/span&gt; &amp;middot;
&lt;i class=&#34;far fa-calendar&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; March 11, 2024 &amp;middot;
&lt;i class=&#34;far fa-clock&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; 45 minute read &lt;/p&gt;
&lt;h2 id=&#34;finite-difference-methods-for-differential-equations&#34;&gt;Finite Difference Methods for Differential Equations&lt;/h2&gt;
&lt;p&gt;Solutions are functions.&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Ordinary Differential Equations&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;An &lt;strong&gt;ordinary differential equation&lt;/strong&gt; (ODE) is an equation that involves one of more derivatives of a function of a single variable.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;!-- For example, with only the first derivative $y^{\prime} \left(t\right) = f\left( y\left(t\right), t)$  --&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Initial Value Problems&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;An &lt;strong&gt;initial value problem&lt;/strong&gt; (IVP) is given by an ordinary differential equation of the form ${y^{\prime} \left(t\right) = f\left( y\left(t\right), t\right)}$ and initial value ${y\left(a\right)=y_a}$ for the unknown function $y\left(t\right)$.&lt;/p&gt;
&lt;p&gt;Often ${a=0}$, so the initial condition reads ${y(0)=y_0}$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;One step methods&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A numerical method for approximating the solution to a differential equation is called a &lt;strong&gt;one step method&lt;/strong&gt; if the solution at time step ${i+1}$ depends only on the previous one.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;forward-euler-method&#34;&gt;Forward Euler Method&lt;/h3&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;strong&gt;Forward Euler&lt;/strong&gt; approximates the derivative through a the first-order forward difference approximation of the first-order derivative
$$
u_{n+1} = u_n + h f_n
$$
where ${f_{n+1} = f\left( u_{n}, t_{n} \right)}$. The error is $\mathcal{O}\left( h^2 \right)$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;backward-euler-method&#34;&gt;Backward Euler Method&lt;/h3&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;strong&gt;Backward Euler&lt;/strong&gt; uses the backward finite difference approximation of the first-order derivative
$$
u_{n+1} = u_n + h f_{n+1}
$$
where ${f_{n+1} = f\left( u_{n+1}, t_{n+1} \right)}$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;crank-nicolson-method&#34;&gt;Crank-Nicolson Method&lt;/h3&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;The &lt;strong&gt;Crank-Nicolson method&lt;/strong&gt; is given by
$$
u_{n+1} = u_n + \dfrac{h}{2}\left( f_n + f_{n+1} \right)
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;heuns-method&#34;&gt;Heun&amp;rsquo;s Method&lt;/h3&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;strong&gt;Heun’s method&lt;/strong&gt;
$$
u_{n+1} = u_n + \dfrac{h}{2}\left( f_n + f_{n+1} \right)
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;p&gt;Both the Forward Euler and Heun&amp;rsquo;s method are explicit. Backward Euler and Crank-Nicolson are implicit.&lt;/p&gt;
&lt;p&gt;Alternatively,
$$
y\left( t + h \right) = y\left( t \right) + \int_{t}^{t+h} f\left( y\left(\tau\right), \tau \right) \, \mathrm{d}\tau
$$&lt;/p&gt;
&lt;p&gt;Then, the Forward Euler method is the left Riemann sum, Backward Euler is the right Riemann sum and the Crank-Nicolson is the trapezoidal rule.&lt;/p&gt;
&lt;div class=&#34;details admonition Example open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-calculator fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Example:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;An &lt;code&gt;.ipynb&lt;/code&gt; notebook with an example of solvers for odes can be accessed online &lt;a href=&#34;https://djps.github.io/courses/numericalmethods24/notebooks/ode_solvers/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/ode_solvers.py&#34;&gt;here&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href=&#34;https://djps.github.io/ipyth/ode_solvers.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h2 id=&#34;analysis-of-one-step-methods&#34;&gt;Analysis of One-Step Methods&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;A numerical method is said to be &lt;strong&gt;explicit&lt;/strong&gt; if an approximation $y_{k+1}$ can be calculated directly from already computed values $y_i$, $i&amp;lt;k$. Otherwise, the method is said to be &lt;strong&gt;implicit&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Often, implicit methods require, at each step, the solution of a nonlinear equation for computing $y_{k+1}$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A function $f$ is &lt;strong&gt;Holder continuous&lt;/strong&gt; if there exists real constants ${C \gt 0}$ and ${\alpha \le 0}$ such that
$$
\left| f\left(x\right) - f\left(y\right) \right| \le  C \Vert x - y\Vert^\alpha
$$
for all $x$ and $y$. If ${\alpha=1}$ the function is said to be &lt;strong&gt;Lipshitz continuous&lt;/strong&gt;.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;!--
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Stable&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;stable&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;


&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Consistent&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Let $y\left( t\right)$ be the solution to an initial value problem and ${h_{\mathrm{max}} = \max\limits_{x_k} h_k}$
A method is said to be &lt;strong&gt;consistent&lt;/strong&gt; if the function $f$ satisfies the Lipshitz condition, with respect to $y$, in an interval then
$$
\lim\limits_{h_{\mathrm{max}}} \left( \max\limits_{x_k \in I_h} \left| f\left(t_k, y\left(t_k\right) \right) - \Phi\left(t_k, y\left(t_k\right), h_k \right) \right| \right) = 0
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
--&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Order&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A one-step method is of order $p \in \mathbb{N}$, if for all ${t \in \left[ 0, \tau \right]}$, the solution satisfies the condition that ${\tau\left(h\right) = \mathcal{O}\left( h^p \right) }$ as ${h \rightarrow \infty}$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Zero Stable Methods&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;A method of the form
$$
u_{n+1} = u_n + h \Phi\left( t_n, u_n, f_n, h \right)
$$
is called &lt;strong&gt;zero-stable&lt;/strong&gt; if there exists both a maximal step size, $h_0$ and a constant, $C$, such that for all
${h \in \left[0, h_0\right]}$ and for ${\varepsilon \gt 0}$, then the following holds:&lt;/p&gt;
&lt;p&gt;If, for all time-steps ${0 \le n \le N}$, there exists a ${\delta_n \le \varepsilon}$ and
$$
z_{n+1} = z_n + h \Phi\left( t_n, z_n, f_n\left(z_n, t_n\right), h \right) + \delta_{n+1}
$$
and $z_0 = {y_0 +\delta_0}$, then
$$
\left| z_n - u_n \right| \le C \varepsilon \quad \mbox{for} \quad 0 \le n \le N.
$$&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;p&gt;This means that small perturbations in the computations lead to small perturbations in the approximations.&lt;/p&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;If increment function is Lipshitz continuous for $y_n$ for any $h$ and $t_n$, then the one step method is zero-stable.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;If the increment function $\Phi$ is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lipshitz continuous for $y_n$ for any $h$ and $t_{n+1}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the method is consistent&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;then
$$
\lim \limits_{h \rightarrow 0} \left| y_n - u_n \right| =0.
$$
Also, if the method is of order $p$ and if ${\left| y_0 - u_0 \right| = \mathcal{O}\left(h^p\right)}$ as ${h\rightarrow 0}$, the convergence is of order $p$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Absolute Stability&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A numerical scheme for approximating the solution to the linear differential equation $y^{\prime}\left(t\right) = \lambda y\left(t\right)$ with ${\lambda \in \mathbb{C}}$ and initial condition ${y_0 = 1}$ is said to be &lt;strong&gt;absolutely stable&lt;/strong&gt; if $\left|u_n\right| \rightarrow 0$ as $n \rightarrow \infty$, when ${\operatorname{Re}\left(\lambda\right) \lt 0}$, for a fixed value of $h$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;runge-kutta-schemes-and-multi-step-schemes&#34;&gt;Runge-Kutta Schemes And Multi-Step Schemes&lt;/h3&gt;
&lt;h2 id=&#34;partial-differential-equations&#34;&gt;Partial Differential Equations&lt;/h2&gt;
</description>
    </item>
    
  </channel>
</rss>
