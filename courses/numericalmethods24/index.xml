<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>JTMS-MAT-13: Numerical Methods | David Sinden</title>
    <link>https://djps.github.io/courses/numericalmethods24/</link>
      <atom:link href="https://djps.github.io/courses/numericalmethods24/index.xml" rel="self" type="application/rss+xml" />
    <description>JTMS-MAT-13: Numerical Methods</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 14 Nov 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://djps.github.io/media/logo_hud7cfbe45e4df55bd5c7c181ea654d8f2_56982_300x300_fit_lanczos_3.png</url>
      <title>JTMS-MAT-13: Numerical Methods</title>
      <link>https://djps.github.io/courses/numericalmethods24/</link>
    </image>
    
    <item>
      <title>Taylor Series</title>
      <link>https://djps.github.io/courses/numericalmethods24/part-1/taylor/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalmethods24/part-1/taylor/</guid>
      <description>&lt;!--
&lt;p style=&#34;font-size: 14px; letter-spacing: 0.03em; color: rgb(0,0,0,0.54); border-top: 1px solid #e8e8e8; margin-top: 30px; padding-top: 10px; border-bottom: 1px solid #e8e8e8; margin-bottom: 30px; padding-bottom: 10px;&#34;&gt; &lt;span style=&#34;font-weight: 700;&#34;&gt; David Sinden &lt;/span&gt; &amp;middot;
&lt;i class=&#34;far fa-calendar&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; March 11, 2022 &amp;middot;
&lt;i class=&#34;far fa-clock&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; 45 minute read &lt;/p&gt;
--&gt;
&lt;p style=&#34;font-size: 14px; letter-spacing: 0.03em; color: rgb(0,0,0,0.54);&#34;&gt; &lt;span style=&#34;font-weight: 700;&#34;&gt; David Sinden &lt;/span&gt; &amp;middot;
&lt;i class=&#34;far fa-calendar&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; December 07, 2023 &amp;middot;
&lt;i class=&#34;far fa-clock&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; 15 minute read &lt;/p&gt;
&lt;h2 id=&#34;taylor-series&#34;&gt;Taylor Series&lt;/h2&gt;
&lt;p&gt;The Taylor series, or Taylor expansion of a function, is defined as&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Taylor Series&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;For a function $f : \mathbb{R} \mapsto \mathbb{R}$ which is infinitely differentiable at a point $c$, the Taylor series of $f(c)$ is given by
\begin{equation}
\sum\limits_{k=0}^{\infty} \dfrac{ f^{(k)} \left( c \right) }{k!} \left( x - c \right)^{k}.
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;p&gt;This is a power series, which is convergent for some radius.&lt;/p&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Taylor&amp;#39;s Theorem&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;For a function $f \in C^{n+1}\left([a, b]\right)$, i.e. $f$ is $(n+1)$-times continuously differentiable in the interval $[a, b]$, then for some $c$ in the interval, the function can be written as
\begin{equation}
f\left( x \right) = \sum\limits_{k=0}^{n} \dfrac{f^{(k)} \left(c\right) }{k!} \left( x- c \right)^{k} + \dfrac{f^{(n+1)} \left( \xi \right) }{\left( n + 1 \right)!} \left( x - c \right)^{n+1}
\end{equation}
for some value $\xi \in \left[ a, b \right]$ where
\begin{equation}
\lim\limits_{\xi \rightarrow c} \dfrac{ f^{(n+1)} \left( \xi \right) }{ \left( n + 1 \right)!} \left( x - c \right)^{n+1} = 0.
\end{equation}&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Example open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-calculator fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Example:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;An &lt;code&gt;.ipynb&lt;/code&gt; notebook with an example of the Taylor series for $\sin\left(x\right)$ can be accessed online &lt;a href=&#34;https://djps.github.io/courses/numericalmethods24/notebooks/taylor_series/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/TaylorSeries.py&#34;&gt;here&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href=&#34;https://djps.github.io/ipyth/TaylorSeries.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Number Representations</title>
      <link>https://djps.github.io/courses/numericalmethods24/part-1/number-representations/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalmethods24/part-1/number-representations/</guid>
      <description>&lt;style&gt;
mjx-container {
  display: inline-block;
}
mjx-assistive-mml {
  right: 0px;
  bottom: 0px;
}
&lt;/style&gt;
&lt;!--
&lt;p style=&#34;font-size: 14px; letter-spacing: 0.03em; color: rgb(0,0,0,0.54); border-top: 1px solid #e8e8e8; margin-top: 30px; padding-top: 10px; border-bottom: 1px solid #e8e8e8; margin-bottom: 30px; padding-bottom: 10px;&#34;&gt; &lt;span style=&#34;font-weight: 700;&#34;&gt; David Sinden &lt;/span&gt; &amp;middot;
&lt;i class=&#34;far fa-calendar&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; March 11, 2022 &amp;middot;
&lt;i class=&#34;far fa-clock&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; 45 minute read &lt;/p&gt;
--&gt;
&lt;p style=&#34;font-size: 14px; letter-spacing: 0.03em; color: rgb(0,0,0,0.54);&#34;&gt; &lt;span style=&#34;font-weight: 700;&#34;&gt; David Sinden &lt;/span&gt; &amp;middot;
&lt;i class=&#34;far fa-calendar&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; January 05, 2024 &amp;middot;
&lt;i class=&#34;far fa-clock&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; 25 minute read &lt;/p&gt;
&lt;h2 id=&#34;errors&#34;&gt;Errors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Errors in data&lt;/li&gt;
&lt;li&gt;Round-off errors&lt;/li&gt;
&lt;li&gt;Truncation errors&lt;/li&gt;
&lt;/ul&gt;
&lt;!--  $\left| \dfrac{\tilde{a}-a}{a} \right|$  --&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Absolute and Relative Errors&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Let $\tilde{a}$ be an approximation to $a$, then the &lt;strong&gt;absolute error&lt;/strong&gt; is given by $\left| {\tilde{a}-a} \right|$.&lt;/p&gt;
&lt;p&gt;If ${a \ne 0}$, the &lt;strong&gt;relative error&lt;/strong&gt; is given by $\left| \dfrac{\tilde{a}-a}{a} \right|$ and the error bound is the magnitude of the admissible error.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;For both addition and subtraction the bounds for the absolute error are added.&lt;/p&gt;
&lt;p&gt;In division and multiplication the bounds for the relative errors are added.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Linear Sensitivity to Uncertainties&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;If $y(x)$ is a smooth function, i.e. is differentiable, then $\left| y^{\prime} \right|$ can be interpreted as the &lt;strong&gt;linear sensitivity&lt;/strong&gt; of $y(x)$ to uncertainties in $x$.&lt;/p&gt;
&lt;p&gt;For functions of several variables, i.e. $f : \mathbb{R}^n \rightarrow \mathbb{R}$, then
$$
\left| \Delta y \right| \le \sum\limits_{i}^{}\left| \dfrac{\partial y}{\partial x_i} \right|\left| \Delta x_i \right|
$$
where
$\left| \Delta x_i \right| = \tilde{x}_i - x_i$ for an approximation $\tilde{x}_i$, thus  ${\left| \Delta y_i \right| = \tilde{y}_i - y_i = f \left( \tilde{x}_i \right) - \left( x_i \right)}$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;!-- Numerical methods are a lot of computational schemes to solve mathematical problems when analytical solutions can&#39;t be found.

How does the representation of a number in computer memory affect calculations? --&gt;
&lt;div class=&#34;details admonition Example open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-calculator fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Example:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;An &lt;code&gt;.ipynb&lt;/code&gt; notebook with an example of the Taylor series for $\sin\left(x\right)$ can be accessed online &lt;a href=&#34;https://djps.github.io/courses/numericalmethods24/notebooks/number_representations/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/number_representations.py&#34;&gt;here&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href=&#34;https://djps.github.io/ipyth/number_representations.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h2 id=&#34;number-representations&#34;&gt;Number Representations&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Base Representation&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Let $b \in \mathbb{N} \backslash \lbrace 1 \rbrace$. Every number $x \in \mathbb{N}_0$
can be written as a unique expansion with respect to base $b$ as&lt;/p&gt;
&lt;p&gt;\begin{equation}
\left(x\right)_b = a_0 b^0 + a_1 b^1 + \ldots a_n b^n = \sum \limits_{i=0}^{n} a_i b^i
\end{equation}&lt;/p&gt;
&lt;p&gt;A number can be written in a nested form:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\left(x\right)_b &amp;amp; = a_0 b^0 + a_1 b^1 + \ldots a_n b^n \\
&amp;amp; = a_0 + b\left( a_1 + b\left( a_2 + b\left( a_3 + \ldots + b a_n\right) \right. \ldots  \right)
\end{align}
$$&lt;/p&gt;
&lt;p&gt;with $a_i &amp;lt; \mathbb{N}_0$ and $a_i &amp;lt; b$, i.e. ${a_i \in \lbrace 0, \ldots, b-1 \rbrace }$.&lt;/p&gt;
&lt;p&gt;For a real number, ${x \in \mathbb{R}}$, write&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
x &amp;amp; = \sum \limits_{i=0}^{n} a_i b^i + \sum \limits_{i=1}^{\infty} \alpha_i b^{-i} \\
&amp;amp; = a_n  \cdots a_0 . \alpha_1 \alpha_2 \ldots
\end{align}
$$&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Algorithm open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-laptop-code fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Algorithm:	&lt;em&gt;Euclid&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Euclid&amp;rsquo;s algorithm can convert number $x$ in base 10, i.e.
$\left(x \right)_{10}$
into another base, $b$, i.e. $\left(x \right)_{b}$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Input $\left(x \right)_{10}$&lt;/li&gt;
&lt;li&gt;Determine the smallest integer $n$ such that ${x &amp;lt; b^{n+1}}$&lt;/li&gt;
&lt;li&gt;Let $y=x$. Then for $i=n, \ldots, 0$
$$
\begin{array}{rcl}
a_i &amp;amp; = &amp;amp; y \mbox{ div } b^i \\
y &amp;amp; = &amp;amp; y \mbox{ mod } b^i
\end{array}
$$
which at each steps provides an $a_i$ and updates $y$.&lt;/li&gt;
&lt;li&gt;Output as $\left(x \right)_{b} = a_n a_{n-1} \cdots a_0$&lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Algorithm open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-laptop-code fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Algorithm:	&lt;em&gt;Horner&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;ol&gt;
&lt;li&gt;Input $\left(x \right)_{10}$&lt;/li&gt;
&lt;li&gt;Set $i=0$&lt;/li&gt;
&lt;li&gt;Let $y=x$. Then while $y&amp;gt;0$
$$
\begin{array}{rcl}
a_i &amp;amp; = &amp;amp; y \mbox{ div } b \\
y &amp;amp; = &amp;amp; y \mbox{ mod } b \\
i &amp;amp; = &amp;amp; i+1
\end{array}
$$
which at each steps provides an $a_i$ and updates $y$.&lt;/li&gt;
&lt;li&gt;Output as $\left(x \right)_{b} = a_n a_{n-1} \cdots a_0$&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Normalized Floating Point Representations&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;em&gt;Normalized&lt;/em&gt; floating point representations with respect to some base $b$, store a number $x$ as
$$x= 0 \cdot a_1 \ldots a_k \times b^n$$
where the $a_i \in \lbrace 0, 1, \ldots b-1 \rbrace$ are called the &lt;strong&gt;digits&lt;/strong&gt;, $k$ is the &lt;strong&gt;precision&lt;/strong&gt; and $n$ is the &lt;strong&gt;exponent&lt;/strong&gt;. The set $a_1, \ldots, a_k$ is called the &lt;strong&gt;mantissa&lt;/strong&gt;. Impose that $a_1 \ne 0$, it makes the representation unique.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Let $x$ and $y$ be two normalized floating point numbers with ${x &amp;gt; y &amp;gt; 0}$ and base ${b=2}$. If there exists integers $p$ and ${q \in \mathbb{N}_0}$ such that
$$2^{-p} \leq 1 - \dfrac{y}{x} \leq 2^{-q}$$
then, at most $p$ and at least $q$ significant bits (i.e. significant figures written in base 2) are lost during subtraction.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Equations</title>
      <link>https://djps.github.io/courses/numericalmethods24/part-1/linear-equations/</link>
      <pubDate>Mon, 20 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalmethods24/part-1/linear-equations/</guid>
      <description>&lt;h2 id=&#34;linear-equations&#34;&gt;Linear Equations&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Systems of Linear Equations&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;A system of linear equations (or linear system) is a collection of one or more linear equations involving the same variables. If there are $m$ equations with $n$ unknown variables to solve for&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
a_{1,1} x_1 + a_{1,2} x_2 + \ldots + a_{1,n} x_n &amp;amp; = b_1 \\
a_{2,1} x_1 + a_{2,2} x_2 + \cdots + a_{2,n} x_n &amp;amp; = b_2 \\
\vdots &amp;amp;  \\
a_{m,1} x_1 + a_{m,2} x_2 + \cdots + a_{m,n} x_n &amp;amp; = b_m
\end{align}
$$&lt;/p&gt;
&lt;p&gt;The system of linear equations can be written in matrix form ${Ax=b}$, where
$$
A = \left(
\begin{array}{cccc}
a_{1,1} &amp;amp; a_{1,2} &amp;amp; \cdots &amp;amp; a_{1,n} \\
a_{2,1} &amp;amp; a_{2,2} &amp;amp; \cdots &amp;amp; a_{2,n} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
a_{m,1} &amp;amp; a_{m,2} &amp;amp; \cdots &amp;amp; a_{m,n}
\end{array}
\right), \quad x = \left(
\begin{array}{c}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{array}
\right), \quad b = \left(
\begin{array}{c}
b_1 \\
b_2 \\
\vdots \\
b_m
\end{array}
\right),
$$
so that $A \in \mathbb{R}^{m \times n}$, $x \in \mathbb{R}^n$ and $b \in \mathbb{R}^m$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Banded Systems&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A &lt;strong&gt;banded&lt;/strong&gt; matrix is a matrix whose non-zero entries are confined to a diagonal band, comprising the main diagonal and zero or more diagonals on either side.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Symmetric Matrices&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;A square matrix $A$ is &lt;strong&gt;symmetric&lt;/strong&gt; if ${A=A^T}$, that is, $a_{i,j}=a_{j,i}$ for all indices $i$ and $j$.&lt;/p&gt;
&lt;p&gt;A square matrix is said to be &lt;strong&gt;Hermitian&lt;/strong&gt; if the matrix is equal to its conjugate transpose, i.e. $a_{i,j}=\overline{a_{j,i}}$ for all indices $i$ and $j$. A Hermitian matrix is written as $A^H$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Positive Definite Matrices&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A matrix, $M$, is said to be &lt;strong&gt;positive definite&lt;/strong&gt; if it is symmetric (or Hermitian) and all its eigenvalues are real and positive.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Nonsingular Matrices&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A matrix is &lt;strong&gt;non-singular&lt;/strong&gt; or &lt;strong&gt;invertible&lt;/strong&gt; if there exists a matrix $A^{-1}$ such that ${A^{-1}A = A A^{-1} = I}$, where $I$ is the identity matrix.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Note open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-pencil-alt fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Note:	&lt;em&gt;Properties of Nonsingular Matrices&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;For a nonsingular matrix, the following all hold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nonsingular matrix has full rank&lt;/li&gt;
&lt;li&gt;A square matrix is nonsingular if and only if the determinant of the matrix is non-zero.&lt;/li&gt;
&lt;li&gt;If a matrix is singular, both versions of Gaussian elimination will fail due to division by zero, yielding a floating exception error.&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;If $\tilde{x}$ is an approximate solution to the linear problem ${Ax=b}$, then the &lt;strong&gt;residual&lt;/strong&gt; is defined as ${r = A \tilde{x}-b}$.&lt;/p&gt;
&lt;p&gt;If $\left| r \right|$ is large due to rounding, the matrix is said to be &lt;strong&gt;ill-conditioned&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;direct-methods&#34;&gt;Direct Methods&lt;/h3&gt;
&lt;div class=&#34;details admonition Algorithm open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-laptop-code fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Algorithm:	&lt;em&gt;Gaussian Elimination&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Here&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Algorithm open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-laptop-code fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Algorithm:	&lt;em&gt;Gaussian Elimination with Scaled Partial Pivoting&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Here&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;$LU$-Decomposition&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Here&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;$LU$-Decomposition&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Here&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Cholesky-Decomposition&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Here&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Algorithm open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-laptop-code fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Algorithm:	&lt;em&gt;Cholesky&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Here&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;indirect-methods&#34;&gt;Indirect Methods&lt;/h3&gt;
&lt;p&gt;$Q x_{k +1}= \left(Q-A \right) x_{k} +Q^{-1} b$.&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Richardson Iteration&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Here $Q=I$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Jacobi&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Here $Q=D$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Gauss-Seidel&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;$Q=L$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Successive Over Relaxation&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;$Q=L + \dfrac{1}{\omega}D$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nonlinear Equations</title>
      <link>https://djps.github.io/courses/numericalmethods24/part-1/nonlinear-equations/</link>
      <pubDate>Fri, 17 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalmethods24/part-1/nonlinear-equations/</guid>
      <description>&lt;p style=&#34;font-size: 14px; letter-spacing: 0.03em; color: rgb(0,0,0,0.54);&#34;&gt; &lt;span style=&#34;font-weight: 700;&#34;&gt; David Sinden &lt;/span&gt; &amp;middot;
&lt;i class=&#34;far fa-calendar&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; March 11, 2024 &amp;middot;
&lt;i class=&#34;far fa-clock&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; 45 minute read &lt;/p&gt;
&lt;h2 id=&#34;root-finding&#34;&gt;Root Finding&lt;/h2&gt;
&lt;h3 id=&#34;bisection-method&#34;&gt;Bisection Method&lt;/h3&gt;
&lt;h3 id=&#34;newtons-method&#34;&gt;Newton&amp;rsquo;s Method&lt;/h3&gt;
&lt;h3 id=&#34;secant-methods&#34;&gt;Secant Methods&lt;/h3&gt;
&lt;h2 id=&#34;convergence&#34;&gt;Convergence&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Order of convergence: cubic, quadratic, linear.
Super-linearly, sub-linear,&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Regularity&lt;/th&gt;
&lt;th&gt;Proximity to $r$&lt;/th&gt;
&lt;th&gt;func calls&lt;/th&gt;
&lt;th&gt;Convergence&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Bisection&lt;/td&gt;
&lt;td&gt;$\mathcal{C}^{0}$&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Newton&lt;/td&gt;
&lt;td&gt;$\mathcal{C}^{2}$&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Secant&lt;/td&gt;
&lt;td&gt;$\mathcal{C}^{2}$&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;systems-of-nonlinear-equation&#34;&gt;Systems of Nonlinear Equation&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Jacobian&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;here.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Interpolation</title>
      <link>https://djps.github.io/courses/numericalmethods24/part-2/interpolation/</link>
      <pubDate>Fri, 17 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalmethods24/part-2/interpolation/</guid>
      <description>&lt;style&gt;
mjx-container {
  display: inline-block;
}
mjx-assistive-mml {
  right: 0px;
  bottom: 0px;
}
&lt;/style&gt;
&lt;p style=&#34;font-size: 14px; letter-spacing: 0.03em; color: rgb(0,0,0,0.54);&#34;&gt; &lt;span style=&#34;font-weight: 700;&#34;&gt; David Sinden &lt;/span&gt; &amp;middot;
&lt;i class=&#34;far fa-calendar&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; March 11, 2024 &amp;middot;
&lt;i class=&#34;far fa-clock&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; 45 minute read &lt;/p&gt;
&lt;h2 id=&#34;interpolation&#34;&gt;Interpolation&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Given as set of points $p_0$, $\ldots$, ${p_n \in \mathbb{R}}$ and corresponding nodes $u_0$, $\ldots$, ${u_n \in \mathbb{R}}$, a function ${f : \mathbb{R} \rightarrow \mathbb{R}}$ with ${f(u_i) = p_i}$ is an &lt;strong&gt;interpolating&lt;/strong&gt; function.&lt;/p&gt;
&lt;p&gt;This can be generalised to higher dimensions, i.e. ${f : \mathbb{R} \rightarrow \mathbb{R}^N }$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;If the interpolating function is a polynomial, write can be written as
$$
p\left( u \right) = \sum\limits_{i=0}^{n} \alpha_i \varphi_i \left( u \right)
$$
So that for every $j$, the polynomial satisfies $p\left( u_j \right) = \sum\limits_{i=0}^{n} \alpha_i \varphi_i \left( u_j \right)$, thus the $\alpha_i$ lead to a linear system of the form
$$\Phi \alpha = p$$
where $p$ is the vector defined the polynomial evaluated at the node points, i.e. ${p=p\left( u_j \right)}$ and $\Phi$ is the &lt;strong&gt;collocation matrix&lt;/strong&gt;, given by
$$
\Phi = \left( \begin{array}{cccc}
\varphi_0\left(u_0\right) &amp;amp; \varphi_1\left(u_1\right) &amp;amp; \cdots &amp;amp; \varphi_n\left(u_n\right) \\
\vdots &amp;amp; &amp;amp; &amp;amp; \vdots \\
\varphi_0\left(u_n\right) &amp;amp; \cdots &amp;amp; \cdots &amp;amp; \varphi_n\left(u_n\right) \end{array} \right)
$$
Thus $ \alpha = \Phi^{-1} p$,&lt;/p&gt;
&lt;p&gt;The collocation matrix is invertible if and only if the set of functions $\varphi$ are linearly independent.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;!--
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;If $$p\left( u \right) = \sum\limits_{i=0}^{n} \alpha_i \varphi_i \left( u \right)$$
So that for every $j$, $p\left( u_j \right) = \sum\limits_{i=0}^{n} \alpha_i \varphi_i \left( u_j \right)$, thus the $\alpha_i$ lead to a linear system of the form
$$\Phi \alpha = p$$
where $\Phi$ is the &lt;strong&gt;Vandermonde matrix&lt;/strong&gt;.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
--&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Lagrange Polynomials&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;The &lt;strong&gt;Lagrange form of an interpolating polynomial&lt;/strong&gt; is given by
\begin{equation}
p \left( x\right) = \sum\limits_{i=0}^{n} \alpha_i l_i\left(x\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$l_{i} \in \mathbb{P}_{n}$
such that $l_{i}\left( x_{j} \right) = \delta_{ij}$. The polynomials $l_i\left(x\right) \in \mathbb{P}_n$ for $i=0, \ldots, n$, are called &lt;strong&gt;characteristic polynomials&lt;/strong&gt; and are given by
\begin{equation}
l_{i} \left( x \right) = \prod \limits_{\substack{j = 0,\newline{}j \ne i}}^{n} \dfrac{x - x_j}{x_i - x_j}. &lt;br&gt;
\end{equation}&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;&lt;strong&gt;Aitken&amp;rsquo;s&lt;/strong&gt; (or Neville&amp;rsquo;s) algorithm is an iterative process for evaluating Lagrange interpolation polynomials without explicitly constructing them.&lt;/p&gt;
&lt;p&gt;$$
p\left( u \right) = \sum\limits_{i=0}^{n} p_{i}^{n} l_i^{n}\left( u \right)
$$&lt;/p&gt;
&lt;p&gt;this can be written as&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{array}{lllll}
l\left( u \right) = p_{0} &amp;amp;                     &amp;amp;                     &amp;amp;                     &amp;amp; \\
&amp;amp; p_0^1\left(u\right) &amp;amp;                     &amp;amp;                     &amp;amp; \\
l\left( u \right) = p_{1} &amp;amp;                     &amp;amp; l_0^1\left(u\right) &amp;amp;                     &amp;amp; \\
&amp;amp; p\left( u \right)   &amp;amp;                     &amp;amp; l_0^1\left(u\right) &amp;amp; \\
l\left( u \right) = p_{2} &amp;amp;                     &amp;amp; l\left( u \right)   &amp;amp;                     &amp;amp; l_0^1\left(u\right) \\
&amp;amp; p\left( u \right)   &amp;amp;                     &amp;amp; l_0^1\left(u\right) &amp;amp; \\
l\left( u \right) = p_{3} &amp;amp;                     &amp;amp;                     &amp;amp;                     &amp;amp;
\end{array}
\end{equation}
$$&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;polynomial’s are all the same.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h2 id=&#34;piecewise-polynomial-interpolation&#34;&gt;Piecewise Polynomial Interpolation&lt;/h2&gt;
&lt;h2 id=&#34;spline-interpolation&#34;&gt;Spline Interpolation&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A function $s\left(u\right)$ is called a &lt;strong&gt;spline&lt;/strong&gt; of degree $k$ on the domain $[a,b]$ if ${s \in C^{k-1}\left( [a, b] \right)}$ and there exists nodes ${a = u_0 \lt u_1 \lt \ldots \lt u_m = b}$ such that $s$ is a polynomial of degree $k$ for ${i=0, \ldots m-1}$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;B-Splines&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;A spline is said to be a &lt;strong&gt;b-spline&lt;/strong&gt; if it is of the form&lt;/p&gt;
&lt;p&gt;$$
s\left(u\right) = \sum\limits_{i=0}^{m} \alpha_{i} \mathcal{N}_{i}^{n} \left(u \right)
$$&lt;/p&gt;
&lt;p&gt;where $\mathcal{N}^n$ are the &lt;strong&gt;basis spline functions&lt;/strong&gt; of degree $n$ with minimal support. (That is they are positive in the domain and zero outside). The functions are defined recursively. Let $u_i$ be the set of nodes ${u_0, u_1, \ldots, u_m}$, then&lt;/p&gt;
&lt;p&gt;$$
\mathcal{N}_{i}^{0} \left(u \right) =
\left\{
\begin{array}{ll}
1 &amp;amp; \quad \mbox{for} \quad u_i \le u \le u_{i+1} \\
0 &amp;amp; \quad \mbox{else.}
\end{array}
\right.
$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$
\mathcal{N}_{i}^{n} \left(u \right) = \alpha_i^{n-1}\left(u \right)  \mathcal{N}_{i}^{n-1} \left(u \right) + \left( 1 - \alpha_{i+1}^{n-1}\left(u \right)\right)  \mathcal{N}_{i+1}^{n-1} \left(u \right)
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
\alpha_{i}^{n-1}\left(u \right) = \dfrac{u - u_i}{u_{i+n} - u_i}
$$
is a local parameter.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;p&gt;Given data with nodes $u_i$ and values $p_i$, to interpolate with splines, of order $n$, requires solving&lt;/p&gt;
&lt;p&gt;$$
\mbox{Find} \quad s = \sum\limits_{i=0}^{m} \alpha_i \mathcal{N}_{i}^{n} \left( u\right) \quad \mbox{such that} \quad s\left(u_i \right) = p_i \quad \mbox{for} \quad i=0, \ldots, m
$$&lt;/p&gt;
&lt;p&gt;which is matrix form is ${\Phi \alpha = p}$, where the collocation matrix, $\Phi \in \mathbb{R}^{\left(m+1\right) \times \left(m+1\right)}$ is given by
$$
\Phi = \left(
\begin{array}{ccc}
\mathcal{N}_{0}^{n} \left(u_0\right) &amp;amp; \cdots &amp;amp; \mathcal{N}_{m}^{n} \left(u_0\right) \\
\vdots &amp;amp; &amp;amp; \vdots \\
\mathcal{N}_{0}^{n} \left(u_m\right) &amp;amp; \cdots &amp;amp; \mathcal{N}_{m}^{n} \left(u_m\right)
\end{array}
\right)
$$&lt;/p&gt;
&lt;h2 id=&#34;least-squares-interpolation&#34;&gt;Least-Squares Interpolation&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Integration</title>
      <link>https://djps.github.io/courses/numericalmethods24/part-2/integration/</link>
      <pubDate>Fri, 17 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalmethods24/part-2/integration/</guid>
      <description>&lt;p style=&#34;font-size: 14px; letter-spacing: 0.03em; color: rgb(0,0,0,0.54);&#34;&gt; &lt;span style=&#34;font-weight: 700;&#34;&gt; David Sinden &lt;/span&gt; &amp;middot;
&lt;i class=&#34;far fa-calendar&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; March 11, 2024 &amp;middot;
&lt;i class=&#34;far fa-clock&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; 45 minute read &lt;/p&gt;
&lt;h2 id=&#34;numerical-differentiation&#34;&gt;Numerical Differentiation&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Finite-Difference Quotients&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Consider the approximations to the first-order derivative:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Forward Difference Quotient:&lt;/strong&gt;
\begin{equation}
D_{j}^{+} u = \dfrac{u_{j+1} - u_j}{h}
\end{equation}&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Backwards Difference Quotient:&lt;/strong&gt;
\begin{equation}
D_{j}^{-} u = \dfrac{u_{j} - u_{j-1}}{h}
\end{equation}&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Central Difference Quotient:&lt;/strong&gt;
\begin{equation}
D_{j}^{0} u = \dfrac{u_{j+1} - u_{j-1}}{2h}
\end{equation}&lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Richardson Extrapolation&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Here&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Higher Order Derivatives&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;with $f(x+h)$ and $f(x-h)$, so
$$
f^{\prime\prime}(x) = \dfrac{f(x-h) - 2f(x) + f(x+h)}{h^2} +\mathcal{O}\left(h^2\right)
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h2 id=&#34;numerical-integration&#34;&gt;Numerical Integration&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Riemann Sum&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Partition
Approximate the area under the curve by summing the rectangles
If $f$ is continuous, the value of $x_{i}^{\ast}$ may be choosen arbitrarily in the interval $\left[ x_i, x_{i+1} \right]$
Then the &lt;strong&gt;Lower&lt;/strong&gt; and &lt;strong&gt;Upper&lt;/strong&gt; sums are given by
$$
L\left(f, p\right) \le \int\limits_{a}^{b} f\left(x \right) , \mathrm{d}x \le U\left(f, p\right).
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Trapezoidal Rule&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Here&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;Error for Trapezoidal Rule&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Here&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Algorithm open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-laptop-code fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Algorithm:	&lt;em&gt;Romberg Algorithm&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Here&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Finite Difference Methods for Differential Equations</title>
      <link>https://djps.github.io/courses/numericalmethods24/part-3/odes/</link>
      <pubDate>Fri, 17 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/courses/numericalmethods24/part-3/odes/</guid>
      <description>&lt;p style=&#34;font-size: 14px; letter-spacing: 0.03em; color: rgb(0,0,0,0.54);&#34;&gt; &lt;span style=&#34;font-weight: 700;&#34;&gt; David Sinden &lt;/span&gt; &amp;middot;
&lt;i class=&#34;far fa-calendar&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; March 11, 2024 &amp;middot;
&lt;i class=&#34;far fa-clock&#34; aria-hidden=&#34;true&#34; style=&#34;color: rgb(0,0,0,0.54);&#34;&gt;&lt;/i&gt; 45 minute read &lt;/p&gt;
&lt;h2 id=&#34;finite-difference-methods-for-differential-equations&#34;&gt;Finite Difference Methods for Differential Equations&lt;/h2&gt;
&lt;p&gt;Solutions are functions.&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Ordinary Differential Equations&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;An &lt;strong&gt;ordinary differential equation&lt;/strong&gt; (ODE) is an equation that involves one of more derivatives of a function of a single variable.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;!-- For example, with only the first derivative $y^{\prime} \left(t\right) = f\left( y\left(t\right), t)$  --&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Initial Value Problems&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;An &lt;strong&gt;initial value problem&lt;/strong&gt; (IVP) is given by an ordinary differential equation of the form ${y^{\prime} \left(t\right) = f\left( y\left(t\right), t\right)}$ and initial value ${y\left(a\right)=y_a}$ for the unknown function $y\left(t\right)$.&lt;/p&gt;
&lt;p&gt;Often ${a=0}$, so the initial condition reads ${y(0)=y_0}$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;One step methods&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A numerical method for approximating the solution to a differential equation is called a &lt;strong&gt;one step method&lt;/strong&gt; if the solution at time step ${i+1}$ depends only on the previous one.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;forward-euler-method&#34;&gt;Forward Euler Method&lt;/h3&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;strong&gt;Forward Euler&lt;/strong&gt; approximates the derivative through a the first-order forward difference approximation of the first-order derivative
$$
u_{n+1} = u_n + h f_n
$$
where ${f_{n+1} = f\left( u_{n}, t_{n} \right)}$. The error is $\mathcal{O}\left( h^2 \right)$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;backward-euler-method&#34;&gt;Backward Euler Method&lt;/h3&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;strong&gt;Backward Euler&lt;/strong&gt; uses the backward finite difference approximation of the first-order derivative
$$
u_{n+1} = u_n + h f_{n+1}
$$
where ${f_{n+1} = f\left( u_{n+1}, t_{n+1} \right)}$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;crank-nicolson-method&#34;&gt;Crank-Nicolson Method&lt;/h3&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;The &lt;strong&gt;Crank-Nicolson method&lt;/strong&gt; is given by
$$
u_{n+1} = u_n + \dfrac{h}{2}\left( f_n + f_{n+1} \right)
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;heuns-method&#34;&gt;Heun&amp;rsquo;s Method&lt;/h3&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;strong&gt;Heun’s method&lt;/strong&gt;
$$
u_{n+1} = u_n + \dfrac{h}{2}\left( f_n + f_{n+1} \right)
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;p&gt;Both the Forward Euler and Heun&amp;rsquo;s method are explicit. Backward Euler and Crank-Nicolson are implicit.&lt;/p&gt;
&lt;p&gt;Alternatively,
$$
y\left( t + h \right) = y\left( t \right) + \int_{t}^{t+h} f\left( y\left(\tau\right), \tau \right) \, \mathrm{d}\tau
$$&lt;/p&gt;
&lt;p&gt;Then, the Forward Euler method is the left Reimann sum, Backward Euler is the right Riemann sum and the Crank-Nicolson is the trapezoidal rule.&lt;/p&gt;
&lt;h2 id=&#34;analysis-of-one-step-methods&#34;&gt;Analysis of One-Step Methods&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;A numerical method is said to be &lt;strong&gt;explicit&lt;/strong&gt; if an approximation $y_{k+1}$ can be calculated directly from already computed values $y_i$, $i&amp;lt;k$. Otherwise, the method is said to be &lt;strong&gt;implicit&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Often, implicit methods require, at each step, the solution of a nonlinear equation for computing $y_{k+1}$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A function $f$ is &lt;strong&gt;Holder continuous&lt;/strong&gt; if there exists real constants ${C \gt 0}$ and ${\alpha \le 0}$ such that
$$
\left| f\left(x\right) - f\left(y\right) \right| \le  C \Vert x - y\Vert^\alpha
$$
for all $x$ and $y$. If ${\alpha=1}$ the function is said to be &lt;strong&gt;Lipshitz continuous&lt;/strong&gt;.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;!-- 
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Stable&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;stable&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;


&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Consistent&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;Let $y\left( t\right)$ be the solution to an initial value problem and ${h_{\mathrm{max}} = \max\limits_{x_k} h_k}$
A method is said to be &lt;strong&gt;consistent&lt;/strong&gt; if the function $f$ satisfies the Lipshitz condition, with respect to $y$, in an interval then
$$
\lim\limits_{h_{\mathrm{max}}} \left( \max\limits_{x_k \in I_h} \left| f\left(t_k, y\left(t_k\right) \right) - \Phi\left(t_k, y\left(t_k\right), h_k \right) \right| \right) = 0
$$&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
--&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Order&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A one-step method is of order $p \in \mathbb{N}$, if for all ${t \in \left[ 0, \tau \right]}$, the solution satisfies the condition that ${\tau\left(h\right) = \mathcal{O}\left( h^p \right) }$ as ${h \rightarrow \infty}$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Zero Stable Methods&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;A method of the form
$$
u_{n+1} = u_n + h \Phi\left( t_n, u_n, f_n, h \right)
$$
is called &lt;strong&gt;zero-stable&lt;/strong&gt; if there exists both a maximal step size, $h_0$ and a constant, $C$, such that for all
${h \in \left[0, h_0\right]}$ and for ${\varepsilon \gt 0}$, then the following holds:&lt;/p&gt;
&lt;p&gt;If, for all time-steps ${0 \le n \le N}$, there exists a ${\delta_n \le \varepsilon}$ and
$$
z_{n+1} = z_n + h \Phi\left( t_n, z_n, f_n\left(z_n, t_n\right), h \right) + \delta_{n+1}
$$
and $z_0 = {y_0 +\delta_0}$, then
$$
\left| z_n - u_n \right| \le C \varepsilon \quad \mbox{for} \quad 0 \le n \le N.
$$&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;p&gt;This means that small perturbations in the computations lead to small perturbations in the approximations.&lt;/p&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;If increment function is Lipshitz continuous for $y_n$ for any $h$ and $t_n$, then the one step method is zero-stable.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-list-ul fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Theorem:	&lt;em&gt;&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;If the increment function $\Phi$ is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lipshitz continuous for $y_n$ for any $h$ and $t_{n+1}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the method is consistent&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;then
$$
\lim \limits_{h \rightarrow 0} \left| y_n - u_n \right| =0.
$$
Also, if the method is of order $p$ and if ${\left| y_0 - u_0 \right| = \mathcal{O}\left(h^p\right)}$ as ${h\rightarrow 0}$, the convergence is of order $p$.&lt;/p&gt;
&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
        &lt;div class=&#34;details-summary admonition-title&#34;&gt;
            &lt;i class=&#34;icon fas fa-info-circle fa-fw&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
			Definition:	&lt;em&gt;Absolute Stability&lt;/em&gt;
			
        &lt;/div&gt;
        &lt;div class=&#34;details-content&#34;&gt;
            &lt;div class=&#34;admonition-content&#34;&gt;A numerical scheme for approximating the solution to the linear differential equation $y^{\prime}\left(t\right) = \lambda y\left(t\right)$ with ${\lambda \in \mathbb{C}}$ and initial condition ${y_0 = 1}$ is said to be &lt;strong&gt;absolutely stable&lt;/strong&gt; if $\left|u_n\right| \rightarrow 0$ as $n \rightarrow \infty$, when ${\operatorname{Re}\left(\lambda\right) \lt 0}$, for a fixed value of $h$.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;h3 id=&#34;runge-kutta-schemes-and-multi-step-schemes&#34;&gt;Runge-Kutta Schemes And Multi-Step Schemes&lt;/h3&gt;
&lt;h2 id=&#34;partial-differential-equations&#34;&gt;Partial Differential Equations&lt;/h2&gt;
</description>
    </item>
    
  </channel>
</rss>
