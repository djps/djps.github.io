[{"authors":null,"categories":null,"content":"David Sinden is a senior scientist at Fraunhofer Institute for Digital Medicine Mevis, in Bremen. His research interests include ultrasound modelling, thermal ablation and pharmacokinetic models. He works within the modelling and simulation and image-guided therapy groups.\n","date":1655683200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1655683200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"David Sinden is a senior scientist at Fraunhofer Institute for Digital Medicine Mevis, in Bremen. His research interests include ultrasound modelling, thermal ablation and pharmacokinetic models. He works within the modelling and simulation and image-guided therapy groups.","tags":null,"title":"David Sinden","type":"authors"},{"authors":null,"categories":null,"content":"Numerical Analysis CA-S-MATH-804 Lectures Lectures take place: 11:15-12:30 Tuesday Thursday (East Hall 8) and Fridays (East Hall 4), from 1 February until 13 May 2022.\nContent and Educational Aims The module is an introduction to the analysis of basic classes of numerical algorithms used in large-scale scientific computing. It introduces the fundamental notions and concepts of numerical mathematics. Then, successively, iterative solvers, interpolation, and quadrature are discussed and analyzed. They serve as the core numerical building blocks for an introduction to the finite element method (FEM) as one of the modern numerical techniques widely used in engineering applications and theoretical physics.\nText Books “Numerical Mathematics” - Quarteroni, Sacco and Saleri (2007) “A First Course in the Numerical Analysis of Differential Equations” - Iserles (2012) The following is also useful:\n“An Introduction to the Conjugate Gradient Method Without the Agonizing Pain” - Shewchuk (1994). Also see [here “An Introduction to Computational Stochastic PDEs” - Lord, Powell and Shardlow (2014). [Chapters 1 \u0026amp; 2] Outline The following topics will be covered:\nPart 1: Foundations Principles of numerical mathematics: well-posedness, stability, robustness, condition, and consistency Equivalence theorem of Lax-Richtmyer for exemplary problems Types of error analysis (forward, backward, a priori, and a posteriori) Sources of errors (modelling, data, discretization, rounding, and truncation) Foundations of matrix analysis: vector norms and matrix norms and compatible/consistent norms Stability analysis for linear systems: the condition number of a matrix, forward/backward a priori analysis, and the convergence of iterative methods Iterative methods: gradient descent and conjugate gradient method Part 2: Interpolation and Numerical Integration Review of Lagrange interpolation, error estimates, drawbacks, Runge’s counterexample the stability of polynomial interpolation, piecewise Lagrange interpolation, and extensions to the multi-dimensional case Quadrature formulas: interpolatory quadrature, error estimates, Gauss quadrature, degree of exactness, and extensions to the multi-dimensional case Part 3: Numerical Solutions of Differential Equations Finite difference methods (FDM), stability and convergence analysis for FDM and error estimates for FDM The notion of a weak solution The Galerkin method and the Finite Element Method (FEM) Error estimates for FEM: Céa’s lemma and approximate estimates Particular examples in 1D and 2D and linear and quadratic shape functions Assessment Examination Type: Module Examination\nAssessment Type: Written examination\nDuration: 120 min\nWeight: 100%\nScope: All intended learning outcomes of this module\nBy submitting homework you can improve this grade by up to 0.66 points. More details about homework will be announced in class.\n","date":1610064000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1610150400,"objectID":"197a80802ed09249031aa21f1f393a2e","permalink":"https://djps.github.io/courses/numericalanalysis22/","publishdate":"2021-01-08T00:00:00Z","relpermalink":"/courses/numericalanalysis22/","section":"courses","summary":"Numerical Analysis CA-S-MATH-804","tags":null,"title":"Numerical Analysis","type":"book"},{"authors":null,"categories":null,"content":"Assignments Assignment 1 Assignment 2 Assignment 3 Assignment 4 Assignment 5 Assignment 6 ","date":1610064000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1610150400,"objectID":"e01c8d28dfe456f729679587345061f4","permalink":"https://djps.github.io/courses/numericalanalysis22/assignments/","publishdate":"2021-01-08T00:00:00Z","relpermalink":"/courses/numericalanalysis22/assignments/","section":"courses","summary":"Assignments Numerical Analysis CMATH-802","tags":null,"title":"Numerical Analysis","type":"book"},{"authors":null,"categories":null,"content":"A collection of jupyter notebooks, referenced in the lecture notes, can be found here:\nLagrange Interpolation Lagrange Polynomials Numerical Integration One-dimensional finite element simulations Two-dimensional finite element simulations ","date":1655424000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1655424000,"objectID":"37eb826f66eb937104975ea8bc41d93e","permalink":"https://djps.github.io/courses/numericalanalysis22/notebooks/","publishdate":"2022-06-17T00:00:00Z","relpermalink":"/courses/numericalanalysis22/notebooks/","section":"courses","summary":"Notebooks","tags":null,"title":"Python Notebooks","type":"book"},{"authors":null,"categories":null,"content":"Find $x$ such that $F(x,d)=0$ for a set of data, $d$ and $F$, a functional relationship between $x$ and $d$.\nWell Posed Problems Definition:\tWell-Posed Problems A problem is said to be well-posed if\na solution exists, the solution is unique, the solution’s behaviour changes continuously with the initial conditions. A problem which does not have these properties is said to be ill-posed.\nCondition Number Definition:\tRelative and Absolute Condition Numbers The relative condition number of a problem is given by: \\begin{equation} K ( d ) = \\sup\\limits_{\\delta d \\in \\mathcal{D}} \\dfrac{ \\left\\Vert \\delta x \\right\\Vert / \\left\\Vert x \\right\\Vert}{\\left\\Vert \\delta d \\right\\Vert / \\left\\Vert d \\right\\Vert} \\end{equation} if either $x=0$ or $d=0$, the absolute condition number is then \\begin{equation} K_{\\mathrm{abs}} ( d ) = \\sup\\limits_{\\delta d \\in \\mathcal{D}}\\dfrac{ \\left\\Vert \\delta x \\right\\Vert }{\\left\\Vert \\delta d \\right\\Vert} \\end{equation} Stability Consider a well-posed problem, a construct a sequence of approximate solutions via a sequence of approximate solutions and data, i.e. $F_n (x_n, d_n)=0$\nDefinition:\tConsistency If the $d$ is admissible for $F_n$ then a numerical method $F_n (x_n, d_n)=0$ is consistent if \\begin{equation} \\lim\\limits_{n\\rightarrow\\infty} F_n (x,d) \\rightarrow F(x,d). \\end{equation} The method is strongly consistent if $F_n(x,d)=0$ for all $n\\ge0$. Definition:\tAsymptotic and Relative Condition Numbers If the sets of functions for $F_n (x_n, d_n)=0$ and $F(x,d)=0$ coincide, that is \\begin{equation} K_n \\left( d_n \\right) = \\sup\\limits_{\\delta d_n \\in \\mathcal{D}_n } \\dfrac{ \\left\\Vert \\delta x_n \\right\\Vert / \\left\\Vert x_n \\right\\Vert }{\\left\\Vert \\delta d_n \\right\\Vert / \\left\\Vert d_n \\right\\Vert} \\end{equation} and\n$$K_{n,\\mathrm{abs}} \\left( d_n \\right) = \\sup \\limits_{\\delta d_n \\in \\mathcal{D}_n} \\dfrac{ \\left\\Vert \\delta x_n \\right\\Vert }{ \\left\\Vert \\delta d_n \\right\\Vert }$$\nthen the asymptotic condition number is\n$$K^{\\mathrm{num}} (d) = \\lim \\limits_{k\\rightarrow \\infty} \\sup\\limits_{n \\le k} K_n \\left( d_n \\right).$$\nThe relative condition number is:\n\\begin{equation} K_{\\mathrm{abs}}^{\\mathrm{num}} \\left( d \\right) = \\lim \\limits_{k\\rightarrow \\infty} \\sup \\limits_{n \\le k} K_{n, \\mathrm{abs}} \\left( d_n \\right). \\end{equation}\nDefinition:\tConvergence A method is convergent if and only if: \\begin{equation} \\forall \\varepsilon \u0026gt; 0, \\quad \\exists , n \\quad \\mbox{such that} \\quad \\left\\Vert x(d) - x_n\\left( d+ \\delta d_n \\right) \\right\\Vert \\le \\varepsilon \\end{equation} Theorem:\tLax-Ritchmyer A numerical algorithm converges if and only if it is consistent and stable. Matrix Analysis Theorem:\tLet $A \\in \\mathbb{R}^{n \\times n}$, then\n$\\lim\\limits_{k \\rightarrow \\infty}A^k =0 \\Leftrightarrow \\rho \\left( A \\right) \u0026lt; 1$. Where $\\rho\\left(A \\right)$ is the largest absolute value of the eigenvalues of $A$. This is called the spectral radius\nThe geometric series, $\\sum\\limits_{k=0}^{\\infty}A^k$ is convergent if and only if $\\rho \\left( A \\right) \u0026lt; 1$. Then in this case, the sum is given by \\begin{equation} \\sum\\limits_{k=0}^{\\infty}A^k = \\left( I - A \\right)^{-1} \\end{equation}\nThus, if $\\rho \\left( A \\right) \u0026lt; 1$, the matrix $I-A$ is invertible and \\begin{equation} \\dfrac{1}{1+\\left\\Vert A \\right\\Vert} \\le \\left\\Vert\\left( I - A \\right)^{-1}\\right\\Vert \\le \\dfrac{1}{1-\\left\\Vert A \\right\\Vert} \\end{equation} where $\\left\\Vert \\cdot \\right\\Vert$ is an induced matrix norm such that $\\left\\Vert A \\right\\Vert \u0026lt;1$.\nTheorem:\tLet $A \\in \\mathbb{R}^{n \\times n}$ be non-singular and let $\\delta A \\in \\mathbb{R}^{n \\times n}$ be such that $\\left\\Vert A^{-1} \\right\\Vert \\bigl\\Vert \\delta A \\bigr\\Vert \u0026lt; 1$. Furthermore, if $x \\in \\mathbb{R}^n$ is a solution to $Ax=b$, where $b \\in \\mathbb{R}^n$ and $b \\neq 0$ and $\\delta x$ is such that \\begin{equation} \\left( A + \\delta A \\right)\\left( x + \\delta x \\right) = b + \\delta b \\end{equation} for a $\\delta b \\in \\mathbb{R}^n$, then \\begin{equation} \\left( A + \\delta A \\right)\\left( x + \\delta x \\right) \\le \\dfrac{K(A)}{1- K(A) \\left\\Vert \\delta A \\right\\Vert_2 / \\left\\Vert A \\right\\Vert_2} \\left(\\dfrac{\\left\\Vert \\delta b \\right\\Vert_2}{\\left\\Vert b \\right\\Vert_2} + \\dfrac{\\left\\Vert \\delta A \\right\\Vert_2}{\\left\\Vert A \\right\\Vert_2} \\right) \\end{equation} Theorem:\tLet $A \\in \\mathbb{R}^{n \\times n}$ be non-singular and if $x \\in \\mathbb{R}^n$ is a solution to $Ax=b$, where $b \\in \\mathbb{R}^n$ and $b \\neq 0$ and $\\delta x$ is such that \\begin{equation} A \\left( x + \\delta x \\right) = b + \\delta b \\end{equation} then \\begin{equation} \\dfrac{1}{K(A)} \\dfrac{\\left\\Vert \\delta b \\right\\Vert}{\\left\\Vert b \\right\\Vert} \\le \\dfrac{\\left\\Vert \\delta x \\right\\Vert}{\\left\\Vert x \\right\\Vert} \\le K(A) \\dfrac{\\left\\Vert \\delta b \\right\\Vert}{\\left\\Vert b \\right\\Vert} \\end{equation} Theorem:\tFor $A \\in \\mathbb{R}^{n \\times n}$ and $b \\in \\mathbb{R}^{n}$, assume $\\left\\Vert \\delta A \\right\\Vert \\le \\gamma \\left\\Vert A \\right\\Vert$ and $\\left\\Vert \\delta …","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"68160d27fe9d9e48930b5d4d273109ac","permalink":"https://djps.github.io/courses/numericalanalysis22/part-1/principles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalanalysis22/part-1/principles/","section":"courses","summary":"Find $x$ such that $F(x,d)=0$ for a set of data, $d$ and $F$, a functional relationship between $x$ and $d$.\nWell Posed Problems Definition:\tWell-Posed Problems A problem is said to be well-posed if","tags":null,"title":"Principles of Numerical Mathematics","type":"book"},{"authors":null,"categories":null,"content":"Quantifying Liver Perfusion-Function Relationship in Complex Resection – A Systems Medicine Approach The goal of the research unit is to better predict the recovery of hepatic function during regeneration after liver surgery of diseased livers.\nHepatic disease affect hepatic processes at different spatial scales: individual hepatocytes, hepatic sinusoids/lobuli (including zonated phenomena), the organ, and the entire organism. Surgery and subsequent regeneration further perturb the hepatic processes at the different scales.\nIn QuaLiPerF, these effects will be included in computational models of the metabolization of selected test compounds representative for liver function.\nThe goal of this project is to build a multi-scale computational model of test compound metabolism during liver regeneration. For this purpose, this project will integrate models at the cellular, lobular, and organism scale (by the modeling partners) with own models at the organ scale, in a four- scale framework. Combining the relevant complex and interacting processes in a computational model will require a deep understanding of the underlying physiology, biology, and biochemistry and of suitable modeling techniques for the respective spatial scales. Our mechanistically driven model translation and integration will be complemented by a data-driven approach in collaboration with the data integration partner. The computational models to be developed will be parameterized and validated using clinical and experimental physiological and metabolic data.\nIn addition, this project will combine the data acquired during regeneration to descriptive computational model of the recovery of physiological and metabolic parameters over time, to be combined with the models of test compound metabolization. Predictions obtained from simulations using the computational models will be used in QuaLiPerF to investigate biological and clinical questions driven by the clinical and experimental partners. Ultimately, a better prediction of the recovery of liver function developed in QuaLiPerF will help improve planning of individual surgery.\nProject Homepage The project homepage can be accessed here.\n","date":1655337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655337600,"objectID":"ec4cbceb7cc0c7570d0a3e8f811cca87","permalink":"https://djps.github.io/project/qualiperf/","publishdate":"2022-06-16T00:00:00Z","relpermalink":"/project/qualiperf/","section":"project","summary":"Quantifying Liver Perfusion-Function Relationship in Complex Resection – A Systems Medicine Approach","tags":["Multiscale Modeling"],"title":"QualiPerF","type":"project"},{"authors":null,"categories":null,"content":"Construct a scheme which solves the linear system $Ax=b$ by generating a sequence ${ x^{(n)} }$ which approximates the solution, $x$, that is \\begin{equation} \\lim_{n\\rightarrow \\infty} x^{(n)} = x. \\nonumber \\end{equation} So that $x = A^{-1} b$. Split the matrix $A=P-N$ and solve \\begin{equation} P x^{(n+1)} = B x^{(n)} + f, \\nonumber \\end{equation}\nwhere $P$ is called a preconditioner and $B=P^{-1}N$ is the iteration matrix.\nAn equivalent way is to write \\begin{equation} x^{(k+1)} = x^{(k)} + P^{-1}r^{(k)} \\nonumber \\end{equation} where \\begin{equation} r^{(k)} = b - A x^{(k)} \\end{equation} is the residual.\nDefinition:\tConsistency An iterative method is consistent if $x=Bx +f$, or equivalently, \\begin{equation} f = (I-B) A^{-1} b \\end{equation} Theorem:\tIf an iterative scheme is consistent, then if and only if $\\rho \\left( B \\right) \u0026lt; 1$ the method will converge for any initial guess $x^{(0)}$. Definition:\tStationary Methods The formulation can be written as \\begin{equation} x^{(0)} = F^{(0)} \\left( A, b \\right) \\quad \\mbox{and} \\quad x^{(k+1)} = F^{(k+1)} \\left( x^{(k)}, x^{(k-1)}, \\ldots, x^{(0)}, A, b \\right) . \\end{equation}\nIf the functions $F^{(k)}$ are independent of the number of iterations, then it is said to be stationary.\nJacobi Method The Jacobi method decomposes the matrix $A$ into diagonal, lower and upper triangular matrices $A=D+L+U$, and solves \\begin{equation} D x^{(n+1)} = -(L + U) x^{(n)} + b . \\label{eq:jacobi_method} \\end{equation}\nElement-wise this is \\begin{equation} x_i^{(k+1)} = \\dfrac{1}{a_{ii}} \\left( b_i - \\sum\\limits_{\\substack{j=1,\\newline{}j \\neq i}}^n a_{ij} x_{j}^{(k)} \\right) . \\nonumber \\end{equation}\nThus, the iterative scheme is \\begin{equation} x^{(n+1)} = -D^{-1}(L + U) x^{(n)} + D^{-1} b. \\nonumber \\end{equation}\nAs $L+U = A-D$, so the iteration matrix can be written as $B=I - D^{-1}A$.\nOver-Relaxation of Jacobi Method Also called the weighted Jacobi method. Introduce $\\omega$ to solve \\begin{equation} x_i^{(k+1)} = \\dfrac{ \\omega }{ a_{ii} } \\left( b_i - \\sum\\limits_{\\substack{j=1,\\newline{}j \\neq i}}^n a_{ij} x_{j}^{(k)} \\right) + \\left( 1 - \\omega \\right) x^{(k)}. \\label{eq:JOR} \\end{equation}\nSuccessive Over-Relaxation Introduce $\\omega$ to solve \\begin{equation} \\left(D + \\omega L \\right) x^{(n+1)} = -( (\\omega-1)D + \\omega U) x^{(n)} + \\omega b. \\label{eq:SOR} \\end{equation}\nGauss-Seidel The Gauss-Seidel method decomposes the matrix $A$ into diagonal, lower and upper triangular matrices $A=D+L+U$, and solves \\begin{equation} (D + L) x^{(n+1)} = -U x^{(n)} + b \\label{eq:GS} \\end{equation}\nTheorem:\tIf $A$ is strictly diagonally dominant by rows, i.e. $|a_{ii}| \u0026gt; \\sum_{j \\ne i} |a_{ij}|$, the Jacobi and Gauss-Seidel methods are convergent.\nIf $A$ and $2D-A$ are symmetric and positive definite, then the Jacobi method is convergent and the spectral radius of the iteration matrix $B$ is equal to \\begin{equation} \\rho \\left( B \\right) = \\left\\Vert B \\right\\Vert_{A} = \\left\\Vert B \\right\\Vert_{D} \\nonumber \\end{equation} where $\\left\\Vert \\cdot \\right\\Vert_{A}$ is an energy norm which is induced by the vector norm $\\left| x \\right|_{A} = \\sqrt{ x \\cdot A x}$\nIf and only if $A$ is symmetric and positive definite, the Jacobi over-relaxation method is convergent if \\begin{equation} 0 \u0026lt; \\omega \u0026lt; \\dfrac{2}{\\rho\\left( D^{-1}A \\right) }. \\nonumber \\end{equation}\nIf and only if $A$ is symmetric and positive definite, the Gauss-Seidel method is monotonically convergent with respect to the energy norm $\\left\\Vert \\cdot \\right\\Vert_{A}$.\nTheorem:\tFor any $\\omega \\in \\mathbb{R}$ we have $\\rho \\left( B \\left( \\omega \\right) \\right) \\ge |\\omega - 1|$. Thus, SOR does not converge if either $\\omega \\le 0$ or $\\omega \\ge 2$. Theorem:\tOstrowski IF $A$ is symmetric and positive definite, then the SOR method is convergent if and only if $0 \u0026lt; \\omega \u0026lt; 2$. Furthermore, the convergence is monotonic with respect to the energy norm $\\left\\Vert \\cdot \\right\\Vert_{A}$. Gradient Descent Consider the function $\\Phi \\left( y \\right) , : , \\mathbb{R}^{n} \\mapsto \\mathbb{R}$ which takes the form: \\begin{equation} \\Phi \\left( y\\right) = \\dfrac{1}{2} y \\cdot A y - y \\cdot b. \\end{equation} It can be shown that solving $Ax=b$ is equivalent to minimizing $\\Phi$.\nIf $x$ is a solution to the linear system and minimizes $\\Phi(x)$ then $\\nabla \\Phi(x) = 0$, so that $Ax-b = \\nabla \\Phi(x) =0$.\nNow \\begin{align} \\Phi(y) \u0026amp; = \\Phi(x + (y-x) ) \\newline \u0026amp; = \\Phi(x) + \\dfrac{1}{2} \\left\\Vert y - x\\right\\Vert_{A}^{2}. \\end{align}\nGradient descent seeks to construct a scheme which updates the vector $x^{(k)}$ according to \\begin{equation} x^{(k+1)} = x^{(k)} + \\alpha^{(k)} d^{(k)} \\end{equation} where $d^{(k)}$ is the update direction and $\\alpha^{(k)}$ is the step size at the $k$-th iterate.\nNote that in contrast to the methods above, the gradient descent method is non-stationary as values $d$ and $\\alpha$ change at every iterate.\nThe idea is to let the search direction be the gradient of the function $\\Phi$ …","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"dffc935dd77297ad888198caa8ddccf4","permalink":"https://djps.github.io/courses/numericalanalysis22/part-1/iterative-methods/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalanalysis22/part-1/iterative-methods/","section":"courses","summary":"Construct a scheme which solves the linear system $Ax=b$ by generating a sequence ${ x^{(n)} }$ which approximates the solution, $x$, that is \\begin{equation} \\lim_{n\\rightarrow \\infty} x^{(n)} = x. \\nonumber \\end{equation} So that $x = A^{-1} b$.","tags":null,"title":"Iterative Methods","type":"book"},{"authors":null,"categories":null,"content":"Numerical treatment of problems often involves the process of discretization - i.e. going from a continuous function to set of discrete points.\nInterpolation provides a way of approximating continuous functions by discrete data. Types of functions which can be used are:\nPolynomial interpolation : using a polynomial to approximate the data, Trigonometric interpolation: using polynomials of trigonometric functions, Spline interpolation: using a set of piecewise polynomials over subintervals of the data. Theorem:\tGiven $n+1$ distinct points $x_0, x_1, \\ldots, x_n$ and $n+1$ corresponding values $y_0, y_1, \\ldots, y_n$ there exists a unique polynomial $\\Pi_n \\in \\mathbb{P}_n$ such that for all $i=0, \\ldots, n$ \\begin{equation} \\Pi_n\\left( x_i \\right) = y_i. \\end{equation} Lagrange Interpolation Definition:\tLagrange Polynomials The Lagrange form of an interpolating polynomial is given by \\begin{equation} \\Pi_n \\left( x\\right) = \\sum\\limits_{i=0}^{n} y_i l_i\\left(x\\right) \\end{equation}\nwhere $l_{i} \\in \\mathbb{P}_{n}$ such that $l_{i}\\left( x_{j} \\right) = \\delta_{ij}$. The polynomials $l_i\\left(x\\right) \\in \\mathbb{P}_n$ for $i=0, \\ldots, n$, are called characteristic polynomials and are given by \\begin{equation} l_{i} \\left( x \\right) = \\prod \\limits_{\\substack{j = 0,\\newline{}j \\ne i}}^{n} \\dfrac{x - x_j}{x_i - x_j}. \\end{equation}\nexample:\tAn .ipynb notebook illustrating the Lagrange polynomials can be accessed here. It can be downloaded from here.\nAn .ipynb notebook illustrating Runge phenomenon can be accessed here. It can be downloaded from here.\nTheorem:\tLet $x_0, x_1, \\ldots x_n$ be $n+1$ distinct nodes and let $x$ be a point belonging to the domain of a given function $f$.\nLet $I_x$ be the smallest interval containing the nodes $x_0, x_1, \\ldots x_n$ and $x$ and assume that $f \\in C^{n+1}\\left( I_x \\right)$. Then the interpolation error at the point $x$ is defined and given by \\begin{equation} \\begin{aligned} E_n(x) \u0026amp;= f(x) − \\Pi_n f (x) \\newline \u0026amp;= \\dfrac{f^{(n+1)}( \\xi ) }{ (n + 1)!} \\omega_{n+1}(x) \\end{aligned} \\tag{1} \\label{eq:basic_interpolation_error} \\end{equation}\nwhere $\\xi \\in I_x$ and $\\omega_{n+1}$ is the nodal polynomial of degree $n + 1$, which is defined as \\begin{equation} \\omega_{n+1}(x) = \\prod\\limits_{i=0}^{n} \\left(x - x_i \\right). \\end{equation}\nPiecewise Lagrange Interpolation Definition:\t$\\mathrm{L}^2$ Space Define the following space \\begin{equation} \\mathrm{L}^{2}(a, b) = \\bigl \\lbrace f : (a, b) \\rightarrow \\mathbb{R}, \\int_{a}^{b} | f(x) |^{2} \\mathrm{d}x \u0026lt; \\infty \\bigr \\rbrace \\end{equation}\nwith\n\\begin{equation} | f |^{}_{\\mathrm{L}^{2}(a, b)} = \\left( \\int_{a}^{b} | f(x) |^{2} \\mathrm{d} x \\right)^{1 / 2}. \\end{equation}\nThis defines a norm for $\\mathrm{L}^{2}(a, b)$. Note that integral of the function $|f|^{2}$ is in the Lebesgue sense - in particular, $f$ needs not be continuous everywhere.\nPartition $\\mathcal{T_h}$ of $[a, b]$ into $K$ subintervals $I_j = \\left[ x_j , x_{j+1} \\right]$ of length $h_j$ such that $[a, b] = \\bigcup\\limits_{j=0}^{K−1}I_j$. Let ${h = \\max\\limits_{0 \\le j \\le K−1} h_j}$.\nFor $k \\geq 1$, introduce on $\\mathcal{T}_h$ the piecewise polynomial space\n\\begin{equation} X_h^k = \\bigl\\lbrace v \\in C^0([a, b]) \\, : \\, v |_{I_j} \\in \\mathbb{P}_k \\left( I_j \\right), \\quad \\forall \\, I_j \\in \\mathcal{T}_h \\bigr\\rbrace \\tag{2} \\label{eq:piecewise} \\end{equation}\nwhich is the space of the continuous functions over the interval $[a, b]$ whose restrictions on each $I_j$ are polynomials of degree less than or equal to $k$.\nThen, for any continuous function $f$ in $[a, b]$, the piecewise interpolation polynomial $\\Pi^k_h f$ coincides on each $I_j$ with the interpolating polynomial of $\\left. f \\right|_{I_j}$ at the $n + 1$ nodes $\\lbrace x_j^{(i)}, 0 \\leq i \\leq n \\rbrace$.\nAs a consequence, if $f \\in C^{k+1}([a, b])$, then from \\eqref{eq:basic_interpolation_error} within each interval the following error estimate holds \\begin{equation} \\left\\Vert f − \\Pi_h^k f \\right\\Vert_\\infty \\leq C h^{k+1} \\left\\Vert f^{(k+1} \\right\\Vert_\\infty . \\end{equation}\nTheorem:\tPartition $\\mathcal{T_h}$ of $[a, b]$ into $K$ subintervals $I_j = [x_j , x_{j+1}]$ of length $h_j$, with $h = \\max\\limits_{0 \\le j \\le K−1} h_j$, such that $[a, b] = \\bigcup\\limits_{j=0}^{K−1}I_j$. Now using Lagrange interpolation on each subinterval $I_j$ using $n + 1$ equally spaced nodes $\\lbrace{ x^{(i)}_{j} , 0 \\le i \\le n \\rbrace}$ with a small $n$. Then $\\Pi_n^k$ is the piecewise interpolation polynomial.\nLet $0 \\leq m \\leq k+1$, with $k \\geq 1$ and assume that $f^{(m)} \\in$ $\\mathrm{L}^{2}(a, b)$ for $0 \\leq m \\leq k+1$ then there exists a positive constant $C$, independent of $h$, such that\n\\begin{equation} \\left| \\left( f - \\Pi_{h}^{k} f\\right)^{(m)} \\right|_{\\mathrm{L}^{2}(a, b)} \\leq C h^{k+1-m} \\left| f^{(k+1)} \\right|_{\\mathrm{L}^{2}(a, b)} . \\end{equation}\nIn particular, for $k=1$ and $m=0$, or $m=1$\n\\begin{equation} \\begin{aligned} \\left| f-\\Pi_{h}^{1} f \\right|_{\\mathrm{L}^{2}(a, b)} \u0026amp; …","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"8afb514223601ad356e9ae5e3e551b1e","permalink":"https://djps.github.io/courses/numericalanalysis22/part-2/interpolation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalanalysis22/part-2/interpolation/","section":"courses","summary":"Numerical treatment of problems often involves the process of discretization - i.e. going from a continuous function to set of discrete points.\nInterpolation provides a way of approximating continuous functions by discrete data.","tags":null,"title":"Interpolation","type":"book"},{"authors":null,"categories":null,"content":"Enhancing Hepatic Microwave Ablation Details.\n","date":1655337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655337600,"objectID":"f78d7be5faf53a7dfcd0f0db37e1f86d","permalink":"https://djps.github.io/project/ehma/","publishdate":"2022-06-16T00:00:00Z","relpermalink":"/project/ehma/","section":"project","summary":"Enhancing Hepatic Microwave Ablation","tags":["Thermal Ablation"],"title":"EHMA","type":"project"},{"authors":null,"categories":null,"content":"If $f \\in C^{0} \\left([a, b]\\right)$, the quadrature error $E_{n}(f) = I(f) - I_{n}(f)$ satisfies \\begin{equation} \\left| E_{n}(f) \\right| \\leq \\int_{a}^{b} \\left| f(x) - f_{n}(x) \\right| \\mathrm{d} x \\leq (b-a) \\left| f - f_{n} \\right|_{\\infty} \\end{equation}\nTherefore, if for some $n$, $\\left\\| f - f_{n}\\right\\|_{\\infty} \u0026lt; \\varepsilon$ , then $\\left|E_{n}(f)\\right| \\leq \\varepsilon(b-a)$.\nThe approximation of the function $f_{n}$ must be easily integrable, which is the case if, for example, $f_{n} \\in \\mathbb{P}_{n}$.\nIn this respect, a natural approach consists of using $f_{n}=\\Pi_{n} f$, the interpolating Lagrange polynomial of $f$ over a set of $n+1$ distinct nodes $\\lbrace x_{i} \\rbrace$, with $i=0, \\ldots, n$. It follows that the approximation to the integral is \\begin{equation} I_{n}(f) = \\sum_{i=0}^{n} f \\left( x_{i} \\right) \\int_{a}^{b} l_{i}(x) \\mathrm{d} x \\end{equation}\nwhere $l_{i}$ is the characteristic Lagrange polynomial of degree $n$ associated with node $x_{i}$. It is called the Lagrange quadrature formula, and is a special instance of the following, generalised, quadrature formula \\begin{equation} I_{n}(f) = \\sum \\limits_{i=0}^{n} \\alpha_{i} f\\left( x_{i} \\right) \\end{equation}\nwhere the coefficients $\\alpha_{i}$ of the linear combination are given by $\\int_{a}^{b} l_{i} \\left( x \\right) \\mathrm{d} x$. The above equation is a weighted sum of the values of $f$ at the points $x_{i}$, for $i=0, \\ldots, n$. These points are said to be the nodes of the quadrature formula, while the $\\alpha_{i} \\in \\mathbb{R}$ are its coefficients or weights. Both weights and nodes depend in general on $n$.\nAnother approximation of the function $f$ leads to the Hermite quadrature formula \\begin{equation} I_{n}(f)=\\sum_{k=0}^{1} \\sum_{i=0}^{n} \\alpha_{i k} f^{(k)}\\left(x_{i}\\right) \\end{equation}\nwhere the weights are now denoted by $\\alpha_{i k}$. This depends on an evaluation of the function and its derivative.\nBoth the above are interpolatory quadrature formula, since the function $f$ has been replaced by its interpolating polynomial (Lagrange and Hermite polynomials, respectively).\nDefine the degree of exactness of a quadrature formula as the maximum integer $r \\geq 0$ for which \\begin{equation} I_{n}(f)=I(f), \\quad \\forall f \\in \\mathbb{P}_{r} . \\end{equation}\nAny interpolatory quadrature formula that makes use of $n+1$ distinct nodes has degree of exactness equal to at least $n$. Indeed, if $f \\in \\mathbb{P}_{n}$, then $\\Pi_n f = f$ and thus $I_n\\left( \\Pi_n f \\right) = I \\left( \\Pi_n f \\right)$.\nMidpoint Rule The zero-th order approximation is given by \\begin{equation} I_0 = (b-a) f \\left( \\dfrac{a+b}{2} \\right). \\end{equation}\nTrapezoidal Rule The trapezoidal rule is given by \\begin{equation} I_1 = \\dfrac{b-a}{2} \\left( f \\left( a \\right) + f \\left(b\\right) \\right). \\end{equation}\nSimpson’s Rule Simpson’s rule is \\begin{equation} I_2 = \\dfrac{b-a}{6} \\left( f \\left( a \\right) + 4f\\left(\\dfrac{a+b}{2} \\right)+ f \\left(b\\right) \\right). \\end{equation}\nwarning:\tExamples An .ipynb notebook, detailing examples of the Midpoint rule, Trapezoidal rule, Simpson’s rule can be accessed here. It can be downloaded from here as a python file or as ipyth It can be downloaded from here. Gaussian Integration Gaussian quadrature integrates a function by a suitable choice of nodes and weights.\nTheorem:\tWith the exact integral of $f$ \\begin{equation} I_g (f) = \\int\\limits_{−1}^{1} f (x) g(x) , \\mathrm{d}x, \\end{equation} being $f \\in C^0 \\left( [−1, 1] \\right)$, consider quadrature rules of the type \\begin{equation} I_{n,g} (f) = \\sum\\limits_{i=0}^{n} \\alpha_i f(x_i) \\end{equation} where $\\alpha_i$ are to be determined.\nFor a given $m \u0026gt; 0$, the quadrature $I_{n,g}$ has degree of exactness $d=n + m$ if and only if it is of interpolatory type and the nodal polynomial $\\omega_{n+1}$ associated with the set of nodes $\\lbrace x_i \\rbrace$, is such that \\begin{equation} \\int_{-1}^{1} \\omega_{n+1}(x) p(x) g(x) , \\mathrm{d}x = 0, \\quad \\forall , p \\in \\mathbb{P}_{m-1}. \\end{equation}\n","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"3a256e4c672bd09e627063ca798f2842","permalink":"https://djps.github.io/courses/numericalanalysis22/part-2/integration/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalanalysis22/part-2/integration/","section":"courses","summary":"If $f \\in C^{0} \\left([a, b]\\right)$, the quadrature error $E_{n}(f) = I(f) - I_{n}(f)$ satisfies \\begin{equation} \\left| E_{n}(f) \\right| \\leq \\int_{a}^{b} \\left| f(x) - f_{n}(x) \\right| \\mathrm{d} x \\leq (b-a) \\left| f - f_{n} \\right|_{\\infty} \\end{equation}","tags":null,"title":"Integration","type":"book"},{"authors":null,"categories":null,"content":"Challenge on Ultrasound Beamforming with Deep Learning Details can be found at the organisers website here.\nAward is here.\n","date":1655337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655337600,"objectID":"0bec50b7db0f4bcd31d2fceaf7d4728f","permalink":"https://djps.github.io/project/cubdl/","publishdate":"2022-06-16T00:00:00Z","relpermalink":"/project/cubdl/","section":"project","summary":"Challenge on Ultrasound Beamforming with Deep Learning","tags":["Deep Learning","Ultrasound"],"title":"CUBDL: Challenge on Ultrasound Beamforming with Deep Learning","type":"project"},{"authors":null,"categories":null,"content":"Green’s functions For a linear differential operator acting on $u$, that is $\\mathcal{L} \\left[ u \\left( x \\right) \\right]$, which has a differential equation of the form \\begin{equation} \\mathcal{L}\\left[ u \\left( x \\right) \\right] = f \\left(x \\right), \\end{equation}\nthen the Green’s function for the operator $\\mathcal{L}$, denoted by $G\\left( x,s \\right)$, can be used to solved the differential equation as \\begin{equation} u(x) = \\int^x G\\left(x,s \\right) f\\left( s \\right) \\, \\mathrm{d} s. \\end{equation}\nFinite Difference Methods First discretize the domain and then approximate the governing equation to produce a linear system. Definition:\tFinite-Difference Quotients Consider the approximations to the first-order derivative:\nForward Difference Quotient: \\begin{equation} D_{j}^{+} u = \\dfrac{u_{j+1} - u_j}{h} \\end{equation} Backwards Difference Quotient: \\begin{equation} D_{j}^{-} u = \\dfrac{u_{j} - u_{j-1}}{h} \\end{equation} Central Difference Quotient: \\begin{equation} D_{j}^{0} u = \\dfrac{u_{j+1} - u_{j-1}}{2h} \\end{equation} With these, approximations to second-order derivatives can be constructed, for example:\n\\begin{equation} \\begin{aligned} D_{j}^{\\pm} u \u0026amp; = \\dfrac{ D_j^{+} u - D_j^{-} u }{h} \\newline \u0026amp; = \\dfrac{ \\dfrac{u_{j+1} - u_j}{h} - \\dfrac{u_j - u_{j-1}}{h} }{h} \\newline \u0026amp; = \\dfrac{ u_{j+1} - 2 u_j + u_{j-1} }{h^2}. \\end{aligned} \\end{equation}\nTheorem:\tErrors for Finite-Difference Quotients The errors for the approximation of the derivatives are given by\n$u\\left( x_j \\right) - D_{j}^{+}u = -\\dfrac{h}{2} u^{\\prime\\prime}\\left( \\xi\\right)$ where $\\xi \\in \\left( x_j, x_{j+1} \\right)$ $u\\left( x_j \\right) - D_{j}^{-}u = \\dfrac{h}{2} u^{\\prime\\prime}\\left( \\xi\\right)$ where $\\xi \\in \\left( x_{j-1}, x_{j} \\right)$ $u\\left( x_j \\right) - D_{j}^{0}u = -\\dfrac{h^2}{6} u^{\\prime\\prime\\prime}\\left( \\xi\\right)$ where $\\xi \\in \\left( x_{j-1}, x_{j+1} \\right)$ $u\\left( x_j \\right) - D_{j}^{\\pm}u = -\\dfrac{h^2}{24}\\left( u^{(4)}\\left( \\xi_1\\right) + u^{(4)}\\left( \\xi_2\\right) \\right)$ where $\\xi_1 \\in \\left( x_{j-1}, x_{j} \\right)$ and $\\xi_2 \\in \\left( x_{j}, x_{j+1} \\right)$. Stability Analysis Let $V_h$ be the set of discrete functions defined on the nodal points $x_j$ and $V_h^0 \\subset V_h$ contain the discrete functions $v_h \\in V_h$ which vanish at $x_0$ and $x_n$, i.e. $v_0 =0$ and $v_n=0$.\nLemma:\tLet $\\mathcal{L}_h$ be the discretization of a linear differential operator which acts on $u_h \\in V_h$, i.e. $\\mathcal{L}_h \\left[ u_h \\right]$. If the discrete inner product for both $v_h$ and $w_h \\in V_h$ is defined as\n\\begin{align} \\left( v_h, w_h \\right)_h^{} \u0026amp; = h \\sum\\limits_{j=0}^{n} c_j v_j w_j \\end{align}\nwhere 1 $c_j = 1$ for $j=1, \\ldots n-1$ and $c_0 = c_n = \\frac{1}{2}$ and a norm is defined as\n\\begin{equation} \\left\\Vert v_h \\right\\Vert_h = \\sqrt{ \\left( v_h, v_h \\right)_h } \\end{equation}\nfor a $v_h \\in V_h$.\nThen the operator $\\mathcal{L}_h$ is symmetric, i.e. \\begin{equation} \\left( \\mathcal{L}_h \\left[ v_h \\right], w_h \\right)_h = \\left( v_h, \\mathcal{L}_h \\left[ w_h \\right]\\right)_h \\quad \\forall \\, w_h , \\, v_h \\in V^0_h \\end{equation}\nand positive definite, that is \\begin{equation} \\left( \\mathcal{L}_h \\left[ v_h \\right], v_h \\right)_h \\ge 0 \\quad \\forall \\, v_h \\in V^0_h \\end{equation}\nand \\begin{equation} \\left( \\mathcal{L}_h \\left[ v_h \\right], v_h \\right)_h = 0 \\Longleftrightarrow v_h = 0 . \\end{equation}\nThis is just the composite trapezium rule, so that the discrete inner product is the discrete analogue to \\begin{equation} \\left( w, v \\right) = \\int w(x) v(x) \\, \\mathrm{d}x \\end{equation} i.e. it approximates an integral. ↩︎\nLemma:\tFor any $v_h \\in V_h$ \\begin{equation} \\left\\Vert v_h \\right\\Vert_h \\le \\dfrac{1}{\\sqrt{2}} \\Bigg( h \\sum \\limits_{j=0}^{n-1} \\left( \\dfrac{v_{j+1} - v_j}{h} \\right)^2 \\Bigg)^{1/2}. \\end{equation} Convergence The finite difference solution $u_h$ can be characterised by a discrete Green’s function. Define $G^k\\left(x\\right) \\in V_h^0$ such that \\begin{equation} \\mathcal{L}_h \\left[ G^k\\left(x\\right) \\right] = e^k\\left(x\\right) \\end{equation}\nwhere $e^k \\in V_h^0$ satisfies $e^k\\left( x_j \\right) = \\delta_{kj}$. Then \\begin{equation} G^k \\left( x_j \\right) = h G\\left( x_j, x_k \\right) . \\end{equation}\nTheorem:\tLet $\\left\\Vert v_h\t\\right\\Vert_{h,\\infty} = \\max \\limits_{0 \\le j \\le n}\\left| v_h\\left( x_j \\right) \\right|$ be the discrete maximum norm.\nAssume that $f \\in C^2 \\left( \\left[ 0,1 \\right] \\right)$, then the nodal error, given by $e\\left( x_j \\right) = u\\left(x_j\\right) - u_h\\left(x_j\\right)$ satisfies: \\begin{equation} \\left\\Vert u - u_h \\right\\Vert_{h,\\infty} \\le \\dfrac{h^2}{96} \\left\\Vert f^{\\prime\\prime} \\right\\Vert_\\infty . \\end{equation}\nGalerkin Method Consider the elementary problem: \\begin{equation} \\left( \\alpha u^{\\prime} \\right)^{\\prime} + \\beta u^{\\prime} + \\gamma u = f\\left( x \\right) \\quad \\mbox{on} \\quad (0,1) \\quad \\mbox{with} \\quad u(0) = u(1)=0 \\end{equation}\nwhere $\\alpha$, $\\beta$, $\\gamma \\in C^0 \\left( \\left[ 0, 1 …","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"9a1b271c4d970e370b60963c1e3fd422","permalink":"https://djps.github.io/courses/numericalanalysis22/part-3/fdm/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalanalysis22/part-3/fdm/","section":"courses","summary":"Green’s functions For a linear differential operator acting on $u$, that is $\\mathcal{L} \\left[ u \\left( x \\right) \\right]$, which has a differential equation of the form \\begin{equation} \\mathcal{L}\\left[ u \\left( x \\right) \\right] = f \\left(x \\right), \\end{equation}","tags":null,"title":"Finite Difference Methods","type":"book"},{"authors":null,"categories":null,"content":" Finite element methods approximate the solution on a small element. Distributions Denote by $\\mathrm{H}^s(a, b)$, for $s \\geq 1$, the space of the functions $f \\in C^{s-1}(a, b)$ such that $f^{(s-1)}$ is continuous and piecewise differentiable, so that $f^{(s)}$ exists unless for a finite number of points and belongs to $\\mathrm{L}^2(a, b)$. The space $\\mathrm{H}^s(a, b)$ is known as the Sobolev function space of order $s$ and is endowed with the norm $\\left\\Vert \\cdot \\right\\Vert_{H^s(a,b)}$ defined as \\begin{equation} \\left\\Vert f \\right\\Vert_s = \\left( \\sum_{k=0}^s \\left\\Vert f^{\\left(k\\right)} \\right\\Vert_{\\mathrm{L}^2\\left(a,b\\right)}^2 \\right)^{1/2}. \\end{equation}\nLet \\begin{align} C_0^\\infty \u0026amp; = \\lbrace \\varphi \\in C^\\infty \\, | \\, \\exists \\, a, b \\in (0,1) \\quad \\mbox{such that} \\quad \\varphi(x)=0 \\quad \\mbox{for} \\quad 0 \\leq x \u0026lt; a \\quad \\mbox{or} \\quad b \u0026lt; x \\leq 1 \\rbrace . \\end{align}\nThen for a function $v \\in \\mathrm{L}^2(0,1)$ we say $v^\\prime$ is the weak derivative (or distributional derivative) if \\begin{equation} \\int \\limits_0^1 v^\\prime \\varphi \\, \\mathrm{d}x = - \\int \\limits_0^1 v \\varphi^\\prime \\, \\mathrm{d}x \\quad \\forall \\, \\varphi \\in C^\\infty_0 \\left(0,1\\right). \\end{equation}\nOf interest is \\begin{equation} H^1 (0, 1) = \\lbrace v \\in \\mathrm{L}^2(0, 1) \\, : \\, v^\\prime \\in \\mathrm{L}^2(0, 1) \\rbrace \\end{equation}\nwhere $v^\\prime$ is the distributional derivative of $v$, and \\begin{equation} H^1_0 (0, 1) = \\lbrace v \\in \\mathrm{L}^2(0, 1) \\, : \\, v^\\prime \\in \\mathrm{L}^2(0, 1), \\, v(0) = v(1) = 0 \\rbrace \\end{equation}\nOn $H^1$ there is the semi-norm:\n$$ \\left| v \\right|_{\\mathrm{H}^1(0,1)} = \\left( \\int_0^1 \\left\\| v^\\prime \\left( x \\right) \\right\\|^2 \\mathrm{d}x \\right)^{1/2} = \\left\\Vert v^\\prime \\right\\Vert_{\\mathrm{L}^2(0,1)}. $$ To see that it is a semi-norm and not a norm, consider $v$ a constant, so ${v^{\\prime}=0}$ thus ${\\left| v \\right|_{\\mathrm{H}^1(0,1)} =0}$ for ${v \\neq 0}$ and thus by definition is a semi-norm, rather than a norm. Now consider the integral on functions in $H_0^1$, it is the case that if the integral is zero so the function is constant, but as it must be zero on the boundaries, so the function is zero and hence a norm.\nGalerkin Method Consider the elementary problem:\n\\begin{equation} -\\left( \\alpha u^{\\prime} \\right)^{\\prime} + \\beta u^{\\prime} + \\gamma u = f\\left( x \\right) \\quad \\mbox{on} \\quad (0,1) \\quad \\mbox{with} \\quad u(0) = u(1) = 0 \\end{equation}\nwhere $\\alpha$, $\\beta$, $\\gamma \\in C^0 \\left( \\left[ 0, 1 \\right] \\right)$ and $\\alpha(x) \\ge \\alpha_0 \u0026gt;0$ for all $x \\in \\left[ 0, 1 \\right]$.\nNext, on $\\mathrm{L}^2(0,1)$, define the scalar product \\begin{equation} \\left( f, v \\right) = \\int \\limits_0^1 f v \\mathrm{d}x \\end{equation}\nand a bilinear form $a : \\left( \\cdot, \\cdot \\right)$ which maps $H_0^1 \\times H^1_0 \\rightarrow \\mathbb{R}$ \\begin{equation} a\\left( u, v \\right) = \\int_0^1 \\left( \\alpha u^\\prime v^\\prime + \\beta u^\\prime v + \\gamma u v \\right) \\mathrm{d}x \\end{equation}\nand consider the weak form \\begin{equation} \\mbox{Find} \\quad u \\in H^1_0 \\quad \\mbox{such that} \\quad a\\left(u,v \\right) =\\left( f, v\\right) \\quad \\forall \\, v \\in H^1_0\\left(0,1\\right). \\end{equation}\nTheorem:\tThe following hold:\nLet $u$ be a $C^2$ be a solution of the elementary problem, then ${u \\in H^1_0}$ also solves the weak form Let $u \\in H^1_0$ be a solution of the weak problem. If and only if ${u \\in C^2\\left( \\left[0,1 \\right] \\right)}$ then $u$ also solves the elementary problem. Theorem:\tFundamental Theorem of the Calculus of Variations Suppose that $f$ is integrable on $(0,1)$ and \\begin{equation} \\int_0^1 \\phi f \\, \\mathrm{d}x = 0 \\quad \\forall \\, \\phi \\in C^\\infty_0\\left(\\left[ 0,1 \\right]\\right) \\end{equation}\nthen $f=0$.\nApproximate $H_0^1$ by $V_h$. The discrete weak problem is then: \\begin{equation} \\mbox{Find a} \\quad u_h \\in V_h \\quad \\mbox{such that} \\quad a\\left(u_h, v_h\\right) = \\left(f, v_h \\right) \\quad \\forall \\, v_h \\in V_h \\end{equation}\nLet $\\lbrace \\varphi_1, \\varphi_2, \\ldots, \\varphi_N \\rbrace$ be a basis of $V_h$, then, with $N=\\mbox{dim}V_h$, so that \\begin{equation} u_h \\left(x \\right) = \\sum\\limits_{j=1}^N u_j \\varphi_j\\left(x\\right). \\end{equation}\nSo the problem can be written as: Find ${\\left(u_1, \\ldots u_N \\right) \\in \\mathbb{R}^N}$ such that \\begin{equation} \\sum\\limits_{j=1}^N u_j a\\left( \\varphi_j, \\varphi_i \\right) = \\left( f, \\varphi_i \\right) \\quad i=1, \\ldots, N. \\end{equation}\nDenote ${a_{ij}=a\\left( \\varphi_j, \\varphi_i \\right)}$ as the elements of the matrix $A$, let ${u=\\left(u_1, \\ldots, u_N\\right)}$ and ${f=\\left(f_1, \\ldots, f_N\\right)}$ be vectors where each entry is given by ${f_i = f \\varphi_i}$, so that the problem is equivalent to solving the linear problem ${Au=f}$\nTheorem:\tPoincaré-Friedrich Inequality Let $\\Omega \\subset \\mathbb{R}^n$ be contained in $n$-dimensional cube of length $s$, then \\begin{equation} \\bigl\\Vert v \\bigr\\Vert_{L^2\\left(\\Omega\\right)} \\le s \\left| v \\right|_{H_0^1\\left( …","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"fa0a9284762b38c15fcfaf2e23a1211a","permalink":"https://djps.github.io/courses/numericalanalysis22/part-3/fem/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalanalysis22/part-3/fem/","section":"courses","summary":"Finite element methods approximate the solution on a small element. Distributions Denote by $\\mathrm{H}^s(a, b)$, for $s \\geq 1$, the space of the functions $f \\in C^{s-1}(a, b)$ such that $f^{(s-1)}$ is continuous and piecewise differentiable, so that $f^{(s)}$ exists unless for a finite number of points and belongs to $\\mathrm{L}^2(a, b)$.","tags":null,"title":"Finite Element Methods","type":"book"},{"authors":null,"categories":null,"content":"Deformation of an elastic rod in a magnetic field This was my PhD which was undertaken at UCL with Prof. Gert van der Heijden.\nThesis can be found online here or here Journal publications can be found\nIntegrability of a conducting elastic rod in a magnetic field Spatial chaos of an extensible conducting rod in a uniform magnetic field Conference proceeding can be found here:\nThe Buckling of Magneto-Strictive Cosserat Rods Localisation of a twisted conducting rod in a uniform magnetic field: the Hamiltonian-Hopf-Hopf bifurcation ","date":1655337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655337600,"objectID":"fbec78fdd31514806d87eb74321447c9","permalink":"https://djps.github.io/project/cosserat/","publishdate":"2022-06-16T00:00:00Z","relpermalink":"/project/cosserat/","section":"project","summary":"Deformation of an elastic conducting rod in a magnetic field","tags":["Cosserat theory"],"title":"Cosserat Theory","type":"project"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://djps.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["David Sinden"],"categories":["Conference"],"content":"Presentation at virtual conference.\nSite\nHomepage\n","date":1655683200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655683200,"objectID":"5c1be06cd201fa46a604b8f8163c55dd","permalink":"https://djps.github.io/blog/stm/","publishdate":"2022-06-20T00:00:00Z","relpermalink":"/blog/stm/","section":"blog","summary":"Computation of Microwave Ablation presented at Annual Meeting of the Society for Thermal Medicine","tags":["MWA","Thermal Ablation"],"title":"Annual Meeting of the Society for Thermal Medicine 2022","type":"blog"},{"authors":null,"categories":null,"content":"Interpolation of regularly space intervals can produce unsatisfactory results\nFirst, load the tools needed.\nimport numpy as np import matplotlib.pyplot as plt plt.style.use(\u0026#39;seaborn-poster\u0026#39;) Now define the basis polynomial function\ndef lagrange(x_int, y_int, x_new): \u0026#34;\u0026#34;\u0026#34; This function takes pairs of points (x_int, y_int) and, from a set of points x_new computes the Lagrange polynomial to return the interpolated values y_new \u0026#34;\u0026#34;\u0026#34; y_new = np.zeros(x_new.shape, dtype=np.float) for xi, yi in zip(x_int, y_int): y_new += yi * np.prod( [(x_new - xj) / (xi - xj) for xj in x_int if xi != xj], axis=0) return y_new Define the Runge function as \\begin{equation} f\\left(x\\right) = \\dfrac{1}{1+25 x^2}\\end{equation}\nrunge = lambda x: 1.0 / (1.0 + 25.0 * x**2) Let the range be $(-1,1)$.\nx = np.linspace(-1, 1, 100) x_int = np.linspace(-1, 1, 11) y_int = runge(x_int) x_new = np.linspace(-1, 1, 1000) y_new = lagrange(x_int, runge(x_int), x_new) x_new0 = np.linspace(-1, 1, 20) y_new0 = lagrange(x_int, runge(x_int), x_new0) plt.plot(x, runge(x), \u0026#34;k--\u0026#34;, label=\u0026#34;Runge function\u0026#34;) plt.plot(x_new, y_new, label=\u0026#34;Uniform Interpolation n=1000\u0026#34;) plt.plot(x_new0, y_new0, label=\u0026#34;Uniform Interpolation n=100\u0026#34;) plt.plot(x_int, y_int, \u0026#34;k*\u0026#34;) plt.legend() plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.grid(True) plt.show() import numpy as np from scipy.interpolate import lagrange import matplotlib.pyplot as plt x = [0, 1, 2] y = [1, 3, 2] x_new = np.arange(-1.0, 3.1, 0.1) f = lagrange(x, y) fig = plt.figure() plt.plot(x_new, f(x_new), \u0026#39;b\u0026#39;, x, y, \u0026#39;ro\u0026#39;) plt.title(r\u0026#39;Lagrange Polynomial\u0026#39;) plt.grid() plt.xlabel(r\u0026#39;$x$\u0026#39;) plt.ylabel(r\u0026#39;$y$\u0026#39;) plt.show() ","date":1655424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655424000,"objectID":"a5ea70e1099d13a15bedbd43c4a57aef","permalink":"https://djps.github.io/courses/numericalanalysis22/notebooks/lagrangeinterpolation/","publishdate":"2022-06-17T00:00:00Z","relpermalink":"/courses/numericalanalysis22/notebooks/lagrangeinterpolation/","section":"courses","summary":"Examples of Lagrange Interpolation","tags":null,"title":"Lagrange Interpolation","type":"book"},{"authors":null,"categories":null,"content":"Consider the integration of the function $f(x) = \\exp \\left( x^2 \\right)$ between $[0,1]$.\nIt can easily be shown that\n\\begin{align} \\int_0^1 e^{-x^2} \\mathrm{d}x = \\dfrac{\\sqrt{\\pi}}{2}. \\nonumber \\end{align}\nFirst import the tools needed\nimport numpy as np import matplotlib.pyplot as plt import scipy.integrate as spi Next define the range to plot the function over and a number of point to evaluate the function.\nx0 = -0.5 x1 = 1.5 n = 100 So we have a range and a function.\nx = np.linspace(x0, x1, n) y = np.exp(-x**2) f = lambda x : np.exp(-x**2) Let $m$ be vector whose elements are the number of evaluations of the integral.\nm = np.array([4,8,16], dtype=int) Compute and plot the successive approximations for the integral, as well as keeping track of the errors.\nWe will use the function trapezoid to compute the integral according to\n\\begin{align} \\int_a^b f(x) ,\\mathrm{d}x \\approx \\sum\\limits_{k=1}^{M} \\dfrac{f\\left( x_{k-1}\\right) + f\\left(x_{k}\\right)}{2} \\Delta x_{k} \\quad \\mbox{where} \\quad x_0 = a \\quad \\mbox{and} \\quad x_M = b. \\nonumber \\end{align}\nfig, ax = plt.subplots(m.size,1) fig.tight_layout() fig.subplots_adjust(top=0.99) T = np.zeros((m.size,)) absolute_error = np.zeros((m.size,)) relative_error = np.zeros((m.size,)) for j,k in enumerate(m): nsteps = np.float(k) dx = 1.0 / nsteps x_approx = np.linspace(0.0, 1.0, num=k) T[j] = spi.trapz(f(x_approx), x_approx) for i in np.arange(0, nsteps): x_start = i*dx x_stop = (i+1)*dx y_start = np.exp(-x_start**2) y_stop = np.exp(-x_stop**2) ax[j].fill_between([x_start,x_stop], [y_start,y_stop], facecolor=\u0026#39;b\u0026#39;, edgecolor=\u0026#39;b\u0026#39;, alpha=0.2) ax[j].scatter([x_start,x_stop], [y_start,y_stop]) ax[j].plot(x, y, \u0026#39;b-\u0026#39;) ax[j].set_title(r\u0026#39;Trapezoid Rule, $N$ = {}, Approx: {}\u0026#39;.format(k,T[j])) ax[j].grid(True) if (j==m.size-1): ax[j].set_xlabel(\u0026#39;x\u0026#39;) ax[j].set_ylabel(\u0026#39;y\u0026#39;) absolute_error[j] = np.abs( np.sqrt(np.pi)/2.0 - T[j]) relative_error[j] = np.abs( (np.sqrt(np.pi)/2.0 - T[j])/(np.sqrt(np.pi)/2.0 ) ) Tabulate the data and display it\nresults = np.vstack([m.T, relative_error, absolute_error]) import pandas as pd df = pd.DataFrame(results.T, columns=[\u0026#34;Order\u0026#34;, \u0026#34;Relative Error\u0026#34;, \u0026#34;Absolute Error\u0026#34;]) df.Order = df.Order.astype(int) display(df.style.hide_index().set_caption(\u0026#34;Results for $e^{-x^2}$\u0026#34;)) Results for $e^{-x^2}$ Order Relative Error Absolute Error 4 0.165015 0.146240 8 0.158712 0.140655 16 0.157607 0.139675 ","date":1655424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655424000,"objectID":"2dddb03394cda67a0744f4950d4a55f2","permalink":"https://djps.github.io/courses/numericalanalysis22/notebooks/integrals/","publishdate":"2022-06-17T00:00:00Z","relpermalink":"/courses/numericalanalysis22/notebooks/integrals/","section":"courses","summary":"Examples of numerical integration","tags":null,"title":"Numerical Integration","type":"book"},{"authors":null,"categories":null,"content":"This notebook is based on an example from Chapter 2 of An Introduction to Computational Stochastic PDEs.\nThe matrices assembled have few non-zero entries, so rather than store all entries in the matrix, only the non-zero values and there locations are stored. For this compressed sparse column matrices are used, but this is an implementational detail.\nFirst import the necessary libraries\nimport numpy as np import scipy from scipy import sparse from scipy.sparse import linalg import matplotlib import matplotlib.pyplot as plt plt.style.use(\u0026#39;seaborn-poster\u0026#39;) Now define the function which assembles and solves the boundary value problem:\n\\begin{align} -\\dfrac{d}{dx} \\left( \\alpha(x) \\dfrac{d u(x)}{dx} \\right) + \\gamma(x) u(x) = f(x) \\quad a \u0026lt; x \u0026lt; b \\end{align}\nwith\n\\begin{equation} u(a) = 0 \\quad \\mbox{and} \\quad u(b) = 0. \\end{equation}\nNote that the boundary value problem has a solution on (0,1) when $\\alpha$, $\\gamma$ and $f$ are constant, given by\n\\begin{equation} u(x) = \\dfrac{f}{\\alpha} \\left( 1 - \\dfrac{ e^{\\sqrt{s}x} + e^{\\sqrt{s}(1-x)} }{ 1 + e^{\\sqrt{s}} } \\right) \\quad \\mbox{where} \\quad s =\\gamma / \\alpha. \\end{equation}\nNow define two functions to solve the problem numerically\nget_elt_arrays : which maps the basis functions one_dimensional_linear_FEM : which assembles and solves the finite element solution for the boundary value problem. def one_dimensional_linear_FEM(N, alpha, gamma, f): \u0026#34;\u0026#34;\u0026#34; Solves the boundary value problem, returns the solution and the assembled matrices. \u0026#34;\u0026#34;\u0026#34; # step size h = 1.0 / N # nodal points xx = np.linspace(0.0, 1.0, N+1) # size of system nvtx = N + 1 # number of elements J = N- 1 # allocate matrix elt2vert = np.vstack((np.arange(0, J + 1, dtype=\u0026#39;int\u0026#39;), np.arange(1, J + 2, dtype=\u0026#39;int\u0026#39;))) # allocate space global matrices K = sparse.csc_matrix((nvtx, nvtx)) M = sparse.csc_matrix((nvtx, nvtx)) # allocate right hand side of linear system b = np.zeros(nvtx) # compute element matrices Kks, Mks, bks = get_elt_arrays(h, alpha, gamma, f, N) # Assemble element arrays into global arrays K = sum(sparse.csc_matrix((Kks[:, row_no, col_no], (elt2vert[row_no,:], elt2vert[col_no,:])), (nvtx,nvtx)) for row_no in range(2) for col_no in range(2)) M = sum(sparse.csc_matrix((Mks[:, row_no, col_no], (elt2vert[row_no,:], elt2vert[col_no,:])), (nvtx,nvtx)) for row_no in range(2) for col_no in range(2)) for row_no in range(2): nrow = elt2vert[row_no,:] b[nrow] = b[nrow] + bks[:, row_no] # set lefthand side of the linear system A = K + M # impose homogeneous boundary condition A = A[1:-1, 1:-1] K = K[1:-1, 1:-1] M = M[1:-1, 1:-1] b = b[1:-1] # solve linear system for interior degrees of freedom u_int = sparse.linalg.spsolve(A,b) # append boundary data to interior solution uh = np.hstack([0.0, u_int, 0.0]) # return solution and the components of the linear system return uh, A, b, K, M def get_elt_arrays(h, alpha, gamma, f, N): \u0026#34;\u0026#34;\u0026#34; Assemble stiffness and mass matrices and right handside vector. \u0026#34;\u0026#34;\u0026#34; Kks = np.zeros((N, 2, 2)); Kks[:,0,0] = alpha /h Kks[:,0,1] = -alpha / h Kks[:,1,0] = -alpha / h Kks[:,1,1] = alpha / h Mks = np.zeros_like(Kks) Mks[:,0,0] = gamma * h / 3.0 Mks[:,0,1] = gamma * h / 6.0 Mks[:,1,0] = gamma * h / 6.0 Mks[:,1,1] = gamma * h / 3.0 bks = np.zeros((N, 2)) bks[:,0] = f * (h / 2.0) bks[:,1] = f * (h / 2.0) return Kks, Mks, bks Now create a function eval_sol which evaluates the exact solution for constant $\\alpha$, $\\gamma$ and $f$ on $(0,1)$\ndef eval_sol(f, alpha, gamma, x): \u0026#34;\u0026#34;\u0026#34; Evaluate exact solution \u0026#34;\u0026#34;\u0026#34; s = gamma / alpha out = (f / gamma) * (1.0 - (np.exp(np.sqrt(s) * x) + np.exp(np.sqrt(s) * (1.0-x))) / (1.0 + np.exp(np.sqrt(s))) ) return out Evaluate the exact solution for $\\alpha=1$, $\\gamma=10$ and $f=1$\nx0 = 0.0 x1 = 1.0 L = 100 x = np.linspace(x0, x1, L) f = 1.0 alpha = 1.0 gamma = 10.0 u = eval_sol(f, alpha, gamma, x) For a number of different step sizes compute the finite element approximation\nnpowers = 6 fig, axes = plt.subplots(2, int(npowers/2), constrained_layout=True) for i in range(1, 1+npowers): plt.subplot(2, int(npowers/2), i) N = int( np.power(2, i) ) xh = np.linspace(x0, x1, N+1) uh, A, b, K, M = one_dimensional_linear_FEM(N, alpha, gamma, f) axes[(i-1)%2, (i-1)//2].plot(xh, uh, \u0026#39;--ob\u0026#39;, label=\u0026#34;$N$={}\u0026#34;.format(N)) axes[(i-1)%2, (i-1)//2].set_xlabel(r\u0026#39;$x$\u0026#39;) axes[(i-1)%2, (i-1)//2].set_ylabel(r\u0026#39;$u$\u0026#39;) axes[(i-1)%2, (i-1)//2].plot(x, u, \u0026#39;-k\u0026#39;) axes[(i-1)%2, (i-1)//2].grid(True) axes[(i-1)%2, (i-1)//2].legend(loc=\u0026#39;lower center\u0026#39;) Error Analysis Compute the error via a norm defined using the bilinear form\n\\begin{equation} a(u,v) = \\int_a^b \\alpha u^{\\prime}(x) v^{\\prime}(x) + \\gamma u(x) v(x) \\mathrm{d}x \\end{equation}\nthat is\n\\begin{equation} \\left\\lVert u_{}^{} \\right\\Vert_V = \\sqrt{a(u,u)} \\end{equation}\nso that the error is given by\n\\begin{equation} \\left\\Vert u - u_h \\right\\Vert_V^2. \\end{equation}\nWhen $\\alpha=1$, $\\gamma=0$ and $f=1$, the exact solution is given by $u=\\frac{1}{2}x(1-x)$. Thus $u^{\\prime}= \\frac{1}{2}-x$. Evaluating the integral for the error then …","date":1655424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655424000,"objectID":"d377b98149d9ffcf39e78b5c0c83f568","permalink":"https://djps.github.io/courses/numericalanalysis22/notebooks/1dfem/","publishdate":"2022-06-17T00:00:00Z","relpermalink":"/courses/numericalanalysis22/notebooks/1dfem/","section":"courses","summary":"Example of one-dimensional finite element simulation","tags":null,"title":"One-Dimensional Finite Element Simulations","type":"book"},{"authors":null,"categories":null,"content":"This notebook, like the one-dimensional finite element example is based on an example from Chapter 2 of An Introduction to Computational Stochastic PDEs.\nThe matrices assembled have few non-zero entries, so rather than store all entries in the matrix, only the non-zero values and there locations are stored. For this compressed sparse column matrices are used, but this is an implementational detail.\nFirst import the necessary libraries\nimport numpy as np import scipy from scipy import sparse from scipy.sparse import linalg import matplotlib from matplotlib import cm import matplotlib.pyplot as plt plt.style.use(\u0026#39;seaborn-poster\u0026#39;) Consider the partial differential equation\n\\begin{equation} -\\nabla \\cdot \\left( a(x) \\nabla u \\left(x\\right) \\right) = f\\left(x\\right) \\quad x \\in \\Omega \\end{equation}\nwhere $a(x) \u0026gt; 0$ for all $x \\in \\Omega$, and with boundary conditions\n\\begin{equation} u = g(x) \\quad x \\in \\partial\\Omega. \\end{equation}\nLabel the boundary vertices after the interior vertices, thus construct a solution of the form\n\\begin{equation} w(x) = \\sum_{i=1}^{J} w_i \\phi_i(x) + \\sum_{i=J+1}^{J+J_b} w_i \\phi_i(x) \\end{equation}\nso, by definition let the solution take the form,\n\\begin{equation} w(x) = w_0(x) + w_g (x) \\end{equation}\nas $w_0$ is zero on the boundary, the values on the boundary are determined by $w_g$.\nFix the values as\n\\begin{equation} w_B(x) = \\left( w_{J+1}, \\ldots, w_{J+J_b} \\right)^T \\end{equation}\nwhere the values of $w_B$ are interpolated as\n\\begin{equation} w_i \\left(x \\right) = g \\left( x_i \\right) \\quad \\mbox{for} \\quad i=J+1,\\ldots,J+J_b. \\end{equation}\nSo the finite element approximation is given by\n\\begin{equation} u_h(x) = \\sum_{i=1}^{J}u_i \\phi_i(x) + \\sum_{i=J+1}^{J+J_b} w_i \\phi_i(x) \\end{equation}\nSubstituting this into the weak form equation yields\n\\begin{equation} \\sum_{i=1}^{J} u_i a\\left( \\phi_i, \\phi_j \\right) = \\left(f,\\phi_j \\right) - \\sum_{i=J+1}^{J+J_b} w_i a\\left( \\phi_i(x), \\phi_j(x) \\right) \\quad \\mbox{for} \\quad j=1,\\ldots,J. \\end{equation}\nFor a linear basis function, the linear system is sparse and can be partition into interior and boundary parts\n\\begin{equation} A = \\left( \\begin{array}{cc} A_{II} \u0026amp; A_{IB} \\newline A_{BI} \u0026amp; A_{BB} \\end{array} \\right) \\quad \\mbox{and} \\quad b = \\left( \\begin{array}{c} b_I, \u0026amp; b_B \\end{array} \\right)^T. \\end{equation}\nSo that the governing equation for the unknown interior values can be written as\n\\begin{equation} A_{II} u_I = b_I - A_{IB} w_B. \\end{equation}\nConsider the local piecewise basis functions associated with the reference triangle as\n\\begin{equation} \\psi_1(x) = 1-s-t, \\quad \\psi_2(x) = s \\quad \\mbox{and} \\quad \\psi_3(x) = t. \\label{eq:local} \\end{equation}\nwhere \\begin{equation} \\phi_p^k\\left( x\\left(s,t\\right), y\\left(s,t\\right) \\right) = \\psi_p\\left(s,t\\right) \\quad p=1,2,3. \\end{equation}\nThen the solution takes the form\n\\begin{align} x(s,t) \u0026amp; = x_1^k \\psi_1 + x_2^k \\psi_2 + x_3^k \\psi_3, \\newline y(s,t) \u0026amp; = y_1^k \\psi_1 + y_2^k \\psi_2 + y_3^k \\psi_3. \\end{align}\nThe Jacobian between the two frames is given by\n\\begin{equation} J = \\left( \\begin{array}{cc} \\dfrac{\\partial x}{\\partial s} \u0026amp; \\dfrac{\\partial y}{\\partial s} \\newline \\dfrac{\\partial x}{\\partial t} \u0026amp; \\dfrac{\\partial y}{\\partial t} \\end{array} \\right). \\end{equation}\nThus, for the local basis functions given above, the Jacobian and inverse are given by\n\\begin{equation} J = \\left( \\begin{array}{cc} x_2 - x_1 \u0026amp; y_2 - y_1 \\newline x_3 - x_1 \u0026amp; y_3 - y_1 \\end{array} \\right) \\end{equation}\nand\n\\begin{equation} J^{-1} = \\dfrac{1}{|J|} \\left( \\begin{array}{cc} y_3 - y_1 \u0026amp; y_1 - y_2 \\newline x_1 - x_3 \u0026amp; x_2 - x_1 \\end{array} \\right) \\end{equation}\nwhere the determinant of the Jacobian is given by\n\\begin{equation} |J| = \\left( x_2 - x_1 \\right)\\left( y_3 - y_1 \\right) - \\left( y_2 - y_1 \\right) \\left( x_3 - x_1 \\right). \\end{equation}\nNext create a uniform mesh on a square domain between $(x_0,x_1)$ with $N$ elements on the line. Thus $N+1$ vertices on the line, so that there are $(N+1)^2$ vertices in the domain.\nuniform_mesh_info : this creates the uniform mesh. It returns the $x$ and $y$ locations of the vertices as xv and yv and the array elt2vert this takes the label of element and returns the labels of the vertices of that element, as well as the number of vertices nvtx, number of elements ne and the step size h. get_jac_info : computes the Jacobian matrix and it’s inverse. get_elt_arrays2D : computes the matrix $A$ and the vector $b$ for the governing equation. two_dimensional_linear_FEM : assembles and computes the solution def uniform_mesh_info(x0, x1, N): \u0026#34;\u0026#34;\u0026#34; Create a uniform square mesh of right angle triangles \u0026#34;\u0026#34;\u0026#34; h = 1 / N x = np.linspace(x0, x1, N+1); y = np.copy(x) # co-ordinates of vertices xv, yv = np.meshgrid(x, y) xv = xv.ravel() yv = yv.ravel() # N squared n2 = N * N # number of vertices nvtx = (N+1) * (N+1) # number of elements as each square is divided into two ne = 2 * n2 # global vertex labels of individual elements elt2vert = np.zeros((ne,3), …","date":1655424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655424000,"objectID":"c8562ddb217597bde2baab652db96793","permalink":"https://djps.github.io/courses/numericalanalysis22/notebooks/2dfem/","publishdate":"2022-06-17T00:00:00Z","relpermalink":"/courses/numericalanalysis22/notebooks/2dfem/","section":"courses","summary":"Example of two-dimensional finite element simulation","tags":null,"title":"Two-Dimensional Finite Element Simulations","type":"book"},{"authors":["Santeri Kaupinmäki","Ben Cox","Simon Arridge","Christian Baker","David Sinden","Bajram Zeqiri"],"categories":null,"content":" ","date":1639180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639180800,"objectID":"2d4eecaa4050db96877c9f33fbedd384","permalink":"https://djps.github.io/publication/kaupinmaki2020pus/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/kaupinmaki2020pus/","section":"publication","summary":"Numerical and experimental investigation of directional response of pyroelectric sensor","tags":["Pyroelectric Sensors","Directivity"],"title":"Pyroelectric ultrasound sensor model: directional response","type":"publication"},{"authors":["Dongwoon Hyun","Alycen Wiacek","Sobhan Goudarzi","Sven Rothlübbers","Amir Asif","Klaus Eickel","Yonina C. Eldar","Jiaqi Huang","Massimo Mischi","Hassan Rivaz","David Sinden","Ruud J. G. van Sloun","Hannah Strohm","Muyinatu A. Lediju Bell"],"categories":null,"content":" ","date":1625443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625443200,"objectID":"5145f58f173b4e2a2ba6ed03150f2ea6","permalink":"https://djps.github.io/publication/hyun2021dlu/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/hyun2021dlu/","section":"publication","summary":"Overview of datasets and methods in beamforming challenge","tags":["beamforming","Imaging"],"title":"Deep learning for ultrasound image formation: CUBDL evaluation framework and open datasets","type":"publication"},{"authors":["Sven Rothlübbers","Hannah Strohm","Klaus Eickel","Jürgen Jenne","Vincent Kuhlen","David Sinden","Matthias Günther"],"categories":null,"content":" ","date":1605571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605571200,"objectID":"808a8d33d23c3c0dffbeedbad4f313df","permalink":"https://djps.github.io/publication/rothluebbers2020iiq/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/rothluebbers2020iiq/","section":"publication","summary":"Deep learning methods for beamforming","tags":["beamforming","Imaging"],"title":"Improving image quality of single plane wave ultrasound via deep learning based channel compounding","type":"publication"},{"authors":["Nadia Smith","David Sinden","Spencer A. Thomas","Marina Romanchikova","Jessica E Talbott","Michael Adeogun"],"categories":null,"content":" ","date":1580169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580169600,"objectID":"8b01d17e59816199aef0028dd0b82957","permalink":"https://djps.github.io/publication/smith2020bcd/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/smith2020bcd/","section":"publication","summary":"Healthcare is increasingly and routinely generating large volumes of data from different sources, which are difficult to handle and integrate. Confidence in data can be established through the knowledge that the data are validated, well-curated and with minimal bias or errors. As the National Measurement Institute of the UK, the National Physical Laboratory (NPL) is running an interdisciplinary project on digital health data curation. The project addresses one of the key challenges of the UK’s Measurement Strategy, to provide confidence in the intelligent and effective use of data. A workshop was organised by NPL in which important stakeholders from NHS, industry and academia outlined the current and future challenges in healthcare data curation. This paper summarises the findings of the workshop and outlines NPL’s views on how a metrological approach to the curation of healthcare data sets could help solve some of the important and emerging challenges of utilising healthcare data.","tags":["Digital Health","Metrology"],"title":"Building confidence in digital health through metrology","type":"publication"},{"authors":null,"categories":null,"content":"This notebook uses the scipy lagrange function to compute the Lagrange polynomial.\nimport numpy as np from numpy.polynomial.polynomial import Polynomial from scipy.interpolate import lagrange import matplotlib.pyplot as plt from IPython.display import Markdown as md plt.style.use(\u0026#39;seaborn-poster\u0026#39;) For some points $x$, define some data $y$ and create the Lagrange polynomial $f$, which has coefficients $p$\nx = [0.0, 1.0, 2.0] y = [1.0, 3.0, 2.0] f = lagrange(x, y) p = Polynomial(f.coef[::-1]).coef z = [\u0026#39;\u0026#39; if i==0 else (\u0026#39;x\u0026#39; if (i==1) else \u0026#39;x^{}\u0026#39;.format(i)) for i in np.arange(np.size(x))] string= [str(p[i]) + str(z[i]) for i in np.arange(np.size(x))] print(string) md(\u0026#34;Coefficients of the interpolating polynomial: {}\u0026#34;.format(p)) [\u0026#39;1.0\u0026#39;, \u0026#39;3.5x\u0026#39;, \u0026#39;-1.5x^2\u0026#39;] Coefficients of the interpolating polynomial: [ 1. 3.5 -1.5]\ndef lagrange_basis(x_int, y_int, x_new): \u0026#34;\u0026#34;\u0026#34; This function takes pairs of points (x_int, y_int) and, from a set of points x_new computes the Lagrange polynomial to return the interpolated values y_new \u0026#34;\u0026#34;\u0026#34; l = np.zeros((np.size(x_int), np.size(x_new)), dtype=np.float) i = np.int(0) for xi, yi in zip(x_int, y_int): l[i,:] = np.prod( [(x_new - xj) / (xi - xj) for xj in x_int if xi != xj], axis=0) i=i+1 return l Plot the data and the individual components of the polynomial\nx_new = np.arange(-1.0, 3.1, 0.1) test = lagrange_basis(x, y, x_new) fig = plt.figure() fig.subplots_adjust(top=1.3, hspace=0.4) ax=fig.add_subplot(3,1,1) ax.axvline(x=0.0, color=\u0026#34;grey\u0026#34;, linestyle=\u0026#34;-\u0026#34;, linewidth=1.75) ax.plot(x_new, f(x_new), \u0026#39;b\u0026#39;, x, y, \u0026#39;ro\u0026#39;) ax.set_title(r\u0026#39;Lagrange Interpolation\u0026#39;, fontsize=12) ax.grid() ax.set_ylabel(r\u0026#39;$y$\u0026#39;) ax.tick_params(axis = \u0026#39;both\u0026#39;, which = \u0026#39;major\u0026#39;, labelsize = 12) ax1=fig.add_subplot(3,1,2) ax1.axhline(y=0.0, color=\u0026#34;grey\u0026#34;, linestyle=\u0026#34;-\u0026#34;, linewidth=1.75) ax1.axhline(y=1.0, color=\u0026#34;grey\u0026#34;, linestyle=\u0026#34;-\u0026#34;, linewidth=1.75) ax1.scatter(np.array(x), np.array(y), c=[\u0026#39;tab:blue\u0026#39;, \u0026#39;tab:orange\u0026#39;, \u0026#39;tab:green\u0026#39;]) ax1.plot(x_new, test[0,:], color=\u0026#39;tab:blue\u0026#39;, label=\u0026#39;$l_0(x)$\u0026#39; ) ax1.plot(x_new, test[1,:], color=\u0026#39;tab:orange\u0026#39;, label=\u0026#39;$l_1(x)$\u0026#39; ) ax1.plot(x_new, test[2,:], color=\u0026#39;tab:green\u0026#39;, label=\u0026#39;$l_2(x)$\u0026#39; ) ax1.set_ylim([-0.5, 6.5]) ax1.grid() ax1.set_ylabel(r\u0026#39;$y$\u0026#39;) ax1.set_title(r\u0026#39;Lagrange Basis Functions\u0026#39;, fontsize=12) ax1.legend(prop={\u0026#39;size\u0026#39;: 12}) ax1.tick_params(axis = \u0026#39;both\u0026#39;, which = \u0026#39;major\u0026#39;, labelsize = 12) ax2=fig.add_subplot(3,1,3) ax2.grid() ax2.axhline(y=0.0, color=\u0026#34;grey\u0026#34;, linestyle=\u0026#34;-\u0026#34;, linewidth=1.75) ax2.axhline(y=1.0, color=\u0026#34;grey\u0026#34;, linestyle=\u0026#34;-\u0026#34;, linewidth=1.75) ax2.scatter(np.array(x), np.array(y), c=[\u0026#39;tab:blue\u0026#39;, \u0026#39;tab:orange\u0026#39;, \u0026#39;tab:green\u0026#39;]) ax2.plot(x_new, y[0]*test[0,:], color=\u0026#39;tab:blue\u0026#39;, label=\u0026#39;$y_0 l_0(x)$\u0026#39;) ax2.plot(x_new, y[1]*test[1,:], color=\u0026#39;tab:orange\u0026#39;, label=\u0026#39;$y_1 l_1(x)$\u0026#39; ) ax2.plot(x_new, y[2]*test[2,:], color=\u0026#39;tab:green\u0026#39;, label=\u0026#39;$y_2 l_2(x)$\u0026#39; ) ax2.set_xlabel(r\u0026#39;$x$\u0026#39;, fontsize=12) ax2.set_ylabel(r\u0026#39;$y$\u0026#39;, fontsize=12) ax2.set_title(r\u0026#39;Scaled Lagrange Basis Functions\u0026#39;, fontsize=12) ax2.legend(prop={\u0026#39;size\u0026#39;: 12}) ax2.tick_params(axis = \u0026#39;both\u0026#39;, which = \u0026#39;major\u0026#39;, labelsize = 12) plt.show() Now use a different set of points, defined by a function $x \\log(x)$.\ng = lambda x: x * np.log(x) We can compute the polynomials of different orders and the errors.\nm = np.array([3,8]) x0 = 0.1 x1 = 2.0 dx = 0.01 x_new = np.arange(x0, x1, dx) err = np.zeros( (m.size, x_new.size) ) y_new = np.zeros( (m.size, x_new.size) ) fig1 = plt.figure() fig1.subplots_adjust(wspace=0.3) for i in np.arange(m.size): z = np.linspace(x0, x1, m[i]) y = g(z) f = lagrange(z, y) p = Polynomial(f.coef[::-1]).coef err[i,:] = g(x_new) - f(x_new) md(\u0026#34;Coefficients of the interpolating polynomial: {}\u0026#34;.format(p)) ax=fig1.add_subplot(1,m.size,i+1) clrs = \u0026#39;tab:orange\u0026#39; if (i==0) else \u0026#39;tab:green\u0026#39; ax.plot(x_new, f(x_new), \u0026#39;-\u0026#39;, c=clrs, label=\u0026#34;interpolation\u0026#34;) ax.plot(z, y, \u0026#39;ro\u0026#39;, label=\u0026#34;data\u0026#34;) ax.plot(x_new, np.log(x_new) * x_new, linestyle=\u0026#39;--\u0026#39;, label=\u0026#34;function\u0026#34;) ax.set_title(r\u0026#39;Lagrange Interpolation of order {}\u0026#39;.format(m[i])) ax.legend() ax.grid() ax.set_xlabel(r\u0026#39;$x$\u0026#39;) ax.set_ylabel(r\u0026#39;$y$\u0026#39;) plt.show() Plot the errors\nfig2=plt.figure() for i in np.arange(m.size): ax=fig2.add_subplot(m.size,1,i+1) clrs = \u0026#39;tab:orange\u0026#39; if (i==0) else \u0026#39;tab:green\u0026#39; ax.plot(x_new, err[i], c=clrs, label=\u0026#34;Error Order {}\u0026#34;.format(m[i])) ax.legend() ax.grid() ax.set_xlabel(r\u0026#39;$x$\u0026#39;) ax.set_ylabel(r\u0026#39;$y$\u0026#39;) plt.show() Compute the bounds on the errors.\nfig3=plt.figure() for i in np.arange(m.size): z = np.linspace(x0, x1, m[i]) bounds1 = np.prod( [(x_new - z[i]) for i in np.arange(0,m[i])], axis=0) bounds2 = (m[i]-2) * (-1)**(m[i]-2) / ( np.math.factorial(m[i]) * x_new**(m[i]-1) ) ax=fig3.add_subplot(m.size,1,i+1) b=np.max(np.abs(bounds1 * bounds2)) ax.plot(x_new, err[i], c=clrs, label=\u0026#34;Error Order {}\u0026#34;.format(m[i])) ax.axhline(y=b) ax.axhline(y=-b) ax.legend() ax.grid() ax.set_xlabel(r\u0026#39;$x$\u0026#39;) ax.set_ylabel(r\u0026#39;$y$\u0026#39;) plt.show() ","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"51370f48a77b14d42a7ae1576fc76c3d","permalink":"https://djps.github.io/courses/numericalanalysis22/notebooks/characteristicpolynomial/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/courses/numericalanalysis22/notebooks/characteristicpolynomial/","section":"courses","summary":"This notebook uses the scipy lagrange function to compute the Lagrange polynomial.\nimport numpy as np from numpy.polynomial.polynomial import Polynomial from scipy.interpolate import lagrange import matplotlib.pyplot as plt from IPython.display import Markdown as md plt.","tags":null,"title":"Lagrange Polynomials","type":"courses"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://djps.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Ki Joo Pahk","Pierre Gélat","David Sinden","Dipok Kumar Dhar","Nader Saffari"],"categories":null,"content":"","date":1510876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510876800,"objectID":"29de1223289ec590094c50feedf9b621","permalink":"https://djps.github.io/publication/pahk2017nes/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/pahk2017nes/","section":"publication","summary":"The aim of boiling histotripsy is to mechanically fractionate tissue as an alternative to thermal ablation for therapeutic applications. In general, the shape of a lesion produced by boiling histotripsy is tadpole like, consisting of a head and a tail. Although many studies have demonstrated the efficacy of boiling histotripsy for fractionating solid tumors, the exact mechanisms underpinning this phenomenon are not yet well understood, particularly the interaction of a boiling vapor bubble with incoming incident shockwaves. To investigate the mechanisms involved in boiling histotripsy, a high-speed camera with a passive cavitation detection system was used to observe the dynamics of bubbles produced in optically transparent tissue-mimicking gel phantoms exposed to the field of a 2.0-MHz high-intensity focused ultrasound (HIFU) transducer. We observed that boiling bubbles were generated in a localized heated region and cavitation clouds were subsequently induced ahead of the expanding bubble. This process was repeated with HIFU pulses and eventually resulted in a tadpole-shaped lesion. A simplified numerical model describing the scattering of the incident ultrasound wave by a vapor bubble was developed to help interpret the experimental observations. Together with the numerical results, these observations suggest that the overall size of a lesion induced by boiling histotripsy is dependent on the sizes of (i) the heated region at the HIFU focus and (ii) the backscattered acoustic field by the original vapor bubble.","tags":["histotripsy"],"title":"Numerical and experimental study of mechanisms involved in boiling histotripsy","type":"publication"},{"authors":["David Sinden","Srinath Rajagopal","N. Christopher Chaggares","Guofeng Pang","Oleg Ivanytskyy"],"categories":null,"content":" ","date":1509580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509580800,"objectID":"f96a83af13c99658cddf973cb2cc2fe0","permalink":"https://djps.github.io/publication/sinden2017rus/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/sinden2017rus/","section":"publication","summary":"Accurate characterization of ultrasound fields generated by diagnostic and therapeutic transducers is critical for patient safety. At high frequencies, spatial variations in the pressure over the surface of the measurement device will produce a different averaged value from that at the intended location. Using a small hydrophone as an idealised point device, this paper seeks to ascertain the spatial-averaging errors for a finite-area reference device along the beam axis in order to find the optimal measurement location.","tags":["Spatial Averaging","Hydrophones"],"title":"Reducing uncertainties for spatial averaging at high frequencies","type":"publication"},{"authors":["David Sinden","Gail ter Haar"],"categories":null,"content":"","date":1413244800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1413244800,"objectID":"afd896bc0d2bf2303ba2f9b40e6d7e1f","permalink":"https://djps.github.io/publication/sinden2014dic/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/sinden2014dic/","section":"publication","summary":"A discursive review of the fundamental requirements for ultrasound (US) treatment delivery in the context of the required dose is presented. The key components discussed are the acoustic, thermal and dose equations and their numerical solutions, the preoperative imaging modalities used, tissue characterisation, motion compensation and treatment planning strategies and how each component affects the others.","tags":[],"title":"Dosimetry implications for correct ultrasound dose deposition: uncertainties in descriptors, planning and treatment delivery","type":"publication"},{"authors":["David Sinden","Eleanor Stride","Nader Saffari"],"categories":null,"content":" ","date":1331251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1331251200,"objectID":"78d7281c1e5fdd5293e9eb4bd087b046","permalink":"https://djps.github.io/publication/sinden2008aae/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/sinden2008aae/","section":"publication","summary":"In this paper the effect of interaction on the expansion of a bubble in a regular monodisperse cluster is investigated. By a geometric construction a two-dimensional ordinary differential equation with an exact expression for first-order bubble interactions is derived for an n-bubble model. An approximate equation is derived for the rapid expansion of the bubble which can be solved yielding an analytic expression for the collapse of a bubble which undergoes inertial cavitation. It is then demonstrated that the maximum volume of a bubble in a cluster is considerably less than that of a single bubble. This result is of significance as typically the dispersion relationship, the wave speed and the co-efficient of attenuation are calculated using a single bubble model and summed for the total number of bubbles to yield the void fraction. Furthermore it is shown that the maximum radius of a bubble in the cluster is considerably smaller than that of a single bubble, yet the duration of the collapse phase is only weakly dependent on the number of bubbles. Hence, it is conjectured that the likelihood of fragmentation due to Rayleigh–Taylor instability is reduced. The results from the analysis are in good agreement with full numerical simulations of multi-bubble dynamics, as well as experimental observations","tags":[],"title":"Approximations for acoustically excited bubble cluster dynamics","type":"publication"},{"authors":["David Sinden","Gert van der Heijden"],"categories":null,"content":" ","date":1311552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1311552000,"objectID":"cf3e6cdee5b2dd5473fc011c52012ea3","permalink":"https://djps.github.io/publication/sinden2011bmc/","publishdate":"2011-08-28T00:00:00Z","relpermalink":"/publication/sinden2011bmc/","section":"publication","summary":"Magnetoc-striction destroys integrability and leads to spatially chaotic and pulse-pulsed homoclinic solutions","tags":["Cosserat"],"title":"The buckling of magneto-strictive Cosserat rods","type":"publication"},{"authors":["Gert van der Heijden","David Sinden"],"categories":null,"content":" ","date":1308528000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1308528000,"objectID":"0a7bc108b00b9625094555bea64dfcc7","permalink":"https://djps.github.io/publication/vanderheijden2011ltc/","publishdate":"2009-08-28T00:00:00Z","relpermalink":"/publication/vanderheijden2011ltc/","section":"publication","summary":"Using quaternions, the ten-dimensional equilibrium equations have a periodic solution but can undergo a co-dimension two Hamiltonian-Hopf-Hopf bifurctaion","tags":["Cosserat"],"title":"Localisation of a twisted conducting rod in a uniform magnetic field: the Hamiltonian-Hopf-Hopf bifurcation","type":"publication"},{"authors":["David Sinden","Eleanor Stride","Nader Saffari"],"categories":null,"content":" ","date":1258934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1258934400,"objectID":"e89908d411b3e355f822e1df625e3706","permalink":"https://djps.github.io/publication/sinden2009enw/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/sinden2009enw/","section":"publication","summary":"In the context of forecasting temperature and pressure fields generated by high-intensity focussed ultrasound, the accuracy of predictive models is critical for the safety and efficacy of treatment. In such fields 'inertial' cavitation is often observed. Classically, estimations of cavitation thresholds have been based on the assumption that the incident wave at the surface of a bubble is the same as in the far-field, neglecting the effect of nonlinear wave propagation. By modelling the incident wave as a solution to Burgers' equation using weak shock theory, the effects of nonlinear wave propagation on inertial cavitation are investigated using both numerical and analytical techniques. From radius-time curves for a single bubble, it is observed that there is a reduction in the maximum size of a bubble undergoing inertial cavitation and that the inertial collapse occurs earlier in contrast with the classical case. Corresponding stability thresholds for a bubble whose initial radius is slightly below the critical Blake radius are calculated, providing a lower bound for the onset of instability. Bifurcation diagrams and frequency-response curves are presented associated with the loss of stability. The consequences and physical implications of the results are discussed with respect to the classical results.","tags":[],"title":"The effects of nonlinear wave propagation on the stability of inertial cavitation","type":"publication"},{"authors":["David Sinden"],"categories":null,"content":" ","date":1245456000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1245456000,"objectID":"0f40cf545adf79432be77d9037b7794b","permalink":"https://djps.github.io/publication/thesis/","publishdate":"2009-03-31T00:00:00Z","relpermalink":"/publication/thesis/","section":"publication","summary":"A conducting rod in uniform magnetic field is super integrable, but extensibility can break the integrability and lead to spatially complex localisation.","tags":["Cosserat"],"title":"Integrability, Localisation and Bifurcation of an Elastic Conducting Rod in a Uniform Magnetic Field","type":"publication"},{"authors":["David Sinden","Gert van der Heijden"],"categories":null,"content":" A preprint can be found on the arxiv.\n","date":1245456000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1245456000,"objectID":"12840161988f7f3a8d16a0c9173bc2b3","permalink":"https://djps.github.io/publication/sinden2009sce/","publishdate":"2009-08-28T00:00:00Z","relpermalink":"/publication/sinden2009sce/","section":"publication","summary":"Extensibility destroy integrability for a conducting rod in a uniform magnetic field","tags":["Cosserat"],"title":"Spatial chaos of an extensible conducting rod in a uniform magnetic field","type":"publication"},{"authors":["David Sinden","Gert van der Heijden"],"categories":null,"content":" A preprint can be found on the arxiv.\n","date":1213920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1213920000,"objectID":"525d99267a5aa1dc638261b218be5065","permalink":"https://djps.github.io/publication/sinden2008ice/","publishdate":"2008-01-15T00:00:00Z","relpermalink":"/publication/sinden2008ice/","section":"publication","summary":"A conducting rod in uniform magnetic field is super integrable","tags":["Cosserat"],"title":"Integrability of a conducting elastic rod in a magnetic field","type":"publication"}]