
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"David Sinden is an applied mathematician working as a senior scientist at the Fraunhofer Institute for Digital Medicine MEVIS, in Bremen. His research interests include ultrasound modelling, thermal ablation and pharmacokinetic models. He works within the modelling and simulation and image-guided therapy groups.\nDownload my resumé.\n","date":1686700800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1687305600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"David Sinden is an applied mathematician working as a senior scientist at the Fraunhofer Institute for Digital Medicine MEVIS, in Bremen. His research interests include ultrasound modelling, thermal ablation and pharmacokinetic models.","tags":null,"title":"David Sinden","type":"authors"},{"authors":null,"categories":null,"content":"This site will be the primary source of course information.\nLectures Lectures will take place in spring 2024, Wednesdays (RLH-172 Conrad Naber Lecture Hall) and Thursdays (R.2-52 Lecture Hall) at 14:15.\nThe first lecture will be on Thursday 1 February, the last on Wednesdays 15 May.\nAn office hour will be announced.\nCourse notes will be uploaded here at a later date.\nIntended Learning Outcomes By the end of the module, students will be able to:\ndescribe the basic principles of discretization which are used in the numerical treatment of continuous problems; command the methods described in the content section of this module description to the extent that they can solve standard text-book problems reliably and with confidence; recognize mathematical terminology used in textbooks and research papers on numerical methods in the quantitative sciences, engineering, and mathematics to the extent that they fall into the content categories covered in this module; implement simple numerical algorithms in a high level programming language; understand the documentation of standard numerical library code and understand potential limitations and caveats of such algorithms. Knowledge, Abilities, or Skills Knowledge of Calculus: functions, inverse functions, sets, real numbers, sequences and limits, polynomials, rational functions, trigonometric functions, logarithm and exponential function, parametric equations, tangent lines, graphs, derivatives, anti-derivatives, elementary techniques for solving equations\nKnowledge of Linear Algebra: vectors, matrices, lines, planes, $n$-dimensional Euclidean vector space, rotation, translation, dot product (scalar product), cross product, normal vectors, eigenvalues, eigenvectors, elementary techniques for solving systems of linear equations\nSome examples will be presented as python notebooks, but no knowledge of python is required, nor will there be any assessment of ability to code.\nRecommendations for Preparation Taking “Calculus and Elements of Linear Algebra II” (CTMS-MAT-10) before taking this module is recommended, but not a pre-requisite. A thorough review of “Calculus and Elements of Linear Algebra” (CTMS-MAT-09), with emphasis on the topics listed as “Content and Educational Aims” is recommended.\nUsability and Relationship to other Modules The module is a mandatory / mandatory elective module of the Methods and Skills area that is part of the Constructor Track (Methods and Skills modules; Community Impact Project module; Language modules; Big Questions modules). This module is a co-recommendation for the module “Applied Dynamical Systems Lab” (CA-S-MAT-810) in which the actual implementation in a high level programming language of the learned methods will be covered. The module is a mandatory / mandatory elective module of the Methods and Skills area that is part of the Constructor Track (Methods and Skills modules; Community Impact Project module; Language modules; Big Questions modules). Mandatory for ECE, IMS, MATH, PHY. Elective for all other study programs. Text Books There is no primary or required book, but there are many suitable books, such as\n“An Introduction to Numerical Methods and Analysis” - J. F. Epperson (2013) Wiley 2nd Edition. “Numerical Analysis” - R. L. Burden and J. D. Faires (2011) Brooks/Cole. 9th Edition. Course Outline The following topics will be covered:\nTaylor series\nnumber representations\nGaussian elimination\nLU decomposition\nCholesky decomposition\niterative methods\nbisection method\nNewton’s method\nsecant method\npolynomial interpolation\nAitken’s algorithm\nLagrange interpolation\nNewton interpolation\nHermite interpolation\nBezier curves\nDe Casteljau’s algorithm\npiecewise interpolation\nspline interpolation\n$b$-splines\nleast squares approximation\npolynomial regression\ndifference schemes\nRichardson extrapolation\nquadrature rules\nMonte Carlo integration\ntime stepping schemes for ordinary differential equations\nRunge-Kutta schemes\nfinite difference method for partial differential equations\nItinerary Provisionally, the course will run as follows\ngantt\rdateFormat MM-DD\raxisFormat %m-%d\rtodayMarker off\rtitle Provisional Schedule\rsection Errors\rTaylor Series :a1, 02-01, 1w\rNumber Representations :after a1, 1w\rsection Equations\rLinear Equations :a2, 02-14, 3w\rNonlinear Equations :after a2, 3w\rsection Approximations\rIntegration :a3, 04-03, 2w\rInterpolation :after a3, 2w\rsection Solutions\rOrdinary Differential Equations :a4, 05-02, 2w\rsection Summary\rSummary :a5, 05-15, 1w Structure The course content has the following structure:\n- Numerical Methods\r- Errors\r- Taylor Series\r- Number Representations\r- Algebraic Equations\r- Linear Equations\r- Direct Methods\r- Gaussian Elimination\r- Partial Pivoting\r- LU Decomposition\r- Cholesky Decomposition\r- Iterative Methods\r- Jacobi\r- Gauss-Seidel\r- SOR\r- Nonlinear Equations\r- Bisection Method\r- Newton Method\r- Secant Method\r- Approximations\r- Interpolation\r- Hermite\r- Bezier\r- $b$-splines\r- Integration\r- Simpson\r- Trapezium …","date":1699920000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1701993600,"objectID":"2b286f6ec5e9283d78941059a5e34c43","permalink":"https://djps.github.io/courses/numericalmethods24/","publishdate":"2023-11-14T00:00:00Z","relpermalink":"/courses/numericalmethods24/","section":"courses","summary":"Numerical Methods JTMS-MAT-13 -- taught at [Constructor University](https://constructor.university/), Bremen 2024","tags":null,"title":"JTMS-MAT-13: Numerical Methods","type":"book"},{"authors":null,"categories":null,"content":"Notebooks A collection of jupyter notebooks, referenced in the lecture notes, can be found here:\nTaylor Series: web, or python script, or ipynb. Number Representations: web Linear Solvers: web Application of secant method Solvers: web Nonlinear Solvers: web Runge: web Bezier curves: web Iterative Solvers: web ODE Solvers: web ","date":1701907200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1701907200,"objectID":"755b7cdd843bb692251437c9976e7d19","permalink":"https://djps.github.io/courses/numericalmethods24/notebooks/","publishdate":"2023-12-07T00:00:00Z","relpermalink":"/courses/numericalmethods24/notebooks/","section":"courses","summary":"Notebooks: Numerical Methods JTMS-MAT-13","tags":null,"title":"Numerical Methods: Notebooks","type":"book"},{"authors":null,"categories":null,"content":"Lectures Lectures take place: 11:15-12:30 Tuesday Thursday (East Hall 8) and Fridays (East Hall 4), from 1 February until 13 May 2022.\nContent and Educational Aims The module is an introduction to the analysis of basic classes of numerical algorithms used in large-scale scientific computing. It introduces the fundamental notions and concepts of numerical mathematics. Then, successively, iterative solvers, interpolation, and quadrature are discussed and analyzed. They serve as the core numerical building blocks for an introduction to the finite element method (FEM) as one of the modern numerical techniques widely used in engineering applications and theoretical physics.\nText Books “Numerical Mathematics” - Quarteroni, Sacco and Saleri (2007) “A First Course in the Numerical Analysis of Differential Equations” - Iserles (2012) The following are also useful:\n“An Introduction to the Conjugate Gradient Method Without the Agonizing Pain” - Shewchuk (1994). Also see [here] “An Introduction to Computational Stochastic PDEs” - Lord, Powell and Shardlow (2014). [Chapters 1 \u0026amp; 2] Course Outline The following topics will be covered:\nPart 1: Foundations Principles of numerical mathematics: well-posedness, stability, robustness, condition, and consistency Equivalence theorem of Lax-Richtmyer for exemplary problems Types of error analysis (forward, backward, a priori, and a posteriori) Sources of errors (modelling, data, discretization, rounding, and truncation) Foundations of matrix analysis: vector norms and matrix norms and compatible/consistent norms Stability analysis for linear systems: the condition number of a matrix, forward/backward a priori analysis, and the convergence of iterative methods Iterative methods: gradient descent and conjugate gradient method Part 2: Interpolation and Numerical Integration Review of Lagrange interpolation, error estimates, drawbacks, Runge’s counterexample the stability of polynomial interpolation, piecewise Lagrange interpolation, and extensions to the multi-dimensional case Quadrature formulas: interpolatory quadrature, error estimates, Gauss quadrature, degree of exactness, and extensions to the multi-dimensional case Part 3: Numerical Solutions of Differential Equations Finite difference methods (FDM), stability and convergence analysis for FDM and error estimates for FDM The notion of a weak solution The Galerkin method and the Finite Element Method (FEM) Error estimates for FEM: Céa’s lemma and approximate estimates Particular examples in 1D and 2D and linear and quadratic shape functions Assessment Examination Type: Module examination Assessment Type: Written examination Scope: All intended learning outcomes of this module Duration: 120 min Weight: 100% By submitting homework you can improve this grade by up to 0.66 points. More details about homework will be announced in class.\n","date":1644278400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657756800,"objectID":"197a80802ed09249031aa21f1f393a2e","permalink":"https://djps.github.io/courses/numericalanalysis22/","publishdate":"2022-02-08T00:00:00Z","relpermalink":"/courses/numericalanalysis22/","section":"courses","summary":"Numerical Analysis CA-S-MATH-804 -- taught at [Jacobs University](https://www.jacobs-university.de/), Bremen 2022","tags":null,"title":"CA-S-MATH-804: Numerical Analysis","type":"book"},{"authors":null,"categories":null,"content":"Assignments To appear:\nAssignment 1 Assignment 2 Assignment 3 Assignment 4 Assignment 5 Assignment 6 ","date":1699920000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1699920000,"objectID":"ca69e6dc950b31481b114a4e9d800a64","permalink":"https://djps.github.io/courses/numericalmethods24/assignments/","publishdate":"2023-11-14T00:00:00Z","relpermalink":"/courses/numericalmethods24/assignments/","section":"courses","summary":"Assignments: Numerical Methods JTMS-MAT-13","tags":null,"title":"Numerical Methods: Assignments","type":"book"},{"authors":null,"categories":null,"content":"Integration Lecture notes on integration can be found here.\nProblem sheet 1 and solutions. Problem sheet 2 and solutions. Problem sheet 3 and solutions. Mid-year test End of year test Linear Algebra Lecture notes on linear algebra can be found here.\nProblem sheet 1 Problem sheet 2 Mid-year test ","date":1642291200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1676505600,"objectID":"2e160078e49b4c4f6b91b80fb503b96c","permalink":"https://djps.github.io/courses/mech1010/","publishdate":"2022-01-16T00:00:00Z","relpermalink":"/courses/mech1010/","section":"courses","summary":"Modelling and Analysis in Engineering I -- taught at [UCL Department of Mechanical Engineering](https://www.ucl.ac.uk/mechanical-engineering/ucl-mechanical-engineering) 2010, 2011","tags":null,"title":"MECH1010: Modelling and Analysis in Engineering I","type":"book"},{"authors":null,"categories":null,"content":"Class Tests Sample tests from 2008 and 2009.\nSample Exam Sample exam from 2008.\n","date":1642204800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1676505600,"objectID":"c05fc51faa08a007fb7de298ea461bb7","permalink":"https://djps.github.io/courses/math6502/","publishdate":"2022-01-15T00:00:00Z","relpermalink":"/courses/math6502/","section":"courses","summary":"Mathematics for Engineers II -- taught at [UCL Department of Civil, Environment and Geomatic Engineering](https://www.ucl.ac.uk/civil-environmental-geomatic-engineering/ucl-civil-environmental-and-geomatic-engineering) 2008, 2009","tags":null,"title":"Math6502: Mathematics for Engineers II","type":"book"},{"authors":null,"categories":null,"content":"Assignments Assignment 1 Assignment 2 Assignment 3 Assignment 4 Assignment 5 Assignment 6 ","date":1610064000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1610150400,"objectID":"e01c8d28dfe456f729679587345061f4","permalink":"https://djps.github.io/courses/numericalanalysis22/assignments/","publishdate":"2021-01-08T00:00:00Z","relpermalink":"/courses/numericalanalysis22/assignments/","section":"courses","summary":"Assignments Numerical Analysis CMATH-802","tags":null,"title":"Assignments","type":"book"},{"authors":null,"categories":null,"content":"Course Notes The course notes can be found here.\n","date":1610064000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1610150400,"objectID":"15130afb659f8ca9245129470c3160e6","permalink":"https://djps.github.io/courses/numericalanalysis22/notes/","publishdate":"2021-01-08T00:00:00Z","relpermalink":"/courses/numericalanalysis22/notes/","section":"courses","summary":"Course Notes for Numerical Analysis CMATH-802","tags":null,"title":"Course Notes","type":"book"},{"authors":null,"categories":null,"content":"A collection of jupyter notebooks, referenced in the lecture notes, can be found here:\nLagrange Interpolation Lagrange Polynomials Numerical Integration One-dimensional finite element simulations Two-dimensional finite element simulations ","date":1655424000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1655424000,"objectID":"37eb826f66eb937104975ea8bc41d93e","permalink":"https://djps.github.io/courses/numericalanalysis22/notebooks/","publishdate":"2022-06-17T00:00:00Z","relpermalink":"/courses/numericalanalysis22/notebooks/","section":"courses","summary":"Notebooks","tags":null,"title":"Python Notebooks","type":"book"},{"authors":null,"categories":null,"content":"\rDavid Sinden ·\rDecember 07, 2023 ·\r15 minute read Taylor Series The Taylor series, or Taylor expansion of a function, is defined as\nDefinition:\tTaylor Series For a function $f : \\mathbb{R} \\mapsto \\mathbb{R}$ which is infinitely differentiable at a point $c$, the Taylor series of $f(c)$ is given by \\begin{equation} \\sum\\limits_{k=0}^{\\infty} \\dfrac{ f^{(k)} \\left( c \\right) }{k!} \\left( x - c \\right)^{k}. \\end{equation} This is a power series, which is convergent for some radius.\nTheorem:\tTaylor\u0026#39;s Theorem For a function $f \\in C^{n+1}\\left([a, b]\\right)$, i.e. $f$ is $(n+1)$-times continuously differentiable in the interval $[a, b]$, then for some $c$ in the interval, the function can be written as \\begin{equation} f\\left( x \\right) = \\sum\\limits_{k=0}^{n} \\dfrac{f^{(k)} \\left(c\\right) }{k!} \\left( x- c \\right)^{k} + \\dfrac{f^{(n+1)} \\left( \\xi \\right) }{\\left( n + 1 \\right)!} \\left( x - c \\right)^{n+1} \\end{equation} for some value $\\xi \\in \\left[ a, b \\right]$ where \\begin{equation} \\lim\\limits_{\\xi \\rightarrow c} \\dfrac{ f^{(n+1)} \\left( \\xi \\right) }{ \\left( n + 1 \\right)!} \\left( x - c \\right)^{n+1} = 0. \\end{equation} Example:\tAn .ipynb notebook with an example of the Taylor series for $\\sin\\left(x\\right)$ can be accessed online here.\nIt can be downloaded from here as a python file or downloaded as a notebook from here.\n","date":1701907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701907200,"objectID":"6666bab7b0cb02eed099225af90772a4","permalink":"https://djps.github.io/courses/numericalmethods24/part-1/taylor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalmethods24/part-1/taylor/","section":"courses","summary":"Analysis","tags":null,"title":"Taylor Series","type":"book"},{"authors":null,"categories":null,"content":"\rDavid Sinden ·\rJanuary 05, 2024 ·\r25 minute read Errors Errors in data Round-off errors Truncation errors Definition:\tAbsolute and Relative Errors Let $\\tilde{a}$ be an approximation to $a$, then the absolute error is given by $\\left| {\\tilde{a}-a} \\right|$.\nIf ${a \\ne 0}$, the relative error is given by $\\left| \\dfrac{\\tilde{a}-a}{a} \\right|$ and the error bound is the magnitude of the admissible error.\nTheorem:\tFor both addition and subtraction the bounds for the absolute error are added.\nIn division and multiplication the bounds for the relative errors are added.\nError propagation\nNumerical methods are a lot of computational schemes to solve mathematical problems when analytical solutions can’t be found.\nHow does the representation of a number in computer memory affect calculations?\nNumber Representations Definition:\tBase Representation Let $b \\in \\mathbb{N} \\backslash \\lbrace 1 \\rbrace$. Every number $x \\in \\mathbb{N}_0$ can be written as a unique expansion with respect to base $b$ as\n\\begin{equation} x = a_0 b^0 + a_1 b^1 + \\ldots a_n b^n = \\sum \\limits_{i=0}^{n} a_i b^i \\end{equation}\nwith $a_i \u0026lt; \\mathbb{N}_0$ and $a_i \u0026lt; b$, i.e. ${a_i \\in \\lbrace 0, \\ldots, b-1 \\rbrace }$\nAlgorithm:\tEuclid Euclid’s algorithm can convert number $x$ in base 10, i.e. $\\left(x \\right)_{10}$ into another base, $b$, i.e. $\\left(y \\right)_{b}$.\nInput $\\left(x \\right)_{10}$ Determine the smallest integer $n$ such that ${x \u0026lt; b^{n+1}}$ Let $y=x$. Then for $i=n, \\ldots, 0$ $$ \\begin{array}{rcl} a_i \u0026amp; = \u0026amp; y \\mbox{ div } b^i \\\\ y \u0026amp; = \u0026amp; y \\mbox{ mod } b^i \\end{array} $$ Output as $\\left(x \\right)_{10} = a_n a_{n-1} \\cdots a_0$ Algorithm:\tHorner Here Definition:\tNormalized Floating Point Representations Normalized floating point representations with respect to some base $b$, store a number $x$ as $$x= 0 \\cdot a_1 \\ldots a_k \\times b^n$$ where the $a_i \\in \\lbrace 0, 1, \\ldots b-1 \\rbrace$ are called the digits, $k$ is the precision and $n$ is the exponent. The set $a_1, \\ldots, a_k$ is called the mantissa. Impose that $a_1 \\ne 0$, it makes the representation unique. Definition:\tSignificant Bits Here Theorem:\tLet $x$ and $y$ be two normalized floating point numbers with ${x \u0026gt; y \u0026gt; 0}$ and base ${b=2}$. If there exists integers $p$ and ${q \\in \\mathbb{N}_0}$ such that $$2^{-p} \\leq 1 - \\dfrac{y}{x} \\leq 2^{-q}$$ then, at most $p$ and at least $q$ significant bits (i.e. digits at base $2$) are lost during subtraction. ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"6617848a8bf72bf7cecef436a83a2ecb","permalink":"https://djps.github.io/courses/numericalmethods24/part-1/number-representations/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalmethods24/part-1/number-representations/","section":"courses","summary":"Number representations","tags":null,"title":"Number Representations","type":"book"},{"authors":null,"categories":null,"content":" David Sinden · March 11, 2022 · 45 minute read Find $x$ such that $F(x,d)=0$ for a set of data, $d$ and $F$, a functional relationship between $x$ and $d$.\nWell Posed Problems Definition:\tWell-Posed Problems A problem is said to be well-posed if\na solution exists, the solution is unique, the solution’s behaviour changes continuously with the initial conditions. A problem which does not have these properties is said to be ill-posed.\nCondition Number Definition:\tRelative and Absolute Condition Numbers The relative condition number of a problem is given by: \\begin{equation} K ( d ) = \\sup\\limits_{\\delta d \\in \\mathcal{D}} \\dfrac{ \\left\\Vert \\delta x \\right\\Vert / \\left\\Vert x \\right\\Vert}{\\left\\Vert \\delta d \\right\\Vert / \\left\\Vert d \\right\\Vert} \\end{equation} if either $x=0$ or $d=0$, the absolute condition number is then \\begin{equation} K_{\\mathrm{abs}} ( d ) = \\sup\\limits_{\\delta d \\in \\mathcal{D}}\\dfrac{ \\left\\Vert \\delta x \\right\\Vert }{\\left\\Vert \\delta d \\right\\Vert} \\end{equation} Stability Consider a well-posed problem, a construct a sequence of approximate solutions via a sequence of approximate solutions and data, i.e. $F_n (x_n, d_n)=0$\nDefinition:\tConsistency If the $d$ is admissible for $F_n$ then a numerical method $F_n (x_n, d_n)=0$ is consistent if \\begin{equation} \\lim\\limits_{n\\rightarrow\\infty} F_n (x,d) \\rightarrow F(x,d). \\end{equation} The method is strongly consistent if $F_n(x,d)=0$ for all $n\\ge0$. Definition:\tAsymptotic and Relative Condition Numbers If the sets of functions for $F_n (x_n, d_n)=0$ and $F(x,d)=0$ coincide, that is \\begin{equation} K_n \\left( d_n \\right) = \\sup\\limits_{\\delta d_n \\in \\mathcal{D}_n } \\dfrac{ \\left\\Vert \\delta x_n \\right\\Vert / \\left\\Vert x_n \\right\\Vert }{\\left\\Vert \\delta d_n \\right\\Vert / \\left\\Vert d_n \\right\\Vert} \\end{equation} and\n$$K_{n,\\mathrm{abs}} \\left( d_n \\right) = \\sup \\limits_{\\delta d_n \\in \\mathcal{D}_n} \\dfrac{ \\left\\Vert \\delta x_n \\right\\Vert }{ \\left\\Vert \\delta d_n \\right\\Vert }$$\nthen the asymptotic condition number is\n$$K^{\\mathrm{num}} (d) = \\lim \\limits_{k\\rightarrow \\infty} \\sup\\limits_{n \\le k} K_n \\left( d_n \\right).$$\nThe relative condition number is:\n\\begin{equation} K_{\\mathrm{abs}}^{\\mathrm{num}} \\left( d \\right) = \\lim \\limits_{k\\rightarrow \\infty} \\sup \\limits_{n \\le k} K_{n, \\mathrm{abs}} \\left( d_n \\right). \\end{equation}\nDefinition:\tConvergence A method is convergent if and only if: \\begin{equation} \\forall \\varepsilon \u0026gt; 0, \\quad \\exists , n \\quad \\mbox{such that} \\quad \\left\\Vert x(d) - x_n\\left( d+ \\delta d_n \\right) \\right\\Vert \\le \\varepsilon \\end{equation} Theorem:\tLax-Ritchmyer A numerical algorithm converges if and only if it is consistent and stable. Matrix Analysis Theorem:\tLet $A \\in \\mathbb{R}^{n \\times n}$, then\n$\\lim\\limits_{k \\rightarrow \\infty}A^k =0 \\Leftrightarrow \\rho \\left( A \\right) \u0026lt; 1$. Where $\\rho\\left(A \\right)$ is the largest absolute value of the eigenvalues of $A$. This is called the spectral radius\nThe geometric series, $\\sum\\limits_{k=0}^{\\infty}A^k$ is convergent if and only if $\\rho \\left( A \\right) \u0026lt; 1$. Then in this case, the sum is given by \\begin{equation} \\sum\\limits_{k=0}^{\\infty}A^k = \\left( I - A \\right)^{-1} \\end{equation}\nThus, if $\\rho \\left( A \\right) \u0026lt; 1$, the matrix $I-A$ is invertible and \\begin{equation} \\dfrac{1}{1+\\left\\Vert A \\right\\Vert} \\le \\left\\Vert\\left( I - A \\right)^{-1}\\right\\Vert \\le \\dfrac{1}{1-\\left\\Vert A \\right\\Vert} \\end{equation} where $\\left\\Vert \\cdot \\right\\Vert$ is an induced matrix norm such that $\\left\\Vert A \\right\\Vert \u0026lt;1$.\nTheorem:\tLet $A \\in \\mathbb{R}^{n \\times n}$ be non-singular and let $\\delta A \\in \\mathbb{R}^{n \\times n}$ be such that $\\left\\Vert A^{-1} \\right\\Vert \\bigl\\Vert \\delta A \\bigr\\Vert \u0026lt; 1$. Furthermore, if $x \\in \\mathbb{R}^n$ is a solution to $Ax=b$, where $b \\in \\mathbb{R}^n$ and $b \\neq 0$ and $\\delta x$ is such that \\begin{equation} \\left( A + \\delta A \\right)\\left( x + \\delta x \\right) = b + \\delta b \\end{equation} for a $\\delta b \\in \\mathbb{R}^n$, then \\begin{equation} \\left( A + \\delta A \\right)\\left( x + \\delta x \\right) \\le \\dfrac{K(A)}{1- K(A) \\left\\Vert \\delta A \\right\\Vert_2 / \\left\\Vert A \\right\\Vert_2} \\left(\\dfrac{\\left\\Vert \\delta b \\right\\Vert_2}{\\left\\Vert b \\right\\Vert_2} + \\dfrac{\\left\\Vert \\delta A \\right\\Vert_2}{\\left\\Vert A \\right\\Vert_2} \\right) \\end{equation} Theorem:\tLet $A \\in \\mathbb{R}^{n \\times n}$ be non-singular and if $x \\in \\mathbb{R}^n$ is a solution to $Ax=b$, where $b \\in \\mathbb{R}^n$ and $b \\neq 0$ and $\\delta x$ is such that \\begin{equation} A \\left( x + \\delta x \\right) = b + \\delta b \\end{equation} then \\begin{equation} \\dfrac{1}{K(A)} \\dfrac{\\left\\Vert \\delta b \\right\\Vert}{\\left\\Vert b \\right\\Vert} \\le \\dfrac{\\left\\Vert \\delta x \\right\\Vert}{\\left\\Vert x \\right\\Vert} \\le K(A) \\dfrac{\\left\\Vert \\delta b \\right\\Vert}{\\left\\Vert b \\right\\Vert} \\end{equation} Theorem:\tFor $A \\in \\mathbb{R}^{n \\times n}$ and $b \\in \\mathbb{R}^{n}$, assume $\\left\\Vert \\delta A \\right\\Vert \\le \\gamma …","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"68160d27fe9d9e48930b5d4d273109ac","permalink":"https://djps.github.io/courses/numericalanalysis22/part-1/principles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalanalysis22/part-1/principles/","section":"courses","summary":"Well-posed problems, condition numbers, stability of numerical algorithms and tools for matrix analysis","tags":null,"title":"Principles of Numerical Mathematics","type":"book"},{"authors":null,"categories":null,"content":"Linear Equations Definition:\tLinear Systems of Equations Here: linear, square. Definition:\tBanded Systems Here Definition:\tSymmetric Matrices A square matrix $A$ is symmetric if ${A=A^T}$, that is, $a_{i,j}=a_{j,i}$ for all indices $i$ and $j$.\nA square matrix is said to be Hermitian if the matrix is equal to its conjugate transpose, i.e. $a_{i,j}=\\overline{a_{j,i}}$ for all indices $i$ and $j$. A Hermitian matrix is written as $A^H$.\nDefinition:\tPositive Definite Matrices A matrix, $M$, is said to be positive definite if it is symmetric (or Hermitian) and all its eigenvalues are real and positive. Definition:\tNonsingular Matrices A matrix is non-singular or invertible if there exists a matrix $A^{-1}$ such that ${A^{-1}A = A A^{-1} = I}$, where $I$ is the identity matrix. Note:\tProperties of Nonsingular Matrices For a nonsingular matrix, the following all hold:\nNonsingular matrix has full rank A square matrix is nonsingular if and only if the determinant of the matrix is non-zero. If a matrix is singular, both versions of Gaussian elimination will fail due to division by zero, yielding a floating exception error. Definition:\tIf $\\tilde{x}$ is an approximate solution to the linear problem ${Ax=b}$, then the residual is defined as ${r = A \\tilde{x}-b}$.\nIf $\\left| r \\right|$ is large due to rounding, the matrix is said to be ill-conditioned.\nDirect Methods Algorithm:\tGaussian Elimination Here Algorithm:\tGaussian Elimination with Scaled Partial Pivoting Here Definition:\t$LU$-Decomposition Here Theorem:\t$LU$-Decomposition Here Definition:\tCholesky-Decomposition Here Algorithm:\tCholesky Here Indirect Methods $Q x_{k +1}= \\left(Q-A \\right) x_{k} +Q^{-1} b$.\nDefinition:\tRichardson Iteration Here $Q=I$ Definition:\tJacobi Here $Q=D$ Definition:\tGauss-Seidel $Q=L$ Definition:\tSuccessive Over Relaxation $Q=L + \\dfrac{1}{\\omega}D$ ","date":1700438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700438400,"objectID":"e85eb11f8168681ab61d1c5b1264eba0","permalink":"https://djps.github.io/courses/numericalmethods24/part-1/linear-equations/","publishdate":"2023-11-20T00:00:00Z","relpermalink":"/courses/numericalmethods24/part-1/linear-equations/","section":"courses","summary":"Solutions to linear equations","tags":null,"title":"Linear Equations","type":"book"},{"authors":null,"categories":null,"content":" David Sinden ·\rMarch 11, 2024 ·\r45 minute read Root Finding Bisection Method Newton’s Method Secant Methods Convergence Definition:\tOrder of convergence: cubic, quadratic, linear. Super-linearly, sub-linear, Regularity Proximity to $r$ func calls Convergence Bisection $\\mathcal{C}^{0}$ Newton $\\mathcal{C}^{2}$ Secant $\\mathcal{C}^{2}$ Systems of Nonlinear Equation Definition:\tJacobian here. ","date":1700179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700179200,"objectID":"86f44b5a1c0119529d7b8f56b4d16bc8","permalink":"https://djps.github.io/courses/numericalmethods24/part-1/nonlinear-equations/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalmethods24/part-1/nonlinear-equations/","section":"courses","summary":"Solutions to nonlinear equations","tags":null,"title":"Nonlinear Equations","type":"book"},{"authors":null,"categories":null,"content":" David Sinden ·\rMarch 11, 2024 ·\r45 minute read Interpolation Definition:\tGiven as set of points $p_0$, $\\ldots$, ${p_n \\in \\mathbb{R}}$ and corresponding nodes $u_0$, $\\ldots$, ${u_n \\in \\mathbb{R}}$, a function ${f , : , \\mathbb{R} \\rightarrow \\mathbb{R}}$ with ${f(u_i) = p_i}$ is an interpolating function. Definition:\tIf $$p\\left( u \\right) = \\sum\\limits_{i=0}^{n} \\alpha_i \\varphi_i \\left( u \\right)$$ So that for every $j$, $p\\left( u_j \\right) = \\sum\\limits_{i=0}^{n} \\alpha_i \\varphi_i \\left( u_j \\right)$, thus the $\\alpha_i$ lead to a linear system of the form $$\\Phi \\alpha = p$$ where $\\Phi$ is the collocation matrix. Definition:\tIf $$p\\left( u \\right) = \\sum\\limits_{i=0}^{n} \\alpha_i \\varphi_i \\left( u \\right)$$ So that for every $j$, $p\\left( u_j \\right) = \\sum\\limits_{i=0}^{n} \\alpha_i \\varphi_i \\left( u_j \\right)$, thus the $\\alpha_i$ lead to a linear system of the form $$\\Phi \\alpha = p$$ where $\\Phi$ is the Vandermonde matrix. Definition:\tLagrange. Definition:\tAitken. Theorem:\tpolynomial’s are all the same. Piecewise Polynomial Interpolation Spline Interpolation Least-Squares Interpolation ","date":1700179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700179200,"objectID":"ceb162139e3de3197147b8d4a671cc2d","permalink":"https://djps.github.io/courses/numericalmethods24/part-2/interpolation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalmethods24/part-2/interpolation/","section":"courses","summary":"Interpolation","tags":null,"title":"Interpolation","type":"book"},{"authors":null,"categories":null,"content":" David Sinden ·\rMarch 11, 2024 ·\r45 minute read Numerical Differentiation Definition:\tFinite-Difference Quotients Consider the approximations to the first-order derivative:\nForward Difference Quotient: \\begin{equation} D_{j}^{+} u = \\dfrac{u_{j+1} - u_j}{h} \\end{equation} Backwards Difference Quotient: \\begin{equation} D_{j}^{-} u = \\dfrac{u_{j} - u_{j-1}}{h} \\end{equation} Central Difference Quotient: \\begin{equation} D_{j}^{0} u = \\dfrac{u_{j+1} - u_{j-1}}{2h} \\end{equation} Definition:\tRichardson Extrapolation Here Definition:\tHigher Order Derivatives with $f(x+h)$ and $f(x-h)$, so $$ f^{\\prime\\prime}(x) = \\dfrac{f(x-h) - 2f(x) + f(x+h)}{h^2} +\\mathcal{O}\\left(h^2\\right) $$ Numerical Integration Definition:\tRiemann Sum Partition Approximate the area under the curve by summing the rectangles If $f$ is continuous, the value of $x_{i}^{\\ast}$ may be choosen arbitrarily in the interval $\\left[ x_i, x_{i+1} \\right]$ Then the Lower and Upper sums are given by $$ L\\left(f, p\\right) \\le \\int\\limits_{a}^{b} f\\left(x \\right) , \\mathrm{d}x \\le U\\left(f, p\\right). $$ Definition:\tTrapezoidal Rule Here Theorem:\tError for Trapezoidal Rule Here Algorithm:\tRomberg Algorithm Here ","date":1700179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700179200,"objectID":"06e8e3d9579c1098379c8fc84d277935","permalink":"https://djps.github.io/courses/numericalmethods24/part-2/integration/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalmethods24/part-2/integration/","section":"courses","summary":"Numerical Integration","tags":null,"title":"Integration","type":"book"},{"authors":null,"categories":null,"content":" David Sinden ·\rMarch 11, 2024 ·\r45 minute read Finite Difference Methods for Differential Equations Solutions are functions.\nDefinition:\tOrdinary Differential Equations An ordinary differential equation (ODE) is an equation that involves one of more derivatives of a function of a single variable. Definition:\tInitial Value Problems An initial value problem (IVP) is given by an ordinary differential equation of the form ${y^{\\prime} \\left(t\\right) = f\\left( y\\left(t\\right), t\\right)}$ and initial value ${y\\left(a\\right)=y_a}$ for the unknown function $y\\left(t\\right)$.\nOften ${a=0}$, so the initial condition reads ${y(0)=y_0}$.\nDefinition:\tOne step methods A numerical method for approximating the solution to a differential equation is called a one step method if the solution at time step ${i+1}$ depends only on the previous one. Forward Euler Method Definition:\tForward Euler approximates the derivative through a the first-order forward difference approximation of the first-order derivative $$ u_{n+1} = u_n + h f_n $$ where ${f_{n+1} = f\\left( u_{n}, t_{n} \\right)}$. The error is $\\mathcal{O}\\left( h^2 \\right)$ Backward Euler Method Definition:\tBackward Euler uses the backward finite difference approximation of the first-order derivative $$ u_{n+1} = u_n + h f_{n+1} $$ where ${f_{n+1} = f\\left( u_{n+1}, t_{n+1} \\right)}$. Crank-Nicolson Method Definition:\tThe Crank-Nicolson method is given by $$ u_{n+1} = u_n + \\dfrac{h}{2}\\left( f_n + f_{n+1} \\right) $$ Heun’s Method Definition:\tHeun’s method $$ u_{n+1} = u_n + \\dfrac{h}{2}\\left( f_n + f_{n+1} \\right) $$ Both the Forward Euler and Heun’s method are explicit. Backward Euler and Crank-Nicolson are implicit.\nAlternatively, $$ y\\left( t + h \\right) = y\\left( t \\right) + \\int_{t}^{t+h} f\\left( y\\left(\\tau\\right), \\tau \\right) \\, \\mathrm{d}\\tau $$\nThen, the Forward Euler method is the left Reimann sum, Backward Euler is the right Riemann sum and the Crank-Nicolson is the trapezoidal rule.\nAnalysis of One-Step Methods Definition:\tA numerical method is said to be explicit if an approximation $y_{k+1}$ can be calculated directly from already computed values $y_i$, $i\u0026lt;k$. Otherwise, the method is said to be implicit.\nOften, implicit methods require, at each step, the solution of a nonlinear equation for computing $y_{k+1}$.\nDefinition:\tA function $f$ is Holder continuous if there exists real constants ${C \\gt 0}$ and ${\\alpha \\le 0}$ such that $$ \\left| f\\left(x\\right) - f\\left(y\\right) \\right| \\le C \\Vert x - y\\Vert^\\alpha $$ for all $x$ and $y$. If ${\\alpha=1}$ the function is said to be Lipshitz continuous. Definition:\tOrder A one-step method is of order $p \\in \\mathbb{N}$, if for all ${t \\in \\left[ 0, \\tau \\right]}$, the solution satisfies the condition that ${\\tau\\left(h\\right) = \\mathcal{O}\\left( h^p \\right) }$ as ${h \\rightarrow \\infty}$. Definition:\tZero Stable Methods A method of the form $$ u_{n+1} = u_n + h \\Phi\\left( t_n, u_n, f_n, h \\right) $$ is called zero-stable if there exists both a maximal step size, $h_0$ and a constant, $C$, such that for all ${h \\in \\left[0, h_0\\right]}$ and for ${\\varepsilon \\gt 0}$, then the following holds:\nIf, for all time-steps ${0 \\le n \\le N}$, there exists a ${\\delta_n \\le \\varepsilon}$ and $$ z_{n+1} = z_n + h \\Phi\\left( t_n, z_n, f_n\\left(z_n, t_n\\right), h \\right) + \\delta_{n+1} $$ and $z_0 = {y_0 +\\delta_0}$, then $$ \\left| z_n - u_n \\right| \\le C \\varepsilon \\quad \\mbox{for} \\quad 0 \\le n \\le N. $$\nThis means that small perturbations in the computations lead to small perturbations in the approximations.\nTheorem:\tIf increment function is Lipshitz continuous for $y_n$ for any $h$ and $t_n$, then the one step method is zero-stable. Theorem:\tIf the increment function $\\Phi$ is\nLipshitz continuous for $y_n$ for any $h$ and $t_{n+1}$ and\nthe method is consistent then $$ \\lim \\limits_{h \\rightarrow 0} \\left| y_n - u_n \\right| =0. $$ Also, if the method is of order $p$ and if ${\\left| y_0 - u_0 \\right| = \\mathcal{O}\\left(h^p\\right)}$ as ${h\\rightarrow 0}$, the convergence is of order $p$.\nDefinition:\tAbsolute Stability A numerical scheme for approximating the solution to the linear differential equation $y^{\\prime}\\left(t\\right) = \\lambda y\\left(t\\right)$ with ${\\lambda \\in \\mathbb{C}}$ and initial condition ${y_0 = 1}$ is said to be absolutely stable if $\\left|u_n\\right| \\rightarrow 0$ as $n \\rightarrow \\infty$, when ${\\operatorname{Re}\\left(\\lambda\\right) \\lt 0}$, for a fixed value of $h$. Runge-Kutta Schemes And Multi-Step Schemes Partial Differential Equations ","date":1700179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700179200,"objectID":"7d6cda50b7a9ffdac78d88d0b277950c","permalink":"https://djps.github.io/courses/numericalmethods24/part-3/odes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalmethods24/part-3/odes/","section":"courses","summary":"Finite difference methods for differential equations","tags":null,"title":"Finite Difference Methods for Differential Equations","type":"book"},{"authors":null,"categories":null,"content":"Quantifying Liver Perfusion-Function Relationship in Complex Resection – A Systems Medicine Approach The goal of the research unit is to better predict the recovery of hepatic function during regeneration after liver surgery of diseased livers.\nHepatic disease affect hepatic processes at different spatial scales: individual hepatocytes, hepatic sinusoids/lobuli (including zonated phenomena), the organ, and the entire organism. Surgery and subsequent regeneration further perturb the hepatic processes at the different scales.\nIn QuaLiPerF, these effects will be included in computational models of the metabolization of selected test compounds representative for liver function.\nThe goal of this project is to build a multi-scale computational model of test compound metabolism during liver regeneration. For this purpose, this project will integrate models at the cellular, lobular, and organism scale (by the modeling partners) with own models at the organ scale, in a four- scale framework. Combining the relevant complex and interacting processes in a computational model will require a deep understanding of the underlying physiology, biology, and biochemistry and of suitable modeling techniques for the respective spatial scales. Our mechanistically driven model translation and integration will be complemented by a data-driven approach in collaboration with the data integration partner. The computational models to be developed will be parameterized and validated using clinical and experimental physiological and metabolic data.\nIn addition, this project will combine the data acquired during regeneration to descriptive computational model of the recovery of physiological and metabolic parameters over time, to be combined with the models of test compound metabolization. Predictions obtained from simulations using the computational models will be used in QuaLiPerF to investigate biological and clinical questions driven by the clinical and experimental partners. Ultimately, a better prediction of the recovery of liver function developed in QuaLiPerF will help improve planning of individual surgery.\nProject Homepage The project homepage can be accessed here.\n","date":1655337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655337600,"objectID":"ec4cbceb7cc0c7570d0a3e8f811cca87","permalink":"https://djps.github.io/project/qualiperf/","publishdate":"2022-06-16T00:00:00Z","relpermalink":"/project/qualiperf/","section":"project","summary":"Quantifying Liver Perfusion-Function Relationship in Complex Resection – A Systems Medicine Approach","tags":["Multiscale Modeling"],"title":"QualiPerF","type":"project"},{"authors":null,"categories":null,"content":" David Sinden · March 11, 2022 · 45 minute read Construct a scheme which solves the linear system $Ax=b$ by generating a sequence ${ x^{(n)} }$ which approximates the solution, $x$, that is \\begin{equation} \\lim_{n\\rightarrow \\infty} x^{(n)} = x. \\nonumber \\end{equation} So that $x = A^{-1} b$. Split the matrix $A=P-N$ and solve \\begin{equation} P x^{(n+1)} = B x^{(n)} + f, \\nonumber \\end{equation}\nwhere $P$ is called a preconditioner and $B=P^{-1}N$ is the iteration matrix.\nAn equivalent way is to write \\begin{equation} x^{(k+1)} = x^{(k)} + P^{-1}r^{(k)} \\nonumber \\end{equation} where \\begin{equation} r^{(k)} = b - A x^{(k)} \\end{equation} is the residual.\nDefinition:\tConsistency An iterative method is consistent if $x=Bx +f$, or equivalently, \\begin{equation} f = (I-B) A^{-1} b \\end{equation} Theorem:\tIf an iterative scheme is consistent, then if and only if $\\rho \\left( B \\right) \u0026lt; 1$ the method will converge for any initial guess $x^{(0)}$. Definition:\tStationary Methods The formulation can be written as \\begin{equation} x^{(0)} = F^{(0)} \\left( A, b \\right) \\quad \\mbox{and} \\quad x^{(k+1)} = F^{(k+1)} \\left( x^{(k)}, x^{(k-1)}, \\ldots, x^{(0)}, A, b \\right) . \\end{equation}\nIf the functions $F^{(k)}$ are independent of the number of iterations, then it is said to be stationary.\nJacobi Method The Jacobi method decomposes the matrix $A$ into diagonal, lower and upper triangular matrices $A=D+L+U$, and solves \\begin{equation} D x^{(n+1)} = -(L + U) x^{(n)} + b . \\label{eq:jacobi_method} \\end{equation}\nElement-wise this is \\begin{equation} x_i^{(k+1)} = \\dfrac{1}{a_{ii}} \\left( b_i - \\sum\\limits_{\\substack{j=1,\\newline{}j \\neq i}}^n a_{ij} x_{j}^{(k)} \\right) . \\nonumber \\end{equation}\nThus, the iterative scheme is \\begin{equation} x^{(n+1)} = -D^{-1}(L + U) x^{(n)} + D^{-1} b. \\nonumber \\end{equation}\nAs $L+U = A-D$, so the iteration matrix can be written as $B=I - D^{-1}A$.\nOver-Relaxation of Jacobi Method Also called the weighted Jacobi method. Introduce $\\omega$ to solve \\begin{equation} x_i^{(k+1)} = \\dfrac{ \\omega }{ a_{ii} } \\left( b_i - \\sum\\limits_{\\substack{j=1,\\newline{}j \\neq i}}^n a_{ij} x_{j}^{(k)} \\right) + \\left( 1 - \\omega \\right) x^{(k)}. \\label{eq:JOR} \\end{equation}\nSuccessive Over-Relaxation Introduce $\\omega$ to solve \\begin{equation} \\left(D + \\omega L \\right) x^{(n+1)} = -( (\\omega-1)D + \\omega U) x^{(n)} + \\omega b. \\label{eq:SOR} \\end{equation}\nGauss-Seidel The Gauss-Seidel method decomposes the matrix $A$ into diagonal, lower and upper triangular matrices $A=D+L+U$, and solves \\begin{equation} (D + L) x^{(n+1)} = -U x^{(n)} + b \\label{eq:GS} \\end{equation}\nTheorem:\tIf $A$ is strictly diagonally dominant by rows, i.e. $|a_{ii}| \u0026gt; \\sum_{j \\ne i} |a_{ij}|$, the Jacobi and Gauss-Seidel methods are convergent.\nIf $A$ and $2D-A$ are symmetric and positive definite, then the Jacobi method is convergent and the spectral radius of the iteration matrix $B$ is equal to \\begin{equation} \\rho \\left( B \\right) = \\left\\Vert B \\right\\Vert_{A} = \\left\\Vert B \\right\\Vert_{D} \\nonumber \\end{equation} where $\\left\\Vert \\cdot \\right\\Vert_{A}$ is an energy norm which is induced by the vector norm $\\left| x \\right|_{A} = \\sqrt{ x \\cdot A x}$\nIf and only if $A$ is symmetric and positive definite, the Jacobi over-relaxation method is convergent if \\begin{equation} 0 \u0026lt; \\omega \u0026lt; \\dfrac{2}{\\rho\\left( D^{-1}A \\right) }. \\nonumber \\end{equation}\nIf and only if $A$ is symmetric and positive definite, the Gauss-Seidel method is monotonically convergent with respect to the energy norm $\\left\\Vert \\cdot \\right\\Vert_{A}$.\nTheorem:\tFor any $\\omega \\in \\mathbb{R}$ we have $\\rho \\left( B \\left( \\omega \\right) \\right) \\ge |\\omega - 1|$. Thus, SOR does not converge if either $\\omega \\le 0$ or $\\omega \\ge 2$. Theorem:\tOstrowski If $A$ is symmetric and positive definite, then the SOR method is convergent if and only if $0 \u0026lt; \\omega \u0026lt; 2$. Furthermore, the convergence is monotonic with respect to the energy norm $\\left\\Vert \\cdot \\right\\Vert_{A}$. Gradient Descent Consider the function $\\Phi \\left( y \\right) \\, : \\, \\mathbb{R}^{n} \\mapsto \\mathbb{R}$ which takes the form: \\begin{equation} \\Phi \\left( y\\right) = \\dfrac{1}{2} y \\cdot A y - y \\cdot b. \\end{equation} It can be shown that solving $Ax=b$ is equivalent to minimizing $\\Phi$.\nIf $x$ is a solution to the linear system and minimizes $\\Phi(x)$ then $\\nabla \\Phi(x) = 0$, so that $Ax-b = \\nabla \\Phi(x) =0$.\nNow \\begin{align} \\Phi(y) \u0026amp; = \\Phi(x + (y-x) ) \\newline \u0026amp; = \\Phi(x) + \\dfrac{1}{2} \\left\\Vert y - x\\right\\Vert_{A}^{2}. \\end{align}\nGradient descent seeks to construct a scheme which updates the vector $x^{(k)}$ according to \\begin{equation} x^{(k+1)} = x^{(k)} + \\alpha^{(k)} d^{(k)} \\end{equation} where $d^{(k)}$ is the update direction and $\\alpha^{(k)}$ is the step size at the $k$-th iterate.\nNote that in contrast to the methods above, the gradient descent method is non-stationary as values $d$ and $\\alpha$ change at every iterate.\nThe idea is to let the search …","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"dffc935dd77297ad888198caa8ddccf4","permalink":"https://djps.github.io/courses/numericalanalysis22/part-1/iterative-methods/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalanalysis22/part-1/iterative-methods/","section":"courses","summary":"Matrix inversion by iterative schemes Jacobi Method, Conjugate gradient and Gauss-Seidel","tags":null,"title":"Iterative Methods","type":"book"},{"authors":null,"categories":null,"content":" David Sinden · March 11, 2022 · 45 minute read Numerical treatment of problems often involves the process of discretization - i.e. going from a continuous function to set of discrete points.\nInterpolation provides a way of approximating continuous functions by discrete data. Types of functions which can be used are:\nPolynomial interpolation : using a polynomial to approximate the data, Trigonometric interpolation: using polynomials of trigonometric functions, Spline interpolation: using a set of piecewise polynomials over subintervals of the data. Theorem:\tGiven $n+1$ distinct points $x_0, x_1, \\ldots, x_n$ and $n+1$ corresponding values $y_0, y_1, \\ldots, y_n$ there exists a unique polynomial $\\Pi_n \\in \\mathbb{P}_n$ such that for all $i=0, \\ldots, n$ \\begin{equation} \\Pi_n\\left( x_i \\right) = y_i. \\end{equation} Lagrange Interpolation Definition:\tLagrange Polynomials The Lagrange form of an interpolating polynomial is given by \\begin{equation} \\Pi_n \\left( x\\right) = \\sum\\limits_{i=0}^{n} y_i l_i\\left(x\\right) \\end{equation}\nwhere\n$l_{i} \\in \\mathbb{P}_{n}$ such that $l_{i}\\left( x_{j} \\right) = \\delta_{ij}$. The polynomials $l_i\\left(x\\right) \\in \\mathbb{P}_n$ for $i=0, \\ldots, n$, are called characteristic polynomials and are given by \\begin{equation} l_{i} \\left( x \\right) = \\prod \\limits_{\\substack{j = 0,\\newline{}j \\ne i}}^{n} \\dfrac{x - x_j}{x_i - x_j}. \\end{equation}\nExample:\tAn .ipynb notebook illustrating the Lagrange polynomials can be accessed here. It can be downloaded from here.\nAn .ipynb notebook illustrating Runge phenomenon can be accessed here. It can be downloaded from here.\nTheorem:\tLet $x_0, x_1, \\ldots x_n$ be $n+1$ distinct nodes and let $x$ be a point belonging to the domain of a given function $f$.\nLet $I_x$ be the smallest interval containing the nodes $x_0, x_1, \\ldots x_n$ and $x$ and assume that $f \\in C^{n+1}\\left( I_x \\right)$. Then the interpolation error at the point $x$ is defined and given by \\begin{equation} \\begin{aligned} E_n(x) \u0026amp;= f(x) − \\Pi_n f (x) \\newline \u0026amp;= \\dfrac{f^{(n+1)}( \\xi ) }{ (n + 1)!} \\omega_{n+1}(x) \\end{aligned} \\tag{1} \\label{eq:basic_interpolation_error} \\end{equation}\nwhere $\\xi \\in I_x$ and $\\omega_{n+1}$ is the nodal polynomial of degree $n + 1$, which is defined as \\begin{equation} \\omega_{n+1}(x) = \\prod\\limits_{i=0}^{n} \\left(x - x_i \\right). \\end{equation}\nPiecewise Lagrange Interpolation Definition:\t$\\mathrm{L}^2$ Space Define the following space \\begin{equation} \\mathrm{L}^{2}(a, b) = \\bigl \\lbrace f : (a, b) \\rightarrow \\mathbb{R}, \\int_{a}^{b} | f(x) |^{2} \\, \\mathrm{d}x \u0026lt; \\infty \\bigr \\rbrace \\end{equation}\nwith\n\\begin{equation} | f |^{}_{\\mathrm{L}^{2}(a, b)} = \\left( \\int_{a}^{b} | f(x) |^{2} \\, \\mathrm{d} x \\right)^{1 / 2}. \\end{equation}\nThis defines a norm for $\\mathrm{L}^{2}(a, b)$. Note that integral of the function $|f|^{2}$ is in the Lebesgue sense - in particular, $f$ needs not be continuous everywhere.\nPartition $\\mathcal{T_h}$ of $[a, b]$ into $K$ subintervals $I_j = \\left[ x_j , x_{j+1} \\right]$ of length $h_j$ such that $[a, b] = \\bigcup\\limits_{j=0}^{K−1}I_j$. Let ${h = \\max\\limits_{0 \\le j \\le K−1} h_j}$.\nFor $k \\geq 1$, introduce on $\\mathcal{T}_h$ the piecewise polynomial space\n\\begin{equation} X_h^k = \\bigl\\lbrace v \\in C^0([a, b]) \\, : \\, v |_{I_j} \\in \\mathbb{P}_k \\left( I_j \\right), \\quad \\forall \\, I_j \\in \\mathcal{T}_h \\bigr\\rbrace \\tag{2} \\label{eq:piecewise} \\end{equation}\nwhich is the space of the continuous functions over the interval $[a, b]$ whose restrictions on each $I_j$ are polynomials of degree less than or equal to $k$.\nThen, for any continuous function $f$ in $[a, b]$, the piecewise interpolation polynomial $\\Pi^k_h f$ coincides on each $I_j$ with the interpolating polynomial of $\\left. f \\right|_{I_j}$ at the $n + 1$ nodes $\\lbrace x_j^{(i)}, 0 \\leq i \\leq n \\rbrace$.\nAs a consequence, if $f \\in C^{k+1}([a, b])$, then from \\eqref{eq:basic_interpolation_error} within each interval the following error estimate holds \\begin{equation} \\left\\Vert f − \\Pi_h^k f \\right\\Vert_\\infty \\leq C h^{k+1} \\left\\Vert f^{(k+1} \\right\\Vert_\\infty . \\end{equation}\nTheorem:\tPartition $\\mathcal{T_h}$ of $[a, b]$ into $K$ subintervals $I_j = [x_j , x_{j+1}]$ of length $h_j$, with $h = \\max\\limits_{0 \\le j \\le K−1} h_j$, such that $[a, b] = \\bigcup\\limits_{j=0}^{K−1}I_j$. Now using Lagrange interpolation on each subinterval $I_j$ using $n + 1$ equally spaced nodes $\\lbrace{ x^{(i)}_{j} , 0 \\le i \\le n \\rbrace}$ with a small $n$. Then $\\Pi_n^k$ is the piecewise interpolation polynomial.\nLet $0 \\leq m \\leq k+1$, with $k \\geq 1$ and assume that $f^{(m)} \\in$ $\\mathrm{L}^{2}(a, b)$ for $0 \\leq m \\leq k+1$ then there exists a positive constant $C$, independent of $h$, such that\n\\begin{equation} \\left| \\left( f - \\Pi_{h}^{k} f\\right)^{(m)} \\right|_{\\mathrm{L}^{2}(a, b)} \\leq C h^{k+1-m} \\left| f^{(k+1)} \\right|_{\\mathrm{L}^{2}(a, b)} . \\end{equation}\nIn particular, for $k=1$ and $m=0$, or $m=1$\n\\begin{equation} \\begin{aligned} …","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"8afb514223601ad356e9ae5e3e551b1e","permalink":"https://djps.github.io/courses/numericalanalysis22/part-2/interpolation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalanalysis22/part-2/interpolation/","section":"courses","summary":"Lagrange polynomials and Lagrange interpolation","tags":null,"title":"Interpolation","type":"book"},{"authors":null,"categories":null,"content":"Enhancing Hepatic Microwave Ablation Details.\n","date":1655337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655337600,"objectID":"f78d7be5faf53a7dfcd0f0db37e1f86d","permalink":"https://djps.github.io/project/ehma/","publishdate":"2022-06-16T00:00:00Z","relpermalink":"/project/ehma/","section":"project","summary":"Enhancing Hepatic Microwave Ablation","tags":["Thermal Ablation"],"title":"EHMA","type":"project"},{"authors":null,"categories":null,"content":" David Sinden · March 11, 2022 · 45 minute read If $f \\in C^{0} \\left([a, b]\\right)$, the quadrature error $E_{n}(f) = I(f) - I_{n}(f)$ satisfies \\begin{equation} \\left| E_{n}(f) \\right| \\leq \\int_{a}^{b} \\left| f(x) - f_{n}(x) \\right| \\, \\mathrm{d} x \\leq (b-a) \\left| f - f_{n} \\right|_{\\infty} \\end{equation}\nTherefore, if for some $n$, $\\left\\| f - f_{n}\\right\\|_{\\infty} \u0026lt; \\varepsilon$ , then $\\left|E_{n}(f)\\right| \\leq \\varepsilon(b-a)$.\nThe approximation of the function $f_{n}$ must be easily integrable, which is the case if, for example, $f_{n} \\in \\mathbb{P}_{n}$.\nIn this respect, a natural approach consists of using $f_{n}=\\Pi_{n} f$, the interpolating Lagrange polynomial of $f$ over a set of $n+1$ distinct nodes $\\lbrace x_{i} \\rbrace$, with $i=0, \\ldots, n$. It follows that the approximation to the integral is \\begin{equation} I_{n}(f) = \\sum_{i=0}^{n} f \\left( x_{i} \\right) \\int_{a}^{b} l_{i}(x) \\, \\mathrm{d} x \\end{equation}\nwhere $l_{i}$ is the characteristic Lagrange polynomial of degree $n$ associated with node $x_{i}$. It is called the Lagrange quadrature formula, and is a special instance of the following, generalised, quadrature formula \\begin{equation} I_{n}(f) = \\sum \\limits_{i=0}^{n} \\alpha_{i} f\\left( x_{i} \\right) \\end{equation}\nwhere the coefficients $\\alpha_{i}$ of the linear combination are given by $\\int_{a}^{b} l_{i} \\left( x \\right) \\, \\mathrm{d} x$. The above equation is a weighted sum of the values of $f$ at the points $x_{i}$, for $i=0, \\ldots, n$. These points are said to be the nodes of the quadrature formula, while the $\\alpha_{i} \\in \\mathbb{R}$ are its coefficients or weights. Both weights and nodes depend in general on $n$.\nAnother approximation of the function $f$ leads to the Hermite quadrature formula \\begin{equation} I_{n}(f)=\\sum_{k=0}^{1} \\sum_{i=0}^{n} \\alpha_{i k} f^{(k)}\\left(x_{i}\\right) \\end{equation}\nwhere the weights are now denoted by $\\alpha_{i k}$. This depends on an evaluation of the function and its derivative.\nBoth the above are interpolatory quadrature formula, since the function $f$ has been replaced by its interpolating polynomial (Lagrange and Hermite polynomials, respectively).\nDefine the degree of exactness of a quadrature formula as the maximum integer $r \\geq 0$ for which \\begin{equation} I_{n}(f)=I(f), \\quad \\forall f \\in \\mathbb{P}_{r} . \\end{equation}\nAny interpolatory quadrature formula that makes use of $n+1$ distinct nodes has degree of exactness equal to at least $n$. Indeed, if $f \\in \\mathbb{P}_{n}$, then $\\Pi_n f = f$ and thus $I_n\\left( \\Pi_n f \\right) = I \\left( \\Pi_n f \\right)$.\nMidpoint Rule The zero-th order approximation is given by \\begin{equation} I_0 = (b-a) f \\left( \\dfrac{a+b}{2} \\right). \\end{equation}\nTrapezoidal Rule The trapezoidal rule is given by \\begin{equation} I_1 = \\dfrac{b-a}{2} \\left( f \\left( a \\right) + f \\left(b\\right) \\right). \\end{equation}\nSimpson’s Rule Simpson’s rule is \\begin{equation} I_2 = \\dfrac{b-a}{6} \\left( f \\left( a \\right) + 4f\\left(\\dfrac{a+b}{2} \\right)+ f \\left(b\\right) \\right). \\end{equation}\nExample:\tAn .ipynb notebook, detailing examples of the Midpoint rule, Trapezoidal rule, Simpson’s rule can be accessed online here. It can be downloaded from here as a python file or downloaded as a notebook from here. Gaussian Integration Gaussian quadrature integrates a function by a suitable choice of nodes and weights.\nTheorem:\tWith the exact integral of $f$ \\begin{equation} I_g (f) = \\int\\limits_{−1}^{1} f (x) g(x) \\, \\mathrm{d}x, \\end{equation} being $f \\in C^0 \\left( [−1, 1] \\right)$, consider quadrature rules of the type \\begin{equation} I_{n,g} (f) = \\sum\\limits_{i=0}^{n} \\alpha_i f(x_i) \\end{equation} where $\\alpha_i$ are to be determined.\nFor a given $m \u0026gt; 0$, the quadrature $I_{n,g}$ has degree of exactness $d=n + m$ if and only if it is of interpolatory type and the nodal polynomial $\\omega_{n+1}$ associated with the set of nodes $\\lbrace x_i \\rbrace$, is such that \\begin{equation} \\int_{-1}^{1} \\omega_{n+1}(x) p(x) g(x) \\, \\mathrm{d}x = 0, \\quad \\forall \\, p \\in \\mathbb{P}_{m-1}. \\end{equation}\n","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"3a256e4c672bd09e627063ca798f2842","permalink":"https://djps.github.io/courses/numericalanalysis22/part-2/integration/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalanalysis22/part-2/integration/","section":"courses","summary":"Notes on integration, midpoint rule, trapezoidal rule, Simpson's rule and Gaussian integration","tags":null,"title":"Integration","type":"book"},{"authors":null,"categories":null,"content":"Computational Topology More details to appear.\n","date":1695081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695081600,"objectID":"341f9d7af7b2449e4609ead0aa0f80b9","permalink":"https://djps.github.io/project/comptop/","publishdate":"2023-09-19T00:00:00Z","relpermalink":"/project/comptop/","section":"project","summary":"Applications of computational topology to medical imaging","tags":["Computational Topology","Topological Data Analysis","Persistent Homology"],"title":"Computational Topology in Medical Imaging","type":"project"},{"authors":null,"categories":null,"content":"Developing a Metrological Framework for Assessment of Image-based Artificial Intelligence Systems for Disease Detection Details can be found at the funder’s website here.\nThe project homepage can be found here.\n","date":1695081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695081600,"objectID":"a0f6aa3aa70e2f0f3372a0bceca6783c","permalink":"https://djps.github.io/project/maibai/","publishdate":"2023-09-19T00:00:00Z","relpermalink":"/project/maibai/","section":"project","summary":"Developing a metrological framework for assessment of image-based Artificial Intelligence systems for disease detection","tags":["Deep Learning","Mammography","Big Data"],"title":"MAIBAI: Metrology for Image-Based AI","type":"project"},{"authors":null,"categories":null,"content":"Challenge on Ultrasound Beamforming with Deep Learning Details can be found at the organisers website here.\nAward is here.\n","date":1655337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655337600,"objectID":"0bec50b7db0f4bcd31d2fceaf7d4728f","permalink":"https://djps.github.io/project/cubdl/","publishdate":"2022-06-16T00:00:00Z","relpermalink":"/project/cubdl/","section":"project","summary":"Challenge on Ultrasound Beamforming with Deep Learning","tags":["Deep Learning","Ultrasound"],"title":"CUBDL: Challenge on Ultrasound Beamforming with Deep Learning","type":"project"},{"authors":null,"categories":null,"content":" David Sinden · March 11, 2022 · 45 minute read Green’s functions For a linear differential operator acting on $u$, that is $\\mathcal{L} \\left[ u \\left( x \\right) \\right]$, which has a differential equation of the form \\begin{equation} \\mathcal{L}\\left[ u \\left( x \\right) \\right] = f \\left(x \\right), \\end{equation}\nthen the Green’s function for the operator $\\mathcal{L}$, denoted by $G\\left( x,s \\right)$, can be used to solved the differential equation as \\begin{equation} u(x) = \\int^x G\\left(x,s \\right) f\\left( s \\right) \\, \\mathrm{d} s. \\end{equation}\nFinite Difference Methods First discretize the domain and then approximate the governing equation to produce a linear system. Definition:\tFinite-Difference Quotients Consider the approximations to the first-order derivative:\nForward Difference Quotient: \\begin{equation} D_{j}^{+} u = \\dfrac{u_{j+1} - u_j}{h} \\end{equation} Backwards Difference Quotient: \\begin{equation} D_{j}^{-} u = \\dfrac{u_{j} - u_{j-1}}{h} \\end{equation} Central Difference Quotient: \\begin{equation} D_{j}^{0} u = \\dfrac{u_{j+1} - u_{j-1}}{2h} \\end{equation} With these, approximations to second-order derivatives can be constructed, for example:\n\\begin{equation} \\begin{aligned} D_{j}^{\\pm} u \u0026amp; = \\dfrac{ D_j^{+} u - D_j^{-} u }{h} \\newline \u0026amp; = \\dfrac{ \\dfrac{u_{j+1} - u_j}{h} - \\dfrac{u_j - u_{j-1}}{h} }{h} \\newline \u0026amp; = \\dfrac{ u_{j+1} - 2 u_j + u_{j-1} }{h^2}. \\end{aligned} \\end{equation}\nTheorem:\tErrors for Finite-Difference Quotients The errors for the approximation of the derivatives are given by\n$u\\left( x_j \\right) - D_{j}^{+}u = -\\dfrac{h}{2} u^{\\prime\\prime}\\left( \\xi\\right)$ where $\\xi \\in \\left( x_j, x_{j+1} \\right)$ $u\\left( x_j \\right) - D_{j}^{-}u = \\dfrac{h}{2} u^{\\prime\\prime}\\left( \\xi\\right)$ where $\\xi \\in \\left( x_{j-1}, x_{j} \\right)$ $u\\left( x_j \\right) - D_{j}^{0}u = -\\dfrac{h^2}{6} u^{\\prime\\prime\\prime}\\left( \\xi\\right)$ where $\\xi \\in \\left( x_{j-1}, x_{j+1} \\right)$ $u\\left( x_j \\right) - D_{j}^{\\pm}u = -\\dfrac{h^2}{24}\\left( u^{(4)}\\left( \\xi_1\\right) + u^{(4)}\\left( \\xi_2\\right) \\right)$ where $\\xi_1 \\in \\left( x_{j-1}, x_{j} \\right)$ and $\\xi_2 \\in \\left( x_{j}, x_{j+1} \\right)$. Stability Analysis Let $V_h$ be the set of discrete functions defined on the nodal points $x_j$ and $V_h^0 \\subset V_h$ contain the discrete functions $v_h \\in V_h$ which vanish at $x_0$ and $x_n$, i.e. $v_0 =0$ and $v_n=0$.\nLemma:\tLet $\\mathcal{L}_h$ be the discretization of a linear differential operator which acts on $u_h \\in V_h$, i.e. $\\mathcal{L}_h \\left[ u_h \\right]$. If the discrete inner product for both $v_h$ and $w_h \\in V_h$ is defined as\n\\begin{align} \\left( v_h, w_h \\right)_h^{} \u0026amp; = h \\sum\\limits_{j=0}^{n} c_j v_j w_j \\end{align}\nwhere 1 $c_j = 1$ for $j=1, \\ldots n-1$ and $c_0 = c_n = \\frac{1}{2}$ and a norm is defined as\n\\begin{equation} \\left\\Vert v_h \\right\\Vert_h = \\sqrt{ \\left( v_h, v_h \\right)_h } \\end{equation}\nfor a $v_h \\in V_h$.\nThen the operator $\\mathcal{L}_h$ is symmetric, i.e. \\begin{equation} \\left( \\mathcal{L}_h \\left[ v_h \\right], w_h \\right)_h = \\left( v_h, \\mathcal{L}_h \\left[ w_h \\right]\\right)_h \\quad \\forall \\, w_h , \\, v_h \\in V^0_h \\end{equation}\nand positive definite, that is \\begin{equation} \\left( \\mathcal{L}_h \\left[ v_h \\right], v_h \\right)_h \\ge 0 \\quad \\forall \\, v_h \\in V^0_h \\end{equation}\nand \\begin{equation} \\left( \\mathcal{L}_h \\left[ v_h \\right], v_h \\right)_h = 0 \\Longleftrightarrow v_h = 0 . \\end{equation}\nThis is just the composite trapezium rule, so that the discrete inner product is the discrete analogue to \\begin{equation} \\left( w, v \\right) = \\int w(x) v(x) \\, \\mathrm{d}x \\end{equation} i.e. it approximates an integral. ↩︎\nLemma:\tFor any $v_h \\in V_h$ \\begin{equation} \\left\\Vert v_h \\right\\Vert_h \\le \\dfrac{1}{\\sqrt{2}} \\Bigg( h \\sum \\limits_{j=0}^{n-1} \\left( \\dfrac{v_{j+1} - v_j}{h} \\right)^2 \\Bigg)^{1/2}. \\end{equation} Convergence The finite difference solution $u_h$ can be characterised by a discrete Green’s function. Define $G^k\\left(x\\right) \\in V_h^0$ such that \\begin{equation} \\mathcal{L}_h \\left[ G^k\\left(x\\right) \\right] = e^k\\left(x\\right) \\end{equation}\nwhere $e^k \\in V_h^0$ satisfies $e^k\\left( x_j \\right) = \\delta_{kj}$. Then \\begin{equation} G^k \\left( x_j \\right) = h G\\left( x_j, x_k \\right) . \\end{equation}\nTheorem:\tLet $\\left\\Vert v_h\t\\right\\Vert_{h,\\infty} = \\max \\limits_{0 \\le j \\le n}\\left| v_h\\left( x_j \\right) \\right|$ be the discrete maximum norm.\nAssume that $f \\in C^2 \\left( \\left[ 0,1 \\right] \\right)$, then the nodal error, given by $e\\left( x_j \\right) = u\\left(x_j\\right) - u_h\\left(x_j\\right)$ satisfies: \\begin{equation} \\left\\Vert u - u_h \\right\\Vert_{h,\\infty} \\le \\dfrac{h^2}{96} \\left\\Vert f^{\\prime\\prime} \\right\\Vert_\\infty . \\end{equation}\nGalerkin Method Consider the elementary problem: \\begin{equation} \\left( \\alpha u^{\\prime} \\right)^{\\prime} + \\beta u^{\\prime} + \\gamma u = f\\left( x \\right) \\quad \\mbox{on} \\quad (0,1) \\quad \\mbox{with} \\quad u(0) = u(1)=0 \\end{equation}\nwhere $\\alpha$, …","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"9a1b271c4d970e370b60963c1e3fd422","permalink":"https://djps.github.io/courses/numericalanalysis22/part-3/fdm/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalanalysis22/part-3/fdm/","section":"courses","summary":"Finite-Difference methods for discretizing differential equations","tags":null,"title":"Finite Difference Methods","type":"book"},{"authors":null,"categories":null,"content":" David Sinden · March 11, 2022 · 45 minute read Finite element methods approximate the solution to a differential equation on a small element of the computational domain. Distributions Denote by $\\mathrm{H}^s(a, b)$, for $s \\geq 1$, the space of the functions $f \\in C^{s-1}(a, b)$ such that $f^{(s-1)}$ is continuous and piecewise differentiable, so that $f^{(s)}$ exists unless for a finite number of points and belongs to $\\mathrm{L}^2(a, b)$. The space $\\mathrm{H}^s(a, b)$ is known as the Sobolev function space of order $s$ and is endowed with the norm $\\left\\Vert \\cdot \\right\\Vert_{H^s(a,b)}$ defined as \\begin{equation} \\left\\Vert f \\right\\Vert_s = \\left( \\sum_{k=0}^s \\left\\Vert f^{\\left(k\\right)} \\right\\Vert_{\\mathrm{L}^2\\left(a,b\\right)}^2 \\right)^{1/2}. \\end{equation}\nLet \\begin{align} C_0^\\infty \u0026amp; = \\lbrace \\varphi \\in C^\\infty \\, | \\, \\exists \\, a, b \\in (0,1) \\quad \\mbox{such that} \\quad \\varphi(x)=0 \\quad \\mbox{for} \\quad 0 \\leq x \u0026lt; a \\quad \\mbox{or} \\quad b \u0026lt; x \\leq 1 \\rbrace . \\end{align}\nThen for a function $v \\in \\mathrm{L}^2(0,1)$ we say $v^\\prime$ is the weak derivative (or distributional derivative) if \\begin{equation} \\int \\limits_0^1 v^\\prime \\varphi \\, \\mathrm{d}x = - \\int \\limits_0^1 v \\varphi^\\prime \\, \\mathrm{d}x \\quad \\forall \\, \\varphi \\in C^\\infty_0 \\left(0,1\\right). \\end{equation}\nOf interest is \\begin{equation} H^1 (0, 1) = \\lbrace v \\in \\mathrm{L}^2(0, 1) \\, : \\, v^\\prime \\in \\mathrm{L}^2(0, 1) \\rbrace \\end{equation}\nwhere $v^\\prime$ is the distributional derivative of $v$, and \\begin{equation} H^1_0 (0, 1) = \\lbrace v \\in \\mathrm{L}^2(0, 1) \\, : \\, v^\\prime \\in \\mathrm{L}^2(0, 1), \\, v(0) = v(1) = 0 \\rbrace \\end{equation}\nOn $H^1$ there is the semi-norm:\n$$ \\left| v \\right|_{\\mathrm{H}^1(0,1)} = \\left( \\int_0^1 \\left\\| v^\\prime \\left( x \\right) \\right\\|^2 \\mathrm{d}x \\right)^{1/2} = \\left\\Vert v^\\prime \\right\\Vert_{\\mathrm{L}^2(0,1)}. $$ To see that it is a semi-norm and not a norm, consider $v$ a constant, so ${v^{\\prime}=0}$ thus ${\\left| v \\right|_{\\mathrm{H}^1(0,1)} =0}$ for ${v \\neq 0}$ and thus by definition is a semi-norm, rather than a norm. Now consider the integral on functions in $H_0^1$, it is the case that if the integral is zero so the function is constant, but as it must be zero on the boundaries, so the function is zero and hence a norm.\nGalerkin Method Consider the elementary problem:\n\\begin{equation} -\\left( \\alpha u^{\\prime} \\right)^{\\prime} + \\beta u^{\\prime} + \\gamma u = f\\left( x \\right) \\quad \\mbox{on} \\quad (0,1) \\quad \\mbox{with} \\quad u(0) = u(1) = 0 \\end{equation}\nwhere $\\alpha$, $\\beta$, $\\gamma \\in C^0 \\left( \\left[ 0, 1 \\right] \\right)$ and $\\alpha(x) \\ge \\alpha_0 \u0026gt;0$ for all $x \\in \\left[ 0, 1 \\right]$.\nNext, on $\\mathrm{L}^2(0,1)$, define the scalar product \\begin{equation} \\left( f, v \\right) = \\int \\limits_0^1 f v \\, \\mathrm{d}x \\end{equation}\nand a bilinear form $a : \\left( \\cdot, \\cdot \\right)$ which maps $H_0^1 \\times H^1_0 \\rightarrow \\mathbb{R}$ \\begin{equation} a\\left( u, v \\right) = \\int_0^1 \\left( \\alpha u^\\prime v^\\prime + \\beta u^\\prime v + \\gamma u v \\right) \\, \\mathrm{d}x \\end{equation}\nand consider the weak form \\begin{equation} \\mbox{Find} \\quad u \\in H^1_0 \\quad \\mbox{such that} \\quad a\\left(u,v \\right) =\\left( f, v\\right) \\quad \\forall \\, v \\in H^1_0\\left(0,1\\right). \\end{equation}\nTheorem:\tThe following hold:\nLet $u$ be a $C^2$ be a solution of the elementary problem, then ${u \\in H^1_0}$ also solves the weak form Let $u \\in H^1_0$ be a solution of the weak problem. If and only if ${u \\in C^2\\left( \\left[0,1 \\right] \\right)}$ then $u$ also solves the elementary problem. Theorem:\tFundamental Theorem of the Calculus of Variations Suppose that $f$ is integrable on $(0,1)$ and \\begin{equation} \\int_0^1 \\phi f \\, \\mathrm{d}x = 0 \\quad \\forall \\, \\phi \\in C^\\infty_0\\left(\\left[ 0,1 \\right]\\right) \\end{equation}\nthen $f=0$.\nApproximate $H_0^1$ by $V_h$. The discrete weak problem is then: \\begin{equation} \\mbox{Find a} \\quad u_h \\in V_h \\quad \\mbox{such that} \\quad a\\left(u_h, v_h\\right) = \\left(f, v_h \\right) \\quad \\forall \\, v_h \\in V_h \\end{equation}\nLet $\\lbrace \\varphi_1, \\varphi_2, \\ldots, \\varphi_N \\rbrace$ be a basis of $V_h$, then, with $N=\\mbox{dim}V_h$, so that \\begin{equation} u_h \\left(x \\right) = \\sum\\limits_{j=1}^N u_j \\varphi_j\\left(x\\right). \\end{equation}\nSo the problem can be written as: Find ${\\left(u_1, \\ldots u_N \\right) \\in \\mathbb{R}^N}$ such that \\begin{equation} \\sum\\limits_{j=1}^N u_j a\\left( \\varphi_j, \\varphi_i \\right) = \\left( f, \\varphi_i \\right) \\quad i=1, \\ldots, N. \\end{equation}\nDenote ${a_{ij}=a\\left( \\varphi_j, \\varphi_i \\right)}$ as the elements of the matrix $A$, let ${u=\\left(u_1, \\ldots, u_N\\right)}$ and ${f=\\left(f_1, \\ldots, f_N\\right)}$ be vectors where each entry is given by ${f_i = f \\varphi_i}$, so that the problem is equivalent to solving the linear problem ${Au=f}$\nTheorem:\tPoincaré-Friedrich Inequality Let $\\Omega \\subset \\mathbb{R}^n$ be contained in $n$-dimensional cube of length $s$, …","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"fa0a9284762b38c15fcfaf2e23a1211a","permalink":"https://djps.github.io/courses/numericalanalysis22/part-3/fem/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/numericalanalysis22/part-3/fem/","section":"courses","summary":"Construction of finite-element schemes for solving differential equations","tags":null,"title":"Finite Element Methods","type":"book"},{"authors":null,"categories":null,"content":"","date":1683158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683158400,"objectID":"8c725eaa97296b2a22a6293b1048cf1a","permalink":"https://djps.github.io/project/aegeus/","publishdate":"2023-05-04T00:00:00Z","relpermalink":"/project/aegeus/","section":"project","summary":"A Novel EEG Ultrasound Device for Functional Brain Imaging and Neurostimulation","tags":["Neurostimulation","Ultrasound","EEG"],"title":"AEGEUS","type":"project"},{"authors":null,"categories":null,"content":"Deformation of an elastic rod in a magnetic field This was my PhD which was undertaken at UCL with Prof. Gert van der Heijden.\nThesis can be found online here or here. Journal publications can be found\nIntegrability of a conducting elastic rod in a magnetic field Spatial chaos of an extensible conducting rod in a uniform magnetic field Conference proceeding can be found here:\nThe Buckling of Magneto-Strictive Cosserat Rods Localisation of a twisted conducting rod in a uniform magnetic field: the Hamiltonian-Hopf-Hopf bifurcation A presentation can be found here:\nIntegrability, localisation and bifurcation of an elastic conducting rod in a magnetic field ","date":1655337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655337600,"objectID":"fbec78fdd31514806d87eb74321447c9","permalink":"https://djps.github.io/project/cosserat/","publishdate":"2022-06-16T00:00:00Z","relpermalink":"/project/cosserat/","section":"project","summary":"Deformation of an elastic conducting rod in a magnetic field","tags":["Cosserat theory"],"title":"Cosserat Theory","type":"project"},{"authors":null,"categories":null,"content":"import numpy as np import matplotlib.pyplot as plt Bezier formulas for $x$ and $y$ axes\ndef bx(t): return 3.0 * t def by(t): return 2.0 + 3.0 * t - 12.0 * t**2 + 7.0 * t**3 Provided $x$ and $y$ values\nx = [0, 1, 2, 3] y = [2, 3, 0, 0] Evaluating over the discretized interval with 100 points\nu = np.linspace(np.min(x), np.max(x), 100) Normalize to interval [0, 1]\nt = (u - np.min(x)) / np.max(x) Calculate Bezier values for both axes\nbxt = [bx(i) for i in t] byt = [by(i) for i in t] plot\nplt.plot(x, y, \u0026#39;ro--\u0026#39;, label = \u0026#39; Points\u0026#39;) plt.plot(bxt, byt, \u0026#39;b-\u0026#39;, label = \u0026#39;Bezier Curve\u0026#39;) plt.legend() plt.grid() plt.show() ","date":1701907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670371200,"objectID":"88155222ed41007bbfea40b11674546d","permalink":"https://djps.github.io/courses/numericalmethods24/notebooks/bezier/","publishdate":"2023-12-07T00:00:00Z","relpermalink":"/courses/numericalmethods24/notebooks/bezier/","section":"courses","summary":"Examples of Bezier curve","tags":null,"title":"Bezier Curves","type":"book"},{"authors":null,"categories":null,"content":" David Sinden ·\rJanuary 11, 2024 ·\r10 minute read Eigenvectors and Eigenvalues of Matrices (assuming real numbers here) Eigenvectors give the direction of change (they are normalized to length 1).\nEigenvalues give the amplitude of change in that direction.\nimport numpy as np import matplotlib.pyplot as plt import copy Consider the matrix\n\\begin{equation} C = \\left( \\begin{array}{cc} 2 \u0026amp; 1.5 \\\\ 1.5 \u0026amp; 2 \\end{array} \\right) \\end{equation}\nC = np.array([[2, 1.5],[1.5, 2]]) Compute the eigenvalues and eigenvectors of the matrix (all real in this case), so that $C v = e v$\ne, v = np.linalg.eig(C) print(\u0026#39;Eigenvalues are\u0026#39;) print(e) print(\u0026#39;Eigenvectors are\u0026#39;) print(v) Eigenvalues are\r[3.5 0.5]\rEigenvectors are\r[[ 0.70710678 -0.70710678]\r[ 0.70710678 0.70710678]]\rOriginal vectors to be transformed by the matrix. They point to the edges of a rectangle around 0.\nx1 = np.array([1, 1]) x2 = np.array([-1, 1]) x3 = np.array([1, -1]) x4 = np.array([-1, -1]) Transformed coordinates after application of matrix\ny1 = C.dot(x1) y2 = C.dot(x2) y3 = C.dot(x3) y4 = C.dot(x4) Plotting of the original square and the transformed points (connected via lines). Also plotted are the two eigenvectors for which the labels give the eigenvalues\nplt.plot([-1,1,1,-1,-1], [1,1,-1,-1,1]) plt.plot([x1[0],y1[0]], [x1[1],y1[1]], \u0026#39;b-.o\u0026#39;) plt.plot([x2[0],y2[0]], [x2[1],y2[1]], \u0026#39;c--+\u0026#39;) plt.plot([x3[0],y3[0]], [x3[1],y3[1]], \u0026#39;r--+\u0026#39;) plt.plot([x4[0],y4[0]], [x4[1],y4[1]], \u0026#39;k-.o\u0026#39;) plt.plot([y1[0],y3[0],y4[0],y2[0],y1[0]], [y1[1],y3[1],y4[1],y2[1],y1[1]]) plt.plot([0,v[0,0]],[0,v[1,0]], label=\u0026#39;Eigenvector $v_1$ with eigenvalue 3.5\u0026#39;) plt.plot([0,v[0,1]],[0,v[1,1]], label=\u0026#39;Eigenvector $v_2$ with eigenvalue 0.5\u0026#39;) plt.legend() Iterative solvers for linear systems of equations: To solve\n\\begin{equation} A x = b \\end{equation}\nfor $x$.\nThey are of general form $x_{k+1} = \\left( I - Q^{-1}A \\right) x_k + Q^{-1}b$, where you have to choose an appropriate $Q$ that is easy to invert and close to the matrix $A$.\nJacobi: $Q$ is diagonal matrix of $A$. Gauss-Seidel: $Q$ is lower triangular part of $A$. Successive over-relaxtion (SOR): Same as Gauss-Seidel, but $Q$ is multiplied with a factor $1 / \\omega $ for the diagonal elements, where ${0 \u0026lt; \\omega \u0026lt; 2}$. In the example we will make use of a number of routines: np.tril as well as np.diag, np.linalg.inv and np.linalg.norm.\nLet the matrix $A$ be given by\n\\begin{equation} A = \\left( \\begin{array}{ccc} 2 \u0026amp; -1 \u0026amp; 0 \\\\ -1 \u0026amp; 3 \u0026amp; -1 \\\\ 0 \u0026amp; -1 \u0026amp; 2 \\end{array} \\right) \\end{equation}\nA = np.array([[2., -1., 0.], [-1., 3., -1.], [0., -1., 2.]]) Right hand side vector $b=\\left( 1, 8, -5 \\right)^T$.\nb = np.array([1., 8., -5.]) For the Jacobi method, $Q$ is the inverse of diagonal matrix\nQ1 = np.diag(1.0 / np.diag(A)) For the Gauss-Seidel method, $Q$ is the inverse of lower triangular matrix of $A$\ntmp = np.tril(A) Q2 = np.linalg.inv(tmp) For SOR, $Q$ is the inverse of lower triangular matrix of $A$, where the diagonal elements have been multiplied by ${1 / \\omega}$, (where here ${\\omega=1.1}$)\nw = 1.1 tmp = np.tril(A) - np.diag(np.diag(A)) + (1/w) * np.diag(np.diag(A)) Q3 = np.linalg.inv(tmp) First part, compute $\\left(I - Q^{-1} A\\right)$\nIt1 = (np.identity(3) - np.matmul(Q1, A)) It2 = (np.identity(3) - np.matmul(Q2, A)) It3 = (np.identity(3) - np.matmul(Q3, A)) Starting point for iteration; starting error set to $\\sqrt{3}$, i.e. norm([0,0,0] - [1,1,1])\nx = np.array([0., 0., 0.]) x_o = np.array([1., 1., 1.]) Iterate as long as successive iterations are changing by more than ${10^{-4}=0.0001}$\nJacobi iteration\ni = 1 while (np.linalg.norm(x - x_o) \u0026gt; 10**-4): x_o = x x = It1.dot(x) + Q1.dot(b) i += 1 print(\u0026#39;Solution Jacobi \u0026#39;, x, \u0026#39; after \u0026#39;, i, \u0026#39; iterations\u0026#39; ) Gauss-Seidel iteration\nx = np.array([0.,0.,0.]) x_o = np.array([1.,1.,1.]) i = 1 while (np.linalg.norm(x - x_o) \u0026gt; 10**-4): x_o = x x = It2.dot(x) + Q2.dot(b) i += 1 print(\u0026#39;Solution GS \u0026#39;, x, \u0026#39; after \u0026#39;, i, \u0026#39; iterations\u0026#39; ) SOR iteration\nx = np.array([0.,0.,0.]) x_o = np.array([1.,1.,1.]) i = 1 while (np.linalg.norm(x - x_o) \u0026gt; 10**-4): x_o = x x = It3.dot(x) + Q3.dot(b) i += 1 print(\u0026#39;Solution SOR \u0026#39;, x, \u0026#39; after \u0026#39;, i, \u0026#39; iterations\u0026#39; ) Results are:\nConvergence of matrix inversion schemes\rScheme\rSolution\rIterations\rJacobi\r(1.9999746 , 2.99999435, -1.0000254)\r22\rGauss-Seidel\r(1.9999619, 2.9999746, -1.0000127)\r11\rSOR\r(2.00000705, 3.00000339, -0.99999909),\r8\r","date":1701907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670371200,"objectID":"ce4c4885db32143f695d1b51c6b59ca5","permalink":"https://djps.github.io/courses/numericalmethods24/notebooks/iterative/","publishdate":"2023-12-07T00:00:00Z","relpermalink":"/courses/numericalmethods24/notebooks/iterative/","section":"courses","summary":"Examples of iterative solvers for linear systems","tags":null,"title":"Iterative Linear Equation Solvers","type":"book"},{"authors":null,"categories":null,"content":" David Sinden ·\rJanuary 11, 2024 ·\r10 minute read Linear Systems Tridiagonal solver (Thomas algorithm); input has to be a tridiagonal square matrix and a solution vector. If the matrix is singular (i.e. determinant = 0), the algorithm will break (division by zero).\nThe algorithm is not generally stable (i.e. does not magnify small errors in the original matrix via, for example, rounding errors), but it is stable in special cases such as diagonally dominant matrices or symmetric positive definite matrices. In practice, one of the two is often the case.\nFirst import the tools needed\nimport numpy as np from math import factorial import matplotlib.pyplot as plt import copy Thomas Algorithm Create a function for solving tridiagonal matrix problems\ndef tridiag(B, z): s = np.shape(B) x = np.zeros(len(z)) A = copy.deepcopy(B) y = copy.deepcopy(z) if s[0] == s[1]: n = s[0] - 1 for i in range(1, n+1): w = A[i, i-1] / A[i-1, i-1] A[i, i] = A[i, i] - w * A[i-1, i] y[i] = y[i] - w * y[i-1] x[n] = y[n] / A[n, n] for i in range(n, 0, -1): j = i - 1 x[j] = (y[j] - A[j, j+1] * x[j+1]) / A[j, j] else: print(\u0026#39;Not a square matrix\u0026#39;) return x We first check if potential conditions for termination of the algorithm hold. This step might be as costly as or even more costly than the algorithm itself, which is why one wouldn’t do this in general. In many practical cases, there might be indications prior to using the algorithm that the conditions hold. Sometimes, one just simply applies the algorithm without checking at all and just checks if it fails. Below, we first apply the checks and then apply the algorithm.\nConsider a matrix $A$ and right-hand side $y$ for linear system $Ax=b$ \\begin{equation} A = \\left( \\begin{array}{ccc} 2 \u0026amp; -1 \u0026amp; 0 \\\\ 1 \u0026amp; 3 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 2 \\end{array} \\right) \\quad \\mbox{and} \\quad b = \\left( \\begin{array}{ccc} 1 \\\\ 2 \\\\ 5 \\end{array} \\right). \\end{equation}\nA = np.array([[2.,1., 0.], [1.,3., 1.], [0.,1.,2.]]) b = np.array([1., 2., 5.]) Check for positive definiteness and symmetry\nprint(\u0026#39;Matrix is pos. def. \u0026#39;, np.all(np.linalg.eigvals(A) \u0026gt; 0)) At = np.matrix.transpose(A) print(\u0026#39;Matrix is symmetric \u0026#39;, np.all(A==At)) Check for diagonal dominance: first get the absolute value of diagonal coefficients and then the sum of every row except the diagonal element\ndiag = np.diag(np.abs(A)) rowsum = np.sum(np.abs(A), axis=1) - diag print(\u0026#39;Matrix is diagonally dominant \u0026#39;, np.all(diag \u0026gt; rowsum)) Matrix is pos. def. True\rMatrix is symmetric True\rMatrix is diagonally dominant True\rSolve\nx = tridiag(A, b) print(\u0026#39;Solution vector of the problem is \u0026#39;, x) Solution vector of the problem is [ 0.75 -0.5 2.75]\rCholesky decomposition Cholesky decomposition of an input matrix $A$, which must be positive definite and symmetric. It returns the lower triangular matrix $L$. The upper one, $U$, is given by the transpose of $L$, i.e. $U=L^T$.\ndef cholesky(A): \u0026#34;\u0026#34;\u0026#34; Performs a Cholesky decomposition of A, which must be a symmetric and positive definite matrix. The function returns the lower triangular matrix, L. \u0026#34;\u0026#34;\u0026#34; # size n = len(A) # Create zero matrix for L L = np.zeros([n, n]) # Perform the Cholesky decomposition for i in range(0, n): for k in range(0, i+1): # temporary variable tmp = sum(L[i, :] * L[k, :]) # if statement for diagonal and nondiagonal elements if (i == k): L[i, k] = np.sqrt(A[i, i] - tmp) else: L[i, k] = 1.0 / L[k, k] * (A[i, k] - tmp) return L We apply the Cholesky decomposition to the same matrix $A$ as before for the Thomas algorithm.\nFor solving an actual system of ${Ax=b}$ one now only needs to do substitution with the matrix $L$ and its transpose to find $x$.\nL = cholesky(A) print(\u0026#39;Lower triangular matrix of Cholesky decomposition \u0026#39;) print(L) Lower triangular matrix of Cholesky decomposition\r[[1.41421356 0.0 0.0 ]\r[0.70710678 1.58113883 0.0]\r[0.0 0.63245553 1.26491106]]\rCheck that $A = LL^T$\nK = L.dot(L.transpose()) print(\u0026#39;Reconstruction of original matrix by LL^T\u0026#39;) print(K) Reconstruction of original matrix by LL^T\r[[2. 1. 0.]\r[1. 3. 1.]\r[0. 1. 2.]]\r","date":1701907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670371200,"objectID":"c5295451589238d8b56848b454fd4814","permalink":"https://djps.github.io/courses/numericalmethods24/notebooks/linear_systems/","publishdate":"2023-12-07T00:00:00Z","relpermalink":"/courses/numericalmethods24/notebooks/linear_systems/","section":"courses","summary":"Examples of solutions to linear systems of equations","tags":null,"title":"Linear Systems of Equations","type":"book"},{"authors":null,"categories":null,"content":" David Sinden ·\rJanuary 11, 2024 ·\r10 minute read Nonlinear solvers: Newton method vs bisection method: One needs the derivative of the original function for Newton method. Here it is given analytically.\nIt can also be estimated with central or other differencing schemes (e.g. Secant method).\nFor Newton’s method to converge, we need to be ‘sufficiently’ close to the root of the nonlinear equation and the function needs to be sufficiently smooth (differentiable; actually in practice often twice differentiable).\nUnder certain conditions, the convergence is quadratic. For bisection method we only need a continuous function and a change of sign between the endpoints of the interval. However, convergence is only linear.\nTake the function $x^3$ first. Analytic root is $0$. Our starting point for Newton method is $1$.\nimport numpy as np import matplotlib.pyplot as plt import copy def f(x): v = x**3 return v Derivative of function, here analytically\ndef f_deriv(x): dv = 3 * x**2 return dv Array to save iteration results\nn = 20 x_N1 = np.zeros(n) Initial guess\nx_N1[0] = 1.0 Newton method with 20 iterations\nfor i in np.arange(1, n): x_N1[i] = x_N1[i-1] - f(x_N1[i-1]) / f_deriv(x_N1[i-1]) Plotting solution; analytic root should be 0\nplt.plot(x_N1) In this case, convergence is relatively slow.\nOne can observe a saddle point of the function at the root 0 which causes the slower convergence.\nShown below is the error with respect to the analytical solution for each iteration.\nfor i in np.arange(0, 20): print(\u0026#39;%.52f\u0026#39; % np.abs(x_N1[i] - 0.0)) 1.0000000000000000000000000000000000000000000000000000\r0.6666666666666667406815349750104360282421112060546875\r0.4444444444444444752839729062543483451008796691894531\r0.2962962962962962798485477833310142159461975097656250\r0.1975308641975308532323651888873428106307983398437500\r0.1316872427983539206586272030108375474810600280761719\r0.0877914951989026137724181353405583649873733520507812\r0.0585276634659350758482787568937055766582489013671875\r0.0390184423106233815858878699600609252229332923889160\r0.0260122948737489187442939453376311575993895530700684\r0.0173415299158326124961959635584207717329263687133789\r0.0115610199438884089090384676978828792925924062728882\r0.0077073466292589395618128911280564352637156844139099\r0.0051382310861726263745419274187042901758104562759399\r0.0034254873907817512054818642752707091858610510826111\r0.0022836582605211671812006635207126237219199538230896\r0.0015224388403474444983465296843405667459592223167419\r0.0010149592268982963322310197895603778306394815444946\r0.0006766394845988642214873465263735852204263210296631\r0.0004510929897325761115181586013989090133691206574440\rNext, we try the function $x^3 - 2$. We use both bisection method and Newton method.\nInitial interval is $[-2, 2]$ and initial guess for Newton method is $2$. Analytic solution is $2^{1/3}$.\ndef f(x): v = x**3 - 2 return v Derivative of function, here analytically\ndef f_deriv(x): dv = 3 * x**2 return dv Array to save iteration results\nniter: int = 20 x_B = np.zeros(niter) Endpoints of initial interval for bisection method.\nx_B[0] = -2.0 x_B[1] = 2.0 a = -2.0 b = 2.0 tol = 10E-7 Bisection method; One needs to first check that it works (i.e. sign change).\nif (f(a) * f(b) \u0026lt; 0): for i in np.arange(2, niter): # bisect interval x_B[i] = (b + a) / 2.0 # Pick the right half of the interval, so that a sign change occurs if (f(x_B[i]) * f(a) \u0026lt; 0): b = x_B[i] else: a = x_B[i] # Break off, if we find the solution. in practice, a sufficiently close solution would # also work # e.g., np.abs(f(x_B[i]))\u0026lt;10**-7 or something like that # or the difference between two new iterations is very small, # i.e. the update from one iteration to the next is smaller than some threshold if (np.abs(f(x_B[i])) \u0026lt; tol): print(\u0026#39;Found root\u0026#39;) break else: print(\u0026#39;No sign change; bisection does not work!\u0026#39;) Newton method (same as before)\nx_N = np.zeros(niter) x_N[0] = 2.0 for i in np.arange(1, niter): x_N[i] = x_N[i-1] - f(x_N[i-1]) / f_deriv(x_N[i-1]) Plotting iterations for both methods\nplt.plot(x_B, label=\u0026#39;Bisection\u0026#39;) plt.plot(x_N, label=\u0026#39;Newton\u0026#39;) plt.legend() plt.xlim([0,niter]) plt.grid() Error of bisection method for each iteration. One can see a linear reduction of error. Sometimes error gets larger again, as the selection of the interval changes (left or right part of bisected interval).\nfor i in np.arange(0,20): print(\u0026#39;%.52f\u0026#39; % np.abs(x_B[i]-x_true)) 3.2599210498948734127111492853146046400070190429687500\r0.7400789501051268093334556397167034447193145751953125\r1.2599210498948731906665443602832965552806854248046875\r0.2599210498948731906665443602832965552806854248046875\r0.2400789501051268093334556397167034447193145751953125\r0.0099210498948731906665443602832965552806854248046875\r0.1150789501051268093334556397167034447193145751953125\r0.0525789501051268093334556397167034447193145751953125\r0.0213289501051268093334556397167034447193145751953125\r0.0057039501051268093334556397167034447193145751953125 …","date":1701907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670371200,"objectID":"a35fa641f5a690fac5abc542be0127f6","permalink":"https://djps.github.io/courses/numericalmethods24/notebooks/nonlinear/","publishdate":"2023-12-07T00:00:00Z","relpermalink":"/courses/numericalmethods24/notebooks/nonlinear/","section":"courses","summary":"Examples of solutions to nonlinear systems of equations","tags":null,"title":"Nonlinear Solvers","type":"book"},{"authors":null,"categories":null,"content":"Number representations: The order of operations and the respective number representation can lead to rounding errors and error propagation\nThe order of operations matters on computers as addition/subtraction is not associative.\nHere, addition of small number $b$ to large number $a$ is causing the issue.\nimport numpy as np import math Here $c$ and $d$ should be the same:\na = 10**7 b = 10**(-20) c = (a + b - a)**(1/10.0) d = (a - a + b)**(1/10.0) print(\u0026#39;Solution 1 is \u0026#39;, c,\u0026#39; and Solution 2 is \u0026#39;, d) Solution 1 is 0.0 and Solution 2 is 0.009999999999999997\rSubtraction of values of similar size also leads to loss of precision (with double precision often quite difficult, as there may actually be extra digits available for certain computations). But it still becomes obvious for $x=1/150000000$ in the example.\nAlternative here: Use approximation of sine by Taylor expansion for angles close to zero and you actually get a better answer, because first terms of Taylor expansion are accurate enough and we work with numbers of not too similar and not too different sizes in the expansion.\n#Try also removing some zeros below x = 1.0 / 150000000.0 y = math.sin(x) z = x - y print(\u0026#39;%.90f\u0026#39; % x) print(\u0026#39;%.90f\u0026#39; % y) print(\u0026#39;%.90f\u0026#39; % z) 0.000000006666666666666666806150405534189817835510893928585574030876159667968750000000000000\r0.000000006666666666666666806150405534189817835510893928585574030876159667968750000000000000\r0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\rIf we use the Taylor series approximation of $x-\\sin(x)$ for $x$ close to $0$; we can get even better by adding more terms\nz = x - x + x**3 / np.math.factorial(3) - x**5 / np.math.factorial(5) + x**7 / np.math.factorial(7) print(\u0026#39;%.90f\u0026#39; % z) 0.000000000000000000000000049382716049382722698032335288340181891714993182513789445547602556\r","date":1701907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670371200,"objectID":"300a0a45fbb552cb3900ffa31ebe8355","permalink":"https://djps.github.io/courses/numericalmethods24/notebooks/number_representations/","publishdate":"2023-12-07T00:00:00Z","relpermalink":"/courses/numericalmethods24/notebooks/number_representations/","section":"courses","summary":"Examples of number representations","tags":null,"title":"Number Representations","type":"book"},{"authors":null,"categories":null,"content":" David Sinden ·\rJanuary 11, 2024 ·\r10 minute read Solving ordinary differential equations Here is a demonstration of stability/instability of different time-stepping (single-step) schemes for solving ordinary differential equations.\nHere a simple damped system is used\n\\begin{equation} y^{\\prime}\\left(t\\right) = a y\\left(t\\right) \\quad \\mbox{with} \\quad a=-2 \\quad \\mbox{and} \\quad y\\left(t=0\\right) = y_0 = 1. \\end{equation}\nThe solution is given by $y\\left(t\\right) = y_0 e^{a t}$\nExplicit schemes have certain conditions on stability for these types of equations (and in general).\nImplicit schemes are unconditionally stable for this equation (and also in general are more stable), but are more computationally costly.\nThe order of convergence is important if the scheme is stable. RK4 is has the highest convergence (fourth order Runge-Kutta scheme). If the timestep $h$ is sufficiently small, it will outperform all of the other methods used below.\nimport numpy as np import matplotlib.pyplot as plt import copy Define the ordinary differential equation\nf = lambda t, u, a: a * u Set the step size; try different values here\nh = 0.2 # 0.1, 0.5, 1.0, 1.2, 2. Coefficient of ${y^{\\prime} = a y}$; you can also use different values here. Note that if ${a \u0026lt; 0}$, the solution is exponential decay, if ${a \u0026gt; 0}$ it is exponential growth\na = -2 Set discrete times of integration\nt = np.arange(0, 10 + h, h) Set initial condition\nu0 = 1.0 Explicit Euler (first order)\nu = np.zeros(len(t)) u[0] = u0 for i in range(0, len(t) - 1): u[i + 1] = u[i] + h * f(t[i], u[i], a) Implicit Euler (first order)\nui = np.zeros(len(t)) ui[0] = u0 for i in range(0, len(t) - 1): ui[i + 1] = 1.0 / (1.0 - h * a) * ui[i] Heun (explicit, second order)\nuh = np.zeros(len(t)) uh[0] = u0 for i in range(0, len(t) - 1): uh[i + 1] = uh[i] + h / 2.0 * (f(t[i], uh[i], a) + f(t[i+1], uh[i] + h * f(t[i], uh[i], a), a)) Crank-Nicolson (implicit, second order)\nuc = np.zeros(len(t)) uc[0] = u0 for i in range(0, len(t) - 1): uc[i + 1] = 1.0 / (1.0 - h * a / 2.0) * (uc[i] + h / 2.0 * (f(t[i], uc[i], a))) Runge-Kutta 4th order (explicit, fourth order)\nuk = np.zeros(len(t)) uk[0] = u0 for i in range(0, len(t) - 1): K1 = f(t[i], uk[i], a) K2 = f(t[i] + h / 2.0, uk[i] + h / 2.0 * K1, a) K3 = f(t[i] + h / 2.0, uk[i] + h / 2.0 * K2, a) K4 = f(t[i] + h, uk[i] + h * K3, a) uk[i + 1] = uk[i] + h * (K1 / 6.0 + K2 / 3.0 + K3 / 3.0 + K4 / 6.0) Plotting the approximate solutions of the different methods as well as the exact solution.\nplt.figure(figsize = (5, 5)) plt.plot(t, u, \u0026#39;bo--\u0026#39;, label=\u0026#39;Approximate FE\u0026#39;) plt.plot(t, ui, \u0026#39;bo-\u0026#39;, label=\u0026#39;Approximate BE\u0026#39;) plt.plot(t, uh, \u0026#39;ko-.\u0026#39;, label=\u0026#39;Approximate HE\u0026#39;) plt.plot(t, uc, \u0026#39;ko-\u0026#39;, label=\u0026#39;Approximate CN\u0026#39;) plt.plot(t, uk, \u0026#39;go--\u0026#39;, label=\u0026#39;Approximate RK\u0026#39;) t2 = np.arange(0, 10 + 0.1, 0.1) plt.plot(t2, np.exp( a * t2), \u0026#39;g\u0026#39;, label=\u0026#39;Exact\u0026#39;) plt.title(\u0026#39;Approximate\u0026#39; + \u0026#39;\\n\u0026#39; + \u0026#39;and Exact Solution for Simple ODE\u0026#39;) plt.xlabel(\u0026#39;$t$\u0026#39;) plt.ylabel(\u0026#39;$y$\u0026#39;) plt.grid() plt.legend() plt.show() ","date":1701907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670371200,"objectID":"bc479f20906be0c4ac70e3b3de16ae7f","permalink":"https://djps.github.io/courses/numericalmethods24/notebooks/ode_solvers/","publishdate":"2023-12-07T00:00:00Z","relpermalink":"/courses/numericalmethods24/notebooks/ode_solvers/","section":"courses","summary":"Examples of solvers for ordinary differential equations","tags":null,"title":"ODE Solvers","type":"book"},{"authors":null,"categories":null,"content":" David Sinden ·\rJanuary 11, 2024 ·\r10 minute read Runge Phenomena An example for when interpolation is not moving closer to the actual function with more data points (Runge’s phenomenon).\nOscillations at the interval ends increase with increasing amount of data points for interpolation (i.e. with increasing polynomial degree).\nThe approximated function actually moves further and further away from the truth the more information one uses.\nimport numpy as np import matplotlib.pyplot as plt import copy from scipy.interpolate import lagrange Lagrange interpolation of the Runge function for 5 data points, i.e. of degree 4\nSubdivision of interval for interpolation points\nh = 1.0 / 2.0 x = np.arange(-1, 1+h, h) Runge function\ny = 1.0 / (1.0 + 25.0 * x**2) Interpolation, use the built-in Lagrange polynomial function lagrange from the scipy interpolation library\npoly = lagrange(x, y) Plotting\nx_new = np.arange(-1.0, 1.01, 0.01) plt.plot(x_new, poly(x_new), label=\u0026#39;degree 4\u0026#39;) Lagrange interpolation of the Runge function for 7 data points, i.e. of degree 6 (same procedure as above)\nh = 1.0 / 3.0 x = np.arange(-1, 1+h, h) y = 1.0 / (1.0 + 25.0 * x**2) poly = lagrange(x, y) plt.plot(x_new, poly(x_new), label=\u0026#39;degree 6\u0026#39;) Lagrange interpolation of the Runge function for 11 data points, i.e. of degree 10\nh = 1.0 / 5.0 x = np.arange(-1, 1+h, h) y = 1.0 / (1.0 + 25.0 * x**2) poly = lagrange(x, y) plt.plot(x_new, poly(x_new), label=\u0026#39;degree 10\u0026#39;) Analytical representation of the Runge function\ny = 1.0 / (1.0 + 25.0 * x_new**2) plot everything\nplt.plot(x_new, y, label=\u0026#39;analytic\u0026#39;) plt.legend() plt.show ","date":1701907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670371200,"objectID":"c8c16e3e059f4947ce1640df354242a0","permalink":"https://djps.github.io/courses/numericalmethods24/notebooks/runge/","publishdate":"2023-12-07T00:00:00Z","relpermalink":"/courses/numericalmethods24/notebooks/runge/","section":"courses","summary":"Examples of Runge's phenomena in interpolation","tags":null,"title":"Runge Phenomena","type":"book"},{"authors":null,"categories":null,"content":"import math import numpy as np from matplotlib import pyplot as plt Let $g(x)$ be a function given by\n\\begin{equation} e^{x^2 + 1} - 2.718 + 0.16969 x - 4.07799 x^2 + 3.3653 x^3 - 4.1279 x^4 \\end{equation}\nTo find maximum of $g(x)$, we need to find roots of $g^{\\prime}(x)$, so a can define a function gprime(x)\nTo find roots and confirm if it is a maxima, we need gdoubleprime(x), as $g(x)$ is at maximum if $g^{\\prime\\prime}(x)$ is negative\ndef g(x): return np.exp(x**2 + 1) - 2.718 + 0.16969*x - 4.07799*x**2 + 3.3653 * x**3 - 4.1279 * x**4 def gprime(x): return np.exp(x**2 + 1) * 2 * x + 0.16969 - 2.0 * 4.07799 * x + 3 * 3.3653 * x**2 - 4 * 4.1279 * x**3 def gdoubleprime(x): return 2 * (2 * x + 1) * np.exp(x**2 + 1) - 2.0 * 4.07799 + 3 * 2 * 3.3653 * x - 4 * 3 * 4.1279 * x**2 Plot the function over the unit interval\nx = np.linspace(0.0, 1.0, 100) plt.plot(x, g(x)) plt.grid(True) Newton method\ndef newton_step(x, step_num): val = x - gprime(x) / gdoubleprime(x) step_num += 1 print(f\u0026#34;Step Num: {step_num}, x: {val}\u0026#34;) return val, step_num Secant method\ndef secant_step(x1, x2, step_num, f): step_num += 1 val = x2 - f(x2) * (x2 - x1) / (f(x2) - f(x1)) print(f\u0026#34;Step Num: {step_num}, x: {val}\u0026#34;) return x2, val, step_num Use secant method here because we can bound it.\ndef calculate_first_max(x1, x2, nmax = 20): step_num = 0 for i in range(nmax): x1, x2, step_num = secant_step(x1, x2, step_num, gprime) return x2 def calculate_second_max(): step_num = 0 x1 = 0. x2 = 0.7 for i in range(6): x1, x2, step_num = secant_step(x1, x2, step_num, gprime) return x2 print(\u0026#34;Calculating x for first maximum in x -\u0026gt; [0, 1], in interval [0, 0.2]:\u0026#34;) max_x_1 = calculate_first_max(0, 0.2, 19) print(\u0026#34;First maximum value: \u0026#34;, g(max_x_1)) Calculating x for first maximum in x -\u0026gt; [0, 1], in interval [0, 0.2]: Step Num: 1, x: 0.14900380590331655 Step Num: 2, x: -0.08821605866212776 Step Num: 3, x: 0.12814220849590097 Step Num: 4, x: 0.11341387156881519 Step Num: 5, x: 0.0806834268432286 Step Num: 6, x: 0.09017014204250429 Step Num: 7, x: 0.08892692738129448 Step Num: 8, x: 0.08886390723627205 Step Num: 9, x: 0.08886440328278541 Step Num: 10, x: 0.08886440309559314 Step Num: 11, x: 0.08886440309559253 Step Num: 12, x: 0.0888644030955926 Step Num: 13, x: 0.08886440309559303 Step Num: 14, x: 0.08886440309559265 Step Num: 15, x: 0.08886440309559258 Step Num: 16, x: 0.08886440309559261 Step Num: 17, x: 0.0888644030955926 Step Num: 18, x: 0.08886440309559261 Step Num: 19, x: 0.08886440309559261 First maximum value: 0.006812940563426053 print(\u0026#34;Calculating x for second maximum in x -\u0026gt; [0, 1], in interval [0.6, 0.7]:\u0026#34;) max_x_2 = calculate_second_max(0.6, 0.7, 6) print(\u0026#34;Second maximum value: \u0026#34;, g(max_x_2)) Calculating x for second maximum in x -\u0026gt; [0, 1], in interval [0.6, 0.7]: Step Num: 1, x: 0.6307415305490232 Step Num: 2, x: 0.6353034667646523 Step Num: 3, x: 0.6358951723781623 Step Num: 4, x: 0.6358866382102938 Step Num: 5, x: 0.6358866518459403 Step Num: 6, x: 0.6358866518462564 Second maximum value: 0.004225969342977254 plt.plot(x, g(x)) plt.plot(max_x_1, g(max_x_1), \u0026#39;*\u0026#39;) plt.plot(max_x_2, g(max_x_2), \u0026#39;o\u0026#39;) plt.grid(True) ","date":1701907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670371200,"objectID":"22a105977008418b526e5c725bc92829","permalink":"https://djps.github.io/courses/numericalmethods24/notebooks/max_finder/","publishdate":"2023-12-07T00:00:00Z","relpermalink":"/courses/numericalmethods24/notebooks/max_finder/","section":"courses","summary":"Examples of Secant Method","tags":null,"title":"Secant Method","type":"book"},{"authors":null,"categories":null,"content":" David Sinden ·\rJanuary 11, 2024 ·\r10 minute read Taylor Series Expansion This notebook presents the Taylor series expansion of the sine function close to zero for an increasing number of terms in the approximation.\nNote that in this example, only odd powers of $x$ contribute to the expansion.\nFirst import the tools needed\nimport numpy as np from math import factorial import matplotlib.pyplot as plt Define the $x$ values and set up a figure for plotting\nx = np.linspace(-np.pi, np.pi, 200) plt.figure(figsize = (10,8)) First begin with the first order approximation\nn = 0 approximate $y$ values\ny = np.zeros(len(x)) Because only every other term is not equal to zero, we only have to compute $n/2$ terms\nn = n / 2 Taylor series expansion up to $n$\nfor i in np.arange(0, int(n)+1): y = y + ((-1)**i * x**(2*i+1)) / factorial(2*i+1) Plotting\nplt.plot(x, y, label = \u0026#39;First Order\u0026#39;) Third order (same structure as before)\nn = 2 y = np.zeros(len(x)) n = n / 2 for i in np.arange(0, int(n)+1): y = y + ((-1)**i * (x)**(2*i+1)) / factorial(2*i+1) plt.plot(x, y, label = \u0026#39;Third Order\u0026#39;) Seventh order\nn = 6 y = np.zeros(len(x)) n = n / 2 for i in np.arange(0,int(n)+1): y = y + ((-1)**i * (x)**(2*i+1)) / factorial(2*i+1) plt.plot(x, y, label = \u0026#39;Seventh Order\u0026#39;) Eleventh order\nn = 10 y = np.zeros(len(x)) n = n / 2 for i in np.arange(0,int(n)+1): y = y + ((-1)**i * (x)**(2*i+1)) / factorial(2*i+1) plt.plot(x, y, label = \u0026#39;Eleventh Order\u0026#39;, marker=\u0026#39;.\u0026#39;) Plotting of full solution, i.e. $\\sin \\left(x \\right)$\nplt.plot(x, np.sin(x), \u0026#39;k\u0026#39;, label = \u0026#39;sin(x)\u0026#39;) plt.grid() plt.title(\u0026#39;Taylor Series Approximations of Various Orders for $\\sin(x)$\u0026#39;) plt.xlabel(\u0026#39;x\u0026#39;) plt.ylabel(\u0026#39;y\u0026#39;) plt.legend() plt.show() ","date":1701907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670371200,"objectID":"5f05c4dd7468bf290b86652bb8ea89e1","permalink":"https://djps.github.io/courses/numericalmethods24/notebooks/taylor_series/","publishdate":"2023-12-07T00:00:00Z","relpermalink":"/courses/numericalmethods24/notebooks/taylor_series/","section":"courses","summary":"Examples of Taylor series","tags":null,"title":"Taylor Series","type":"book"},{"authors":["David Sinden"],"categories":["Handsday 7"],"content":"Motivated by the problem of electrodynamic space tethers, the equilibrium equations for an elastic conducting rod in a magnetic field are investigated. In body coordinates the equations are found to sit in a family of noncanonical Hamiltonian systems. These systems, which include the classical Euler and Kirchhoff rods, are shown to be completely integrable under certain material conditions. Mel’nikov method is then used to show that two integrable perturbations can in fact destroy the integrable structure, resulting in multimodal rod configurations. Such solutions are investigated numerically revealing a rich bifurcation structure.\n","date":1686009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686009600,"objectID":"8c1f7c16d54e6c87445d7d3ad8c58ec7","permalink":"https://djps.github.io/abstracts/hamburg/","publishdate":"2023-06-06T00:00:00Z","relpermalink":"/abstracts/hamburg/","section":"abstracts","summary":"Motivated by the problem of electrodynamic space tethers, the equilibrium equations for an elastic conducting rod in a magnetic field are investigated. In body coordinates the equations are found to sit in a family of noncanonical Hamiltonian systems.","tags":["Cosserat","Hamiltonian","Bifurcation"],"title":"Integrability, localisation and bifurcation of an elastic conducting rod in a uniform magnetic field","type":"abstracts"},{"authors":["David Sinden","Sven Rothlübbers"],"categories":["ISTU 2023"],"content":"This educational talk presents an accessible introduction to the basic methods and terminology in artificial intelligence which could be applied to develop tools for therapeutic ultrasound delivery. The talk begins with an introduction to AI, focusing on what different neural networks are, and what is necessary to train a network. With this knowledge, a small number of applications are presented, covering segmentation and registration, treatment planning and physics-informed neural networks, treatment monitoring and outcome prediction.\n","date":1681948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681948800,"objectID":"2134af2bc5430470a981815b136481a6","permalink":"https://djps.github.io/abstracts/lyon/","publishdate":"2023-04-20T00:00:00Z","relpermalink":"/abstracts/lyon/","section":"abstracts","summary":"International Symposium on Therapeutic Ultrasound","tags":["AI","HIFU"],"title":"Artificial Intelligence in Therapeutic Ultrasound","type":"abstracts"},{"authors":["David Sinden"],"categories":["Awesome lists"],"content":"\nUseful resources for scientific computing and numerical analysis, with a selection of ultrasound and medical tools\nScientific computing and numerical analysis are research fields that aim to provide methods for solving large-scale problems from various areas of science with the help of computers. Typical problems are ordinary and partial differential equations (ODEs, PDEs), their discretizations, and the solution of linear algebra problems arising from them.\nThis is from my fork of Nico Schlömer’s github repo, which can be found here.\nContents Basic linear algebra Multi-purpose toolkits Finite Elements Meshing Data formats Sparse linear solvers Visualization Platforms I/O ODEs GPUs Solvers Wave propagation Physics-informed neural networks SBML Resources Other libraries and tools Community Basic linear algebra BLAS - Standard building blocks for performing basic vector and matrix operations. (Fortran, public domain, GitHub) OpenBLAS - Optimized BLAS library based on GotoBLAS2. (C and Assembly, BSD, GitHub) BLIS - High-performance BLAS-like dense linear algebra libraries. (C, BSD, GitHub) LAPACK - Routines for solving systems of linear equations, linear least-squares, eigenvalue problems, etc. (Fortran, BSD, GitHub) Eigen - C++ template library for linear algebra. (C++, MPL 2, GitLab) Ginkgo - High-performance manycore linear algebra library, focus on sparse systems. (C++, BSD, GitHub) blaze - High-performance C++ math library for dense and sparse arithmetic. (C++, BSD, Bitbucket) Multi-purpose toolkits PETSc - Parallel solution of scientific applications modeled by PDEs. (C, 2-clause BSD, GitLab) DUNE Numerics - Toolbox for solving PDEs with grid-based methods. (C++, GPL 2, GitLab) SciPy - Python modules for statistics, optimization, integration, linear algebra, etc. (Python, mostly BSD, GitHub) NumPy - Fundamental package needed for scientific computing with Python. (Python, BSD, GitHub) DifferentialEquations.jl - Toolbox for solving different types of differential equations numerically. (Julia, MIT, GitHub) Finite Elements FEniCS - Computing platform for solving PDEs in Python and C++. (C++/Python, LGPL 3, GitHub/Bitbucket) libMesh - Framework for the numerical simulation of PDEs using unstructured discretizations. (C++, LGPL 2.1, GitHub) deal.II - Software library supporting the creation of finite element codes. (C++, LGPL 2.1, GitHub) Netgen/NGSolve - High performance multiphysics finite element software. (C++, LGPL 2.1, GitHub) Firedrake - Automated system for the solution of PDEs using the finite element method. (Python, LGPL 3, GitHub) MOOSE - Multiphysics Object Oriented Simulation Environment. (C++, LGPL 2.1, GitHub) MFEM - Free, lightweight, scalable C++ library for finite element methods. (C++, BSD-3-Clause, GitHub) SfePy - Simple Finite Elements in Python. (Python, BSD, GitHub) FreeFEM - High level multiphysics-multimesh finite element language. (C++, LGPL, GitHub) libceed - Code for Efficient Extensible Discretizations. (C, 2-clause BSD, GitHub) scikit-fem - Simple finite element assemblers. (Python, BSD/GPL, GitHub) Meshing Triangular and tetrahedral meshing Gmsh - Three-dimensional finite element mesh generator with pre- and post-processing facilities. (C++, GPL, GitLab) pygmsh - Python interface for Gmsh. (Python, GPL 3, GitHub) MeshPy - Quality triangular and tetrahedral mesh generation. (Python, MIT, GitHub) CGAL - Algorithms for computational geometry. (C++, mixed LGPL/GPL, GitHub) pygalmesh - Python interface for CGAL’s 3D meshing capabilities. (Python, GPL 3, GitHub) TetGen - Quality tetrahedral mesh generator and 3D Delaunay triangulator. (C++, AGPLv3) Triangle - Two-dimensional quality mesh generator and Delaunay triangulator. (C, nonfree software) distmesh - Simple generator for unstructured triangular and tetrahedral meshes. (MATLAB, GPL 3) trimesh - Loading and using triangular meshes with an emphasis on watertight surfaces. (Python, MIT, GitHub) dmsh - Simple generator for unstructured triangular meshes, inspired by distmesh. (Python, proprietary, GitHub) TetWild - Generate tetrahedral meshes for triangular surface meshes. (C++, GPL 3, GitHub) TriWild - Robust triangulation with curve constraints. (C++, MPL 2, GitHub) fTetWild - Same as TetWild, but faster. (C++, MPL 2, GitHub) SeismicMesh - Parallel 2D/3D triangle/tetrahedral mesh generation with sliver removal. (Python and C++, GPL 3, GitHub) Quadrilateral and hexahedral meshing QuadriFlow - Scalable and robust quadrangulation from triangulation. (C++, BSD, GitHub) Mesh tools meshio - I/O for various mesh formats, file conversion. (Python, MIT, GitHub) MOAB - Representing and evaluating mesh data. (C++, mostly LGPL 3, Bitbucket) optimesh - Triangular mesh smoothing. (Python, proprietary, GitHub) pmp-library - Polygon mesh processing library. (C++, MIT with Employer Disclaimer, GitHub) Mmg - Robust, open-source \u0026amp; multidisciplinary software for remeshing. (C, LGPL 3, GitHub) meshplex - Fast tools for simplex meshes. (Python, …","date":1676764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676764800,"objectID":"3f47d995adb182b97aa0f98cc59ea7e3","permalink":"https://djps.github.io/blog/awesome-scientific-computing/","publishdate":"2023-02-19T00:00:00Z","relpermalink":"/blog/awesome-scientific-computing/","section":"blog","summary":"Useful resources for scientific computing and numerical analysis, with a selection of ultrasound tools","tags":null,"title":"Awesome List of Scientific Computing Resources","type":"blog"},{"authors":["Pauline Coralie Guillemin","David Sinden","Yacine M’Rad","Michael Schwenke","Jennifer Le Guevelou","Johan Uiterwijk","Orane Lorton","Max Scheffler","Pierre-Alexandre Poletti","Jürgen Jenne","Thomas Zilli","Rares Salomir"],"categories":null,"content":"","date":1672099200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672099200,"objectID":"d70ba74d3f8833aa94a5e2d3f6fa1174","permalink":"https://djps.github.io/publication/guillemin2022nct/","publishdate":"2022-12-27T00:00:00Z","relpermalink":"/publication/guillemin2022nct/","section":"publication","summary":"transperineal ultrasound hyperthermia for prostate treatment","tags":["hyperthermia","prostate ultrasound","adjuvant therapies"],"title":"A novel concept of transperineal focused ultrasound transducer for prostate cancer local deep hyperthermia treatments","type":"publication"},{"authors":["Christina A Neizert","Hoang N C Do","Miriam Zibell","Christian Rieder","David Sinden","Stefan M Niehues","Janis L Vahldiek","Kai S Lehmann","Franz G M Poch"],"categories":null,"content":"","date":1665532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665532800,"objectID":"6f541cebb8afd7a74d607dfc93dac6f0","permalink":"https://djps.github.io/publication/neizert2022tav/","publishdate":"2022-10-17T00:00:00Z","relpermalink":"/publication/neizert2022tav/","section":"publication","summary":"The aim of this study was a three-dimensional analysis of vascular cooling effects on microwave ablation (MWA) in an ex vivo porcine model. A glass tube, placed in parallel to the microwave antenna at distances of 2.5, 5.0 and 10.0 mm (A–V distance), simulated a natural liver vessel. Seven flow rates (0, 1, 2, 5, 10, 100, 500 ml/min) were evaluated. Ablations were segmented into 2 mm slices for a 3D-reconstruction. A qualitative and quantitative analysis was performed. 126 experiments were carried out. Cooling effects occurred in all test series with flow rates ≥ 2 ml/min in the ablation periphery. These cooling effects had no impact on the total ablation volume (p \u003e 0.05) but led to changes in ablation shape at A–V distances of 5.0 mm and 10.0 mm. Contrary, at a A–V distance of 2.5 mm only flow rates of ≥ 10 ml/min led to relevant cooling effects in the ablation centre. These cooling effects influenced the ablation shape, whereas the total ablation volume was reduced only at a maximal flow rate of 500 ml/min (p = 0.002). Relevant cooling effects exist in MWA. They mainly depend on the distance of the vessel to the ablation centre.","tags":["Vascular Cooling","MWA"],"title":"Three-dimensional assessment of vascular cooling effects on hepatic microwave ablation in a standardized ex vivo model","type":"publication"},{"authors":["Karl Heimes","Marina Evers","Tim Gerrits","Sandeep Gyawali","David Sinden","Tobias Preusser","Lars Linsen"],"categories":null,"content":"","date":1665014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665014400,"objectID":"80ef548d3db05bf03e509833a277215f","permalink":"https://djps.github.io/publication/heimes2022set/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/heimes2022set/","section":"publication","summary":"Radiofrequency ablation is a minimally invasive, needle-based medical treatment to ablate tumors by heating due to absorption of radiofrequency electromagnetic waves. To ensure the complete target volume of destroyed, radiofrequency ablation simulations required treatment planning. However, the choice of tissue properties used as parameters during simulation induce a high uncertainty, as the tissue properties are strongly patient-dependent. To capture this uncertainty, a simulation ensemble can be created. Understanding the dependency of the simulation outcome on the input parameters helps to create improved simulation ensembles by focusing on the main sources of uncertainty and, thus, reducing computation costs. We present an interactive visual analysis tool for radiofrequency ablation simulation ensembles to target this objective. Spatial 2D and 3D visualizations allow for the comparison of ablation results of individual simulation runs and for the quantification of differences. Simulation runs can be interactively selected based on a parallel coordinates visualization of the parameter space. A 3D parameter space visualization allows for the analysis of the ablation outcome when altering a selected tissue property for the three tissue types involved in the ablation process. We discuss our approach with domain experts working on the development of new simulation models and demonstrate the usefulness of our approach for analyzing the influence of different tissue properties on radiofrequency ablations.","tags":["Thermal ablation","Treatment planning","Uncertainty"],"title":"Studying the effect of tissue properties on radiofrequency ablation by visual simulation ensemble analysis","type":"publication"},{"authors":["David Sinden"],"categories":["Conference"],"content":"Presentation at virtual conference on solving the curl-curl form of Maxwell’s equations for needle-based microwave ablations based on segmented patient data.\nThe conference site, with details on talks and schedules, can be found [here].\nThe conference homepage can be found [here].\nThe presentation can be accessed from [here].\nThe abstract is below or [here]\nPatient-Specific Modelling of Microwave Ablation Introduction: For the frequencies used in radiofrequency ablation it is possible to simplify the harmonic Maxwell equations by considering a quasi-static approximation. The problem reduces to a generalised Laplace equation, which can be solved in a straight forward manner. However, for the frequencies used in microwave ablation, the assumptions which lead to this model are longer valid. Solving the time-harmonic Maxwell equations can be computationally demanding. In order to overcome this, many predictions regarding ablation volumes in needle-based microwave ablation use ellipsoids which are derived from axisymmetric finite-element models, validated against homogeneous phantom data.\nHowever, patient data is not homogeneous, and the presence of vascular structures or tumour heterogeneity mean that predicted ablation volumes based on axisymmetric models may not be accurate.\nMethods: We present a numerical scheme, based on a finite-difference frequency-domain scheme, which can model the electromagnetic, thermal and dosimetric fields for continuous wave exposures on patient data. Data from segmented DICOM images is used to form a number of staggered-grids. A perfectly matched layer suppresses spurious numerical artefacts from the boundary of the domain. A novel preconditioner is constructed which significantly accelerates the computation of the electro-magnetic field. The input into the simulation is provided as the electro-magnetic field in a homogeneous medium. This can be obtained via measurements or via an analytical model, for example by modelling the probe as a dipole source in a vacuum.\nResults: The advantage is that the scheme is fully three-dimensional, can be simulated in inhomogeneous media on large, clinically relevant, domains and can simulate multiple exposures. Having computed the electro-magnetic field for a set of material properties, strategies are presented to update electro-magnetic field as the material properties change due to temperature, dehydration or coagulation in order to reduce the duration of simulations.\nValidation is performed by comparing the predicted and measured ablated volumes, while considering the sources of uncertainty for both the numerical model, such as staircasing errors, and the experimental results, such as in the identification of the ablated regions.\nConclusion: A numerical scheme is presented which may allow patient-specific treatment planning to be performed for microwave ablation.\n","date":1655683200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655683200,"objectID":"5c1be06cd201fa46a604b8f8163c55dd","permalink":"https://djps.github.io/blog/stm/","publishdate":"2022-06-20T00:00:00Z","relpermalink":"/blog/stm/","section":"blog","summary":"Talk on patient-specific computation of microwave ablation presented at Annual Meeting of the Society for Thermal Medicine","tags":["MWA","Thermal Ablation"],"title":"Annual Meeting of the Society for Thermal Medicine 2022","type":"blog"},{"authors":null,"categories":null,"content":" David Sinden ·\rMarch 11, 2022 ·\r10 minute read As will be seen, interpolation of regularly space intervals can produce unsatisfactory results.\nFirst, load the tools needed.\nimport numpy as np import matplotlib.pyplot as plt plt.style.use(\u0026#39;seaborn-poster\u0026#39;) Now define the basis polynomial function\ndef lagrange(x_int, y_int, x_new): \u0026#34;\u0026#34;\u0026#34; This function takes pairs of points (x_int, y_int) and, from a set of points x_new computes the Lagrange polynomial to return the interpolated values y_new \u0026#34;\u0026#34;\u0026#34; y_new = np.zeros(x_new.shape, dtype=np.float) for xi, yi in zip(x_int, y_int): y_new += yi * np.prod( [(x_new - xj) / (xi - xj) for xj in x_int if xi != xj], axis=0) return y_new Define the Runge function as \\begin{equation} f\\left(x\\right) = \\dfrac{1}{1+25 x^2}\\end{equation}\nrunge = lambda x: 1.0 / (1.0 + 25.0 * x**2) Let the range be $(-1,1)$.\nx = np.linspace(-1, 1, 100) x_int = np.linspace(-1, 1, 11) y_int = runge(x_int) x_new = np.linspace(-1, 1, 1000) y_new = lagrange(x_int, runge(x_int), x_new) x_new0 = np.linspace(-1, 1, 20) y_new0 = lagrange(x_int, runge(x_int), x_new0) plt.plot(x, runge(x), \u0026#34;k--\u0026#34;, label=\u0026#34;Runge function\u0026#34;) plt.plot(x_new, y_new, label=\u0026#34;Uniform Interpolation n=1000\u0026#34;) plt.plot(x_new0, y_new0, label=\u0026#34;Uniform Interpolation n=100\u0026#34;) plt.plot(x_int, y_int, \u0026#34;k*\u0026#34;) plt.legend() plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.grid(True) plt.show() import numpy as np from scipy.interpolate import lagrange import matplotlib.pyplot as plt x = [0, 1, 2] y = [1, 3, 2] x_new = np.arange(-1.0, 3.1, 0.1) f = lagrange(x, y) fig = plt.figure() plt.plot(x_new, f(x_new), \u0026#39;b\u0026#39;, x, y, \u0026#39;ro\u0026#39;) plt.title(r\u0026#39;Lagrange Polynomial\u0026#39;) plt.grid() plt.xlabel(r\u0026#39;$x$\u0026#39;) plt.ylabel(r\u0026#39;$y$\u0026#39;) plt.show() ","date":1655424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655424000,"objectID":"a5ea70e1099d13a15bedbd43c4a57aef","permalink":"https://djps.github.io/courses/numericalanalysis22/notebooks/lagrangeinterpolation/","publishdate":"2022-06-17T00:00:00Z","relpermalink":"/courses/numericalanalysis22/notebooks/lagrangeinterpolation/","section":"courses","summary":"Examples of Lagrange Interpolation","tags":null,"title":"Lagrange Interpolation","type":"book"},{"authors":null,"categories":null,"content":" David Sinden ·\rMarch 11, 2022 ·\r20 minute read This notebook uses the scipy lagrange function to compute the Lagrange polynomial.\nimport numpy as np from numpy.polynomial.polynomial import Polynomial from scipy.interpolate import lagrange import matplotlib.pyplot as plt from IPython.display import Markdown as md plt.style.use(\u0026#39;seaborn-poster\u0026#39;) For some points $x$, define some data $y$ and create the Lagrange polynomial $f$, which has coefficients $p$\nx = [0.0, 1.0, 2.0] y = [1.0, 3.0, 2.0] f = lagrange(x, y) p = Polynomial(f.coef[::-1]).coef z = [\u0026#39;\u0026#39; if i==0 else (\u0026#39;x\u0026#39; if (i==1) else \u0026#39;x^{}\u0026#39;.format(i)) for i in np.arange(np.size(x))] string= [str(p[i]) + str(z[i]) for i in np.arange(np.size(x))] print(string) md(\u0026#34;Coefficients of the interpolating polynomial: {}\u0026#34;.format(p)) [\u0026#39;1.0\u0026#39;, \u0026#39;3.5x\u0026#39;, \u0026#39;-1.5x^2\u0026#39;]\rCoefficients of the interpolating polynomial: [ 1.0, 3.5, -1.5]\ndef lagrange_basis(x_int, y_int, x_new): \u0026#34;\u0026#34;\u0026#34; This function takes pairs of points (x_int, y_int) and, from a set of points x_new computes the Lagrange polynomial to return the interpolated values y_new \u0026#34;\u0026#34;\u0026#34; l = np.zeros((np.size(x_int), np.size(x_new)), dtype=np.float) i = np.int(0) for xi, yi in zip(x_int, y_int): l[i,:] = np.prod( [(x_new - xj) / (xi - xj) for xj in x_int if xi != xj], axis=0) i=i+1 return l Plot the data and the individual components of the polynomial\nx_new = np.arange(-1.0, 3.1, 0.1) test = lagrange_basis(x, y, x_new) fig = plt.figure() fig.subplots_adjust(top=1.3, hspace=0.4) ax=fig.add_subplot(3,1,1) ax.axvline(x=0.0, color=\u0026#34;grey\u0026#34;, linestyle=\u0026#34;-\u0026#34;, linewidth=1.75) ax.plot(x_new, f(x_new), \u0026#39;b\u0026#39;, x, y, \u0026#39;ro\u0026#39;) ax.set_title(r\u0026#39;Lagrange Interpolation\u0026#39;, fontsize=12) ax.grid() ax.set_ylabel(r\u0026#39;$y$\u0026#39;) ax.tick_params(axis = \u0026#39;both\u0026#39;, which = \u0026#39;major\u0026#39;, labelsize = 12) ax1=fig.add_subplot(3,1,2) ax1.axhline(y=0.0, color=\u0026#34;grey\u0026#34;, linestyle=\u0026#34;-\u0026#34;, linewidth=1.75) ax1.axhline(y=1.0, color=\u0026#34;grey\u0026#34;, linestyle=\u0026#34;-\u0026#34;, linewidth=1.75) ax1.scatter(np.array(x), np.array(y), c=[\u0026#39;tab:blue\u0026#39;, \u0026#39;tab:orange\u0026#39;, \u0026#39;tab:green\u0026#39;]) ax1.plot(x_new, test[0,:], color=\u0026#39;tab:blue\u0026#39;, label=\u0026#39;$l_0(x)$\u0026#39; ) ax1.plot(x_new, test[1,:], color=\u0026#39;tab:orange\u0026#39;, label=\u0026#39;$l_1(x)$\u0026#39; ) ax1.plot(x_new, test[2,:], color=\u0026#39;tab:green\u0026#39;, label=\u0026#39;$l_2(x)$\u0026#39; ) ax1.set_ylim([-0.5, 6.5]) ax1.grid() ax1.set_ylabel(r\u0026#39;$y$\u0026#39;) ax1.set_title(r\u0026#39;Lagrange Basis Functions\u0026#39;, fontsize=12) ax1.legend(prop={\u0026#39;size\u0026#39;: 12}) ax1.tick_params(axis = \u0026#39;both\u0026#39;, which = \u0026#39;major\u0026#39;, labelsize = 12) ax2=fig.add_subplot(3,1,3) ax2.grid() ax2.axhline(y=0.0, color=\u0026#34;grey\u0026#34;, linestyle=\u0026#34;-\u0026#34;, linewidth=1.75) ax2.axhline(y=1.0, color=\u0026#34;grey\u0026#34;, linestyle=\u0026#34;-\u0026#34;, linewidth=1.75) ax2.scatter(np.array(x), np.array(y), c=[\u0026#39;tab:blue\u0026#39;, \u0026#39;tab:orange\u0026#39;, \u0026#39;tab:green\u0026#39;]) ax2.plot(x_new, y[0]*test[0,:], color=\u0026#39;tab:blue\u0026#39;, label=\u0026#39;$y_0 l_0(x)$\u0026#39;) ax2.plot(x_new, y[1]*test[1,:], color=\u0026#39;tab:orange\u0026#39;, label=\u0026#39;$y_1 l_1(x)$\u0026#39; ) ax2.plot(x_new, y[2]*test[2,:], color=\u0026#39;tab:green\u0026#39;, label=\u0026#39;$y_2 l_2(x)$\u0026#39; ) ax2.set_xlabel(r\u0026#39;$x$\u0026#39;, fontsize=12) ax2.set_ylabel(r\u0026#39;$y$\u0026#39;, fontsize=12) ax2.set_title(r\u0026#39;Scaled Lagrange Basis Functions\u0026#39;, fontsize=12) ax2.legend(prop={\u0026#39;size\u0026#39;: 12}) ax2.tick_params(axis = \u0026#39;both\u0026#39;, which = \u0026#39;major\u0026#39;, labelsize = 12) plt.show() Now use a different set of points, defined by a function $x \\log(x)$.\ng = lambda x: x * np.log(x) We can compute the polynomials of different orders and the errors.\nm = np.array([3,8]) x0 = 0.1 x1 = 2.0 dx = 0.01 x_new = np.arange(x0, x1, dx) err = np.zeros( (m.size, x_new.size) ) y_new = np.zeros( (m.size, x_new.size) ) fig1 = plt.figure() fig1.subplots_adjust(wspace=0.3) for i in np.arange(m.size): z = np.linspace(x0, x1, m[i]) y = g(z) f = lagrange(z, y) p = Polynomial(f.coef[::-1]).coef err[i,:] = g(x_new) - f(x_new) md(\u0026#34;Coefficients of the interpolating polynomial: {}\u0026#34;.format(p)) ax=fig1.add_subplot(1,m.size,i+1) clrs = \u0026#39;tab:orange\u0026#39; if (i==0) else \u0026#39;tab:green\u0026#39; ax.plot(x_new, f(x_new), \u0026#39;-\u0026#39;, c=clrs, label=\u0026#34;interpolation\u0026#34;) ax.plot(z, y, \u0026#39;ro\u0026#39;, label=\u0026#34;data\u0026#34;) ax.plot(x_new, np.log(x_new) * x_new, linestyle=\u0026#39;--\u0026#39;, label=\u0026#34;function\u0026#34;) ax.set_title(r\u0026#39;Lagrange Interpolation of order {}\u0026#39;.format(m[i])) ax.legend() ax.grid() ax.set_xlabel(r\u0026#39;$x$\u0026#39;) ax.set_ylabel(r\u0026#39;$y$\u0026#39;) plt.show() Plot the errors\nfig2=plt.figure() for i in np.arange(m.size): ax=fig2.add_subplot(m.size,1,i+1) clrs = \u0026#39;tab:orange\u0026#39; if (i==0) else \u0026#39;tab:green\u0026#39; ax.plot(x_new, err[i], c=clrs, label=\u0026#34;Error Order {}\u0026#34;.format(m[i])) ax.legend() ax.grid() ax.set_xlabel(r\u0026#39;$x$\u0026#39;) ax.set_ylabel(r\u0026#39;$y$\u0026#39;) plt.show() Compute the bounds on the errors.\nfig3=plt.figure() for i in np.arange(m.size): z = np.linspace(x0, x1, m[i]) bounds1 = np.prod( [(x_new - z[i]) for i in np.arange(0,m[i])], axis=0) bounds2 = (m[i]-2) * (-1)**(m[i]-2) / ( np.math.factorial(m[i]) * x_new**(m[i]-1) ) ax=fig3.add_subplot(m.size,1,i+1) b=np.max(np.abs(bounds1 * bounds2)) ax.plot(x_new, err[i], c=clrs, label=\u0026#34;Error Order {}\u0026#34;.format(m[i])) ax.axhline(y=b) ax.axhline(y=-b) ax.legend() ax.grid() ax.set_xlabel(r\u0026#39;$x$\u0026#39;) ax.set_ylabel(r\u0026#39;$y$\u0026#39;) plt.show() ","date":1655424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655424000,"objectID":"51370f48a77b14d42a7ae1576fc76c3d","permalink":"https://djps.github.io/courses/numericalanalysis22/notebooks/characteristicpolynomial/","publishdate":"2022-06-17T00:00:00Z","relpermalink":"/courses/numericalanalysis22/notebooks/characteristicpolynomial/","section":"courses","summary":"Examples of Lagrange polynomials to interpolate and integrate","tags":null,"title":"Lagrange Polynomials","type":"book"},{"authors":null,"categories":null,"content":" David Sinden ·\rMarch 11, 2022 ·\r10 minute read Consider the integration of the function $f(x) = \\exp \\left( x^2 \\right)$ between $[0,1]$.\nIt can easily be shown that\n\\begin{align} \\int_0^1 e^{-x^2} \\mathrm{d}x = \\dfrac{\\sqrt{\\pi}}{2}. \\nonumber \\end{align}\nFirst import the tools needed\nimport numpy as np import matplotlib.pyplot as plt import scipy.integrate as spi Next define the range to plot the function over and a number of point to evaluate the function.\nx0 = -0.5 x1 = 1.5 n = 100 So we have a range and a function.\nx = np.linspace(x0, x1, n) y = np.exp(-x**2) f = lambda x : np.exp(-x**2) Let $m$ be vector whose elements are the number of evaluations of the integral.\nm = np.array([4,8,16], dtype=int) Compute and plot the successive approximations for the integral, as well as keeping track of the errors.\nWe will use the function trapezoid to compute the integral according to\n\\begin{align} \\int_a^b f(x) ,\\mathrm{d}x \\approx \\sum\\limits_{k=1}^{M} \\dfrac{f\\left( x_{k-1}\\right) + f\\left(x_{k}\\right)}{2} \\Delta x_{k} \\quad \\mbox{where} \\quad x_0 = a \\quad \\mbox{and} \\quad x_M = b. \\nonumber \\end{align}\nfig, ax = plt.subplots(m.size,1) fig.tight_layout() fig.subplots_adjust(top=0.99) T = np.zeros((m.size,)) absolute_error = np.zeros((m.size,)) relative_error = np.zeros((m.size,)) for j,k in enumerate(m): nsteps = np.float(k) dx = 1.0 / nsteps x_approx = np.linspace(0.0, 1.0, num=k) T[j] = spi.trapz(f(x_approx), x_approx) for i in np.arange(0, nsteps): x_start = i*dx x_stop = (i+1)*dx y_start = np.exp(-x_start**2) y_stop = np.exp(-x_stop**2) ax[j].fill_between([x_start,x_stop], [y_start,y_stop], facecolor=\u0026#39;b\u0026#39;, edgecolor=\u0026#39;b\u0026#39;, alpha=0.2) ax[j].scatter([x_start,x_stop], [y_start,y_stop]) ax[j].plot(x, y, \u0026#39;b-\u0026#39;) ax[j].set_title(r\u0026#39;Trapezoid Rule, $N$ = {}, Approx: {}\u0026#39;.format(k,T[j])) ax[j].grid(True) if (j==m.size-1): ax[j].set_xlabel(\u0026#39;x\u0026#39;) ax[j].set_ylabel(\u0026#39;y\u0026#39;) absolute_error[j] = np.abs( np.sqrt(np.pi)/2.0 - T[j]) relative_error[j] = np.abs( (np.sqrt(np.pi)/2.0 - T[j])/(np.sqrt(np.pi)/2.0 ) ) Tabulate the data and display it\nresults = np.vstack([m.T, relative_error, absolute_error]) import pandas as pd df = pd.DataFrame(results.T, columns=[\u0026#34;Order\u0026#34;, \u0026#34;Relative Error\u0026#34;, \u0026#34;Absolute Error\u0026#34;]) df.Order = df.Order.astype(int) display(df.style.hide_index().set_caption(\u0026#34;Results for $e^{-x^2}$\u0026#34;)) Results for $e^{-x^2}$\rOrder\rRelative Error\rAbsolute Error\r4\r0.165015\r0.146240\r8\r0.158712\r0.140655\r16\r0.157607\r0.139675\r","date":1655424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655424000,"objectID":"2dddb03394cda67a0744f4950d4a55f2","permalink":"https://djps.github.io/courses/numericalanalysis22/notebooks/integrals/","publishdate":"2022-06-17T00:00:00Z","relpermalink":"/courses/numericalanalysis22/notebooks/integrals/","section":"courses","summary":"Examples of numerical integration","tags":null,"title":"Numerical Integration","type":"book"},{"authors":null,"categories":null,"content":" David Sinden ·\rMarch 11, 2022 ·\r20 minute read This notebook is based on an example from Chapter 2 of An Introduction to Computational Stochastic PDEs.\nConstruction of Matrices The matrices assembled have few non-zero entries, so rather than store all entries in the matrix, only the non-zero values and there locations are stored. For this compressed sparse column matrices are used, but this is an implementational detail.\nFirst import the necessary libraries\nimport numpy as np import scipy from scipy import sparse from scipy.sparse import linalg import matplotlib import matplotlib.pyplot as plt plt.style.use(\u0026#39;seaborn-poster\u0026#39;) Now define the function which assembles and solves the boundary value problem:\n\\begin{align} -\\dfrac{d}{dx} \\left( \\alpha(x) \\dfrac{d u(x)}{dx} \\right) + \\gamma(x) u(x) = f(x) \\quad a \u0026lt; x \u0026lt; b \\end{align}\nwith\n\\begin{equation} u(a) = 0 \\quad \\mbox{and} \\quad u(b) = 0. \\end{equation}\nNote that the boundary value problem has a solution on (0,1) when $\\alpha$, $\\gamma$ and $f$ are constant, given by\n\\begin{equation} u(x) = \\dfrac{f}{\\alpha} \\left( 1 - \\dfrac{ e^{\\sqrt{s}x} + e^{\\sqrt{s}(1-x)} }{ 1 + e^{\\sqrt{s}} } \\right) \\quad \\mbox{where} \\quad s =\\gamma / \\alpha. \\end{equation}\nNow define two functions to solve the problem numerically\nget_elt_arrays : which maps the basis functions one_dimensional_linear_FEM : which assembles and solves the finite element solution for the boundary value problem. def one_dimensional_linear_FEM(N, alpha, gamma, f): \u0026#34;\u0026#34;\u0026#34; Solves the boundary value problem, returns the solution and the assembled matrices. \u0026#34;\u0026#34;\u0026#34; # step size h = 1.0 / N # nodal points xx = np.linspace(0.0, 1.0, N+1) # size of system nvtx = N + 1 # number of elements J = N- 1 # allocate matrix elt2vert = np.vstack((np.arange(0, J + 1, dtype=\u0026#39;int\u0026#39;), np.arange(1, J + 2, dtype=\u0026#39;int\u0026#39;))) # allocate space global matrices K = sparse.csc_matrix((nvtx, nvtx)) M = sparse.csc_matrix((nvtx, nvtx)) # allocate right hand side of linear system b = np.zeros(nvtx) # compute element matrices Kks, Mks, bks = get_elt_arrays(h, alpha, gamma, f, N) # Assemble element arrays into global arrays K = sum(sparse.csc_matrix((Kks[:, row_no, col_no], (elt2vert[row_no,:], elt2vert[col_no,:])), (nvtx,nvtx)) for row_no in range(2) for col_no in range(2)) M = sum(sparse.csc_matrix((Mks[:, row_no, col_no], (elt2vert[row_no,:], elt2vert[col_no,:])), (nvtx,nvtx)) for row_no in range(2) for col_no in range(2)) for row_no in range(2): nrow = elt2vert[row_no,:] b[nrow] = b[nrow] + bks[:, row_no] # set lefthand side of the linear system A = K + M # impose homogeneous boundary condition A = A[1:-1, 1:-1] K = K[1:-1, 1:-1] M = M[1:-1, 1:-1] b = b[1:-1] # solve linear system for interior degrees of freedom u_int = sparse.linalg.spsolve(A,b) # append boundary data to interior solution uh = np.hstack([0.0, u_int, 0.0]) # return solution and the components of the linear system return uh, A, b, K, M def get_elt_arrays(h, alpha, gamma, f, N): \u0026#34;\u0026#34;\u0026#34; Assemble stiffness and mass matrices and right handside vector. \u0026#34;\u0026#34;\u0026#34; Kks = np.zeros((N, 2, 2)); Kks[:,0,0] = alpha /h Kks[:,0,1] = -alpha / h Kks[:,1,0] = -alpha / h Kks[:,1,1] = alpha / h Mks = np.zeros_like(Kks) Mks[:,0,0] = gamma * h / 3.0 Mks[:,0,1] = gamma * h / 6.0 Mks[:,1,0] = gamma * h / 6.0 Mks[:,1,1] = gamma * h / 3.0 bks = np.zeros((N, 2)) bks[:,0] = f * (h / 2.0) bks[:,1] = f * (h / 2.0) return Kks, Mks, bks Now create a function eval_sol which evaluates the exact solution for constant $\\alpha$, $\\gamma$ and $f$ on $(0,1)$\ndef eval_sol(f, alpha, gamma, x): \u0026#34;\u0026#34;\u0026#34; Evaluate exact solution \u0026#34;\u0026#34;\u0026#34; s = gamma / alpha out = (f / gamma) * (1.0 - (np.exp(np.sqrt(s) * x) + np.exp(np.sqrt(s) * (1.0-x))) / (1.0 + np.exp(np.sqrt(s))) ) return out Evaluate the exact solution for $\\alpha=1$, $\\gamma=10$ and $f=1$\nx0 = 0.0 x1 = 1.0 L = 100 x = np.linspace(x0, x1, L) f = 1.0 alpha = 1.0 gamma = 10.0 u = eval_sol(f, alpha, gamma, x) For a number of different step sizes compute the finite element approximation\nnpowers = 6 fig, axes = plt.subplots(2, int(npowers/2), constrained_layout=True) for i in range(1, 1+npowers): plt.subplot(2, int(npowers/2), i) N = int( np.power(2, i) ) xh = np.linspace(x0, x1, N+1) uh, A, b, K, M = one_dimensional_linear_FEM(N, alpha, gamma, f) axes[(i-1)%2, (i-1)//2].plot(xh, uh, \u0026#39;--ob\u0026#39;, label=\u0026#34;$N$={}\u0026#34;.format(N)) axes[(i-1)%2, (i-1)//2].set_xlabel(r\u0026#39;$x$\u0026#39;) axes[(i-1)%2, (i-1)//2].set_ylabel(r\u0026#39;$u$\u0026#39;) axes[(i-1)%2, (i-1)//2].plot(x, u, \u0026#39;-k\u0026#39;) axes[(i-1)%2, (i-1)//2].grid(True) axes[(i-1)%2, (i-1)//2].legend(loc=\u0026#39;lower center\u0026#39;) Error Analysis Compute the error via a norm defined using the bilinear form\n\\begin{equation} a(u,v) = \\int_a^b \\alpha u^{\\prime}(x) v^{\\prime}(x) + \\gamma u(x) v(x) \\mathrm{d}x \\end{equation}\nthat is\n\\begin{equation} \\left\\lVert u_{}^{} \\right\\Vert_V = \\sqrt{a(u,u)} \\end{equation}\nso that the error is given by\n\\begin{equation} \\left\\Vert u - u_h \\right\\Vert_V^2. \\end{equation}\nWhen $\\alpha=1$, $\\gamma=0$ and $f=1$, the exact solution is given by $u=\\frac{1}{2}x(1-x)$. Thus …","date":1655424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655424000,"objectID":"d377b98149d9ffcf39e78b5c0c83f568","permalink":"https://djps.github.io/courses/numericalanalysis22/notebooks/1dfem/","publishdate":"2022-06-17T00:00:00Z","relpermalink":"/courses/numericalanalysis22/notebooks/1dfem/","section":"courses","summary":"Example of one-dimensional finite element simulation","tags":null,"title":"One-Dimensional Finite Element Simulations","type":"book"},{"authors":null,"categories":null,"content":" David Sinden ·\rMarch 11, 2022 ·\r20 minute read This notebook, like the one-dimensional finite element example is based on an example from Chapter 2 of An Introduction to Computational Stochastic PDEs.\nThe matrices assembled have few non-zero entries, so rather than store all entries in the matrix, only the non-zero values and there locations are stored. For this compressed sparse column matrices are used, but this is an implementational detail.\nFirst import the necessary libraries\nimport numpy as np import scipy from scipy import sparse from scipy.sparse import linalg import matplotlib from matplotlib import cm import matplotlib.pyplot as plt plt.style.use(\u0026#39;seaborn-poster\u0026#39;) Consider the partial differential equation\n\\begin{equation} -\\nabla \\cdot \\left( a(x) \\nabla u \\left(x\\right) \\right) = f\\left(x\\right) \\quad x \\in \\Omega \\end{equation}\nwhere $a(x) \u0026gt; 0$ for all $x \\in \\Omega$, and with boundary conditions\n\\begin{equation} u = g(x) \\quad x \\in \\partial\\Omega. \\end{equation}\nLabel the boundary vertices after the interior vertices, thus construct a solution of the form\n\\begin{equation} w(x) = \\sum_{i=1}^{J} w_i \\phi_i(x) + \\sum_{i=J+1}^{J+J_b} w_i \\phi_i(x) \\end{equation}\nso, by definition let the solution take the form,\n\\begin{equation} w(x) = w_0(x) + w_g (x) \\end{equation}\nas $w_0$ is zero on the boundary, the values on the boundary are determined by $w_g$.\nFix the values as\n\\begin{equation} w_B(x) = \\left( w_{J+1}, \\ldots, w_{J+J_b} \\right)^T \\end{equation}\nwhere the values of $w_B$ are interpolated as\n\\begin{equation} w_i \\left(x \\right) = g \\left( x_i \\right) \\quad \\mbox{for} \\quad i=J+1,\\ldots,J+J_b. \\end{equation}\nSo the finite element approximation is given by\n\\begin{equation} u_h(x) = \\sum_{i=1}^{J}u_i \\phi_i(x) + \\sum_{i=J+1}^{J+J_b} w_i \\phi_i(x) \\end{equation}\nSubstituting this into the weak form equation yields\n\\begin{equation} \\sum_{i=1}^{J} u_i a\\left( \\phi_i, \\phi_j \\right) = \\left(f,\\phi_j \\right) - \\sum_{i=J+1}^{J+J_b} w_i a\\left( \\phi_i(x), \\phi_j(x) \\right) \\quad \\mbox{for} \\quad j=1,\\ldots,J. \\end{equation}\nFor a linear basis function, the linear system is sparse and can be partition into interior and boundary parts\n\\begin{equation} A = \\left( \\begin{array}{cc} A_{II} \u0026amp; A_{IB} \\newline A_{BI} \u0026amp; A_{BB} \\end{array} \\right) \\quad \\mbox{and} \\quad b = \\left( \\begin{array}{c} b_I, \u0026amp; b_B \\end{array} \\right)^T. \\end{equation}\nSo that the governing equation for the unknown interior values can be written as\n\\begin{equation} A_{II} u_I = b_I - A_{IB} w_B. \\end{equation}\nConsider the local piecewise basis functions associated with the reference triangle as\n\\begin{equation} \\psi_1(x) = 1-s-t, \\quad \\psi_2(x) = s \\quad \\mbox{and} \\quad \\psi_3(x) = t. \\label{eq:local} \\end{equation}\nwhere \\begin{equation} \\phi_p^k\\left( x\\left(s,t\\right), y\\left(s,t\\right) \\right) = \\psi_p\\left(s,t\\right) \\quad p=1,2,3. \\end{equation}\nThen the solution takes the form\n\\begin{align} x(s,t) \u0026amp; = x_1^k \\psi_1 + x_2^k \\psi_2 + x_3^k \\psi_3, \\newline y(s,t) \u0026amp; = y_1^k \\psi_1 + y_2^k \\psi_2 + y_3^k \\psi_3. \\end{align}\nThe Jacobian between the two frames is given by\n\\begin{equation} J = \\left( \\begin{array}{cc} \\dfrac{\\partial x}{\\partial s} \u0026amp; \\dfrac{\\partial y}{\\partial s} \\newline \\dfrac{\\partial x}{\\partial t} \u0026amp; \\dfrac{\\partial y}{\\partial t} \\end{array} \\right). \\end{equation}\nThus, for the local basis functions given above, the Jacobian and inverse are given by\n\\begin{equation} J = \\left( \\begin{array}{cc} x_2 - x_1 \u0026amp; y_2 - y_1 \\newline x_3 - x_1 \u0026amp; y_3 - y_1 \\end{array} \\right) \\end{equation}\nand\n\\begin{equation} J^{-1} = \\dfrac{1}{|J|} \\left( \\begin{array}{cc} y_3 - y_1 \u0026amp; y_1 - y_2 \\newline x_1 - x_3 \u0026amp; x_2 - x_1 \\end{array} \\right) \\end{equation}\nwhere the determinant of the Jacobian is given by\n\\begin{equation} |J| = \\left( x_2 - x_1 \\right)\\left( y_3 - y_1 \\right) - \\left( y_2 - y_1 \\right) \\left( x_3 - x_1 \\right). \\end{equation}\nNext create a uniform mesh on a square domain between $(x_0,x_1)$ with $N$ elements on the line. Thus $N+1$ vertices on the line, so that there are $(N+1)^2$ vertices in the domain.\nuniform_mesh_info : this creates the uniform mesh. It returns the $x$ and $y$ locations of the vertices as xv and yv and the array elt2vert this takes the label of element and returns the labels of the vertices of that element, as well as the number of vertices nvtx, number of elements ne and the step size h. get_jac_info : computes the Jacobian matrix and it’s inverse. get_elt_arrays2D : computes the matrix $A$ and the vector $b$ for the governing equation. two_dimensional_linear_FEM : assembles and computes the solution def uniform_mesh_info(x0, x1, N): \u0026#34;\u0026#34;\u0026#34; Create a uniform square mesh of right angle triangles \u0026#34;\u0026#34;\u0026#34; h = 1 / N x = np.linspace(x0, x1, N+1); y = np.copy(x) # co-ordinates of vertices xv, yv = np.meshgrid(x, y) xv = xv.ravel() yv = yv.ravel() # N squared n2 = N * N # number of vertices nvtx = (N+1) * (N+1) # number of elements as each square is divided into two ne = 2 * n2 # global vertex labels of …","date":1655424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655424000,"objectID":"c8562ddb217597bde2baab652db96793","permalink":"https://djps.github.io/courses/numericalanalysis22/notebooks/2dfem/","publishdate":"2022-06-17T00:00:00Z","relpermalink":"/courses/numericalanalysis22/notebooks/2dfem/","section":"courses","summary":"Example of two-dimensional finite element simulation","tags":null,"title":"Two-Dimensional Finite Element Simulations","type":"book"},{"authors":["David Sinden","Christian Rieder","Miriam Zibell","Franz Poch","Kai Lehmann","Tobias Preusser"],"categories":["Society of Thermal Medicine 2022 Annual Meeting"],"content":"Introduction: For the frequencies used in radiofrequency ablation it is possible to simplify the harmonic Maxwell equations by considering a quasi-static approximation. The problem reduces to a generalised Laplace equation, which can be solved in a straight forward manner. However, for the frequencies used in microwave ablation, the assumptions which lead to this model are longer valid. Solving the time-harmonic Maxwell equations can be computationally demanding. In order to overcome this, many predictions regarding ablation volumes in needle-based microwave ablation use ellipsoids which are derived from axisymmetric finite-element models, validated against homogeneous phantom data.\nHowever, patient data is not homogeneous, and the presence of vascular structures or tumour heterogeneity mean that predicted ablation volumes based on axisymmetric models may not be accurate.\nMethods: We present a numerical scheme, based on a finite-difference frequency-domain scheme, which can model the electromagnetic, thermal and dosimetric fields for continuous wave exposures on patient data. Data from segmented DICOM images is used to form a number of staggered-grids. A perfectly matched layer suppresses spurious numerical artefacts from the boundary of the domain. A novel preconditioner is constructed which significantly accelerates the computation of the electro-magnetic field. The input into the simulation is provided as the electro-magnetic field in a homogeneous medium. This can be obtained via measurements or via an analytical model, for example by modelling the probe as a dipole source in a vacuum.\nResults: The advantage is that the scheme is fully three-dimensional, can be simulated in inhomogeneous media on large, clinically relevant, domains and can simulate multiple exposures. Having computed the electro-magnetic field for a set of material properties, strategies are presented to update electro-magnetic field as the material properties change due to temperature, dehydration or coagulation in order to reduce the duration of simulations.\nValidation is performed by comparing the predicted and measured ablated volumes, while considering the sources of uncertainty for both the numerical model, such as staircasing errors, and the experimental results, such as in the identification of the ablated regions.\nConclusion: A numerical scheme is presented which may allow patient-specific treatment planning to be performed for microwave ablation.\n","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"acbc379a52b192ccdeb32744efa9da57","permalink":"https://djps.github.io/abstracts/stm/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/abstracts/stm/","section":"abstracts","summary":"Abstract for Society of Thermal Medicine 2022 Annual Meeting","tags":["MWA","Thermal Ablation"],"title":"Patient-Specific Modelling of Microwave Ablation","type":"abstracts"},{"authors":["Santeri Kaupinmäki","Ben Cox","Simon Arridge","Christian Baker","David Sinden","Bajram Zeqiri"],"categories":null,"content":"","date":1639180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639180800,"objectID":"2d4eecaa4050db96877c9f33fbedd384","permalink":"https://djps.github.io/publication/kaupinmaki2020pus/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/kaupinmaki2020pus/","section":"publication","summary":"Numerical and experimental investigation of directional response of pyroelectric sensor","tags":["Pyroelectric Sensors","Directivity"],"title":"Pyroelectric ultrasound sensor model: directional response","type":"publication"},{"authors":["Dongwoon Hyun","Alycen Wiacek","Sobhan Goudarzi","Sven Rothlübbers","Amir Asif","Klaus Eickel","Yonina C. Eldar","Jiaqi Huang","Massimo Mischi","Hassan Rivaz","David Sinden","Ruud J. G. van Sloun","Hannah Strohm","Muyinatu A. Lediju Bell"],"categories":null,"content":"","date":1625443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625443200,"objectID":"5145f58f173b4e2a2ba6ed03150f2ea6","permalink":"https://djps.github.io/publication/hyun2021dlu/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/hyun2021dlu/","section":"publication","summary":"Overview of datasets and methods in beamforming challenge","tags":["beamforming","Imaging"],"title":"Deep learning for ultrasound image formation: CUBDL evaluation framework and open datasets","type":"publication"},{"authors":["David Sinden"],"categories":[],"content":"","date":1607385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607385600,"objectID":"4ec8d5a52accc8b38bd1dd18ad193109","permalink":"https://djps.github.io/acknowledged/gyongy2011/","publishdate":"2020-12-08T00:00:00Z","relpermalink":"/acknowledged/gyongy2011/","section":"acknowledged","summary":"Stuff on cavitation","tags":[],"title":"Characterization of cavitation based on autocorrelation of acoustic emissions","type":"acknowledged"},{"authors":["David Sinden","Srinath Rajagopal","Piero Miloro","Bajra Zeqiri"],"categories":["Acoustics Virtually Eveywhere 2020"],"content":"In many instances of therapeutic ultrasound it is preferable to produce near-field measurement-based simulations, rather than perform direct measurements. For example simulation from near-field measurements may have lower uncertainties than direct measurements at higher pressures; the entire field can be inspected and differing material properties can be modelled rather than make repeated measurements. However, to properly validate an approach there needs to be an understanding of uncertainties in data acquisition, the limitations of the governing equations and errors from the numerical methods employed. These are not independent, and are primarily determined by a combination of three factors: duration, intensity and inhomogeneity. Criteria for characterizing each factor are presented and consequent procedures outlined.\nMeasurements should consider burst length, positioning from transducer, spatial spacing, required resolution and averaging. From measurements, phase unwrapping and interpolation methods, pseudo-continuous wave approximations, as well as sparse and low-rank methods for data completion or identification of outliers, can be used to characterized the transducer as an initial source condition in an appropriate governing equation. This should consider transmission losses and reflections, frequency-dependent attenuation relations, shock-capturing and -fitting schemes, absorbing boundary conditions and the characterization of source terms for thermal simulations.\n","date":1607385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607385600,"objectID":"0b7413bd04c6d569d2c3035a12b81025","permalink":"https://djps.github.io/abstracts/asa_virtual/","publishdate":"2020-12-08T00:00:00Z","relpermalink":"/abstracts/asa_virtual/","section":"abstracts","summary":"Abstract: Acoustics Virtually Eveywhere 2020","tags":["Measurement-based simulation"],"title":"Factors for validation of measurement-based simulation","type":"abstracts"},{"authors":["David Sinden"],"categories":[],"content":"","date":1607385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607385600,"objectID":"adc22fdab789ecb62fcf270c1ca74d05","permalink":"https://djps.github.io/acknowledged/gelat2014/","publishdate":"2020-12-08T00:00:00Z","relpermalink":"/acknowledged/gelat2014/","section":"acknowledged","summary":"","tags":[],"title":"HIFU scattering by the ribs: constrained optimisation with a complex surface impedance boundary condition","type":"acknowledged"},{"authors":["David Sinden"],"categories":[],"content":"","date":1607385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607385600,"objectID":"eacee803c25f3897213d8a8670c47762","permalink":"https://djps.github.io/acknowledged/gelat2011/","publishdate":"2020-12-08T00:00:00Z","relpermalink":"/acknowledged/gelat2011/","section":"acknowledged","summary":"","tags":[],"title":"Modelling of the acoustic field of a multi-element HIFU array scattered by human ribs","type":"acknowledged"},{"authors":["David Sinden"],"categories":[],"content":"","date":1607385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607385600,"objectID":"32261caba113f23901595dae1663ef4d","permalink":"https://djps.github.io/acknowledged/baker2022/","publishdate":"2020-12-08T00:00:00Z","relpermalink":"/acknowledged/baker2022/","section":"acknowledged","summary":"","tags":[],"title":"Numerical analysis of a wave equation for lossy media obeying a frequency power law","type":"acknowledged"},{"authors":["David Sinden"],"categories":[],"content":"","date":1607385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607385600,"objectID":"a84895d8e05a2bb01a4798555b1add76","permalink":"https://djps.github.io/acknowledged/jensen2013/","publishdate":"2020-12-08T00:00:00Z","relpermalink":"/acknowledged/jensen2013/","section":"acknowledged","summary":"Cavitation and heating","tags":[],"title":"Real-time temperature estimation and monitoring of HIFU ablation through a combined modeling and passive acoustic mapping approach","type":"acknowledged"},{"authors":["Sven Rothlübbers","Hannah Strohm","Klaus Eickel","Jürgen Jenne","Vincent Kuhlen","David Sinden","Matthias Günther"],"categories":null,"content":"","date":1605571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605571200,"objectID":"808a8d33d23c3c0dffbeedbad4f313df","permalink":"https://djps.github.io/publication/rothluebbers2020iiq/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/rothluebbers2020iiq/","section":"publication","summary":"Deep learning methods for beamforming","tags":["beamforming","Imaging"],"title":"Improving image quality of single plane wave ultrasound via deep learning based channel compounding","type":"publication"},{"authors":["Nadia A. S. Smith","David Sinden","Spencer A. Thomas","Marina Romanchikova","Jessica E. Talbott","Michael Adeogun"],"categories":null,"content":"","date":1580169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580169600,"objectID":"8b01d17e59816199aef0028dd0b82957","permalink":"https://djps.github.io/publication/smith2020bcd/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/smith2020bcd/","section":"publication","summary":"Healthcare is increasingly and routinely generating large volumes of data from different sources, which are difficult to handle and integrate. Confidence in data can be established through the knowledge that the data are validated, well-curated and with minimal bias or errors. As the National Measurement Institute of the UK, the National Physical Laboratory (NPL) is running an interdisciplinary project on digital health data curation. The project addresses one of the key challenges of the UK’s Measurement Strategy, to provide confidence in the intelligent and effective use of data. A workshop was organised by NPL in which important stakeholders from NHS, industry and academia outlined the current and future challenges in healthcare data curation. This paper summarises the findings of the workshop and outlines NPL’s views on how a metrological approach to the curation of healthcare data sets could help solve some of the important and emerging challenges of utilising healthcare data.","tags":["Digital Health","Metrology"],"title":"Building confidence in digital health through metrology","type":"publication"},{"authors":["Ki Joo Pahk","Pierre Gélat","David Sinden","Dipok Kumar Dhar","Nader Saffari"],"categories":null,"content":"","date":1510876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510876800,"objectID":"29de1223289ec590094c50feedf9b621","permalink":"https://djps.github.io/publication/pahk2017nes/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/pahk2017nes/","section":"publication","summary":"The aim of boiling histotripsy is to mechanically fractionate tissue as an alternative to thermal ablation for therapeutic applications. In general, the shape of a lesion produced by boiling histotripsy is tadpole like, consisting of a head and a tail. Although many studies have demonstrated the efficacy of boiling histotripsy for fractionating solid tumors, the exact mechanisms underpinning this phenomenon are not yet well understood, particularly the interaction of a boiling vapor bubble with incoming incident shockwaves. To investigate the mechanisms involved in boiling histotripsy, a high-speed camera with a passive cavitation detection system was used to observe the dynamics of bubbles produced in optically transparent tissue-mimicking gel phantoms exposed to the field of a 2.0-MHz high-intensity focused ultrasound (HIFU) transducer. We observed that boiling bubbles were generated in a localized heated region and cavitation clouds were subsequently induced ahead of the expanding bubble. This process was repeated with HIFU pulses and eventually resulted in a tadpole-shaped lesion. A simplified numerical model describing the scattering of the incident ultrasound wave by a vapor bubble was developed to help interpret the experimental observations. Together with the numerical results, these observations suggest that the overall size of a lesion induced by boiling histotripsy is dependent on the sizes of (i) the heated region at the HIFU focus and (ii) the backscattered acoustic field by the original vapor bubble.","tags":["histotripsy"],"title":"Numerical and experimental study of mechanisms involved in boiling histotripsy","type":"publication"},{"authors":["David Sinden","Srinath Rajagopal","N. Christopher Chaggares","Guofeng Pang","Oleg Ivanytskyy"],"categories":null,"content":"","date":1509580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509580800,"objectID":"f96a83af13c99658cddf973cb2cc2fe0","permalink":"https://djps.github.io/publication/sinden2017rus/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/sinden2017rus/","section":"publication","summary":"Accurate characterization of ultrasound fields generated by diagnostic and therapeutic transducers is critical for patient safety. At high frequencies, spatial variations in the pressure over the surface of the measurement device will produce a different averaged value from that at the intended location. Using a small hydrophone as an idealised point device, this paper seeks to ascertain the spatial-averaging errors for a finite-area reference device along the beam axis in order to find the optimal measurement location.","tags":["Spatial Averaging","Hydrophones"],"title":"Reducing uncertainties for spatial averaging at high frequencies","type":"publication"},{"authors":["David Sinden","Gail ter Haar"],"categories":null,"content":"","date":1413244800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1413244800,"objectID":"afd896bc0d2bf2303ba2f9b40e6d7e1f","permalink":"https://djps.github.io/publication/sinden2014dic/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/sinden2014dic/","section":"publication","summary":"A discursive review of the fundamental requirements for ultrasound (US) treatment delivery in the context of the required dose is presented. The key components discussed are the acoustic, thermal and dose equations and their numerical solutions, the preoperative imaging modalities used, tissue characterisation, motion compensation and treatment planning strategies and how each component affects the others.","tags":[],"title":"Dosimetry implications for correct ultrasound dose deposition: uncertainties in descriptors, planning and treatment delivery","type":"publication"},{"authors":["David Sinden","Nader Saffari","Gail ter Haar"],"categories":["Conference","Abstract"],"content":"In this presentation we describe a comprehensive package of computation procedures developed for treatment planning and treatment delivery for transcostal high-intensity focused ultrasound using a phased-array.\nA number of robust computational routines designed to identify a target area, specify the position of the transducer, then perform parallelised ray-tracing from segmented CT data to ascertain the optimal position and angulation has been developed. The aim is to:\nMinimize transmission losses through the skin/water interface arising from the angle of incidence between the field from each active element and the skin interface. Minimize the propagation path in tissue. Maximize the number of available elements which may be switched on if a isobaric beam corresponding to a given pressure value does not pass through the ribs. Ensure that the transducer can be viewed by an optical tracking system, in order to register the position of the treatment head to the position defined by the treatment plan. Ensure that an imaging probe located in the central aperture of the treatment head can image the target, for treatment monitoring. Ensure that organs at risk such as lungs or bowel are not exposed to the field. The location of an optical tracker placed on the transducer head is calculated, so that the location of the treatment head location compared to the treatment plan can be verified. For ease of identifying the location of ablated regions, surface lesions can be generated automatically. From the computed position image planes which can be compared with ultrasound images are computed.\nThe appropriate phases can be calculated so that the acoustic intensity on the ribs is constrained using a boundary element method. A Rayleigh integral method, is used where the acoustic window is sufficiently large, or the tolerances of the ray-tracing algorithm sufficiently strict . For a given exposure duration, the amplitudes of each element required to give either a user-defined focal peak temperature rise or a volumetric dose as a function of the full-width half maximum are calculated. Thermal calculations are performed using a parallelised alternating-direction implicit method on a structured rectilinear grid derived from the unstructured polydata sets of the surface meshes of the skin, ribs and liver.\nAlgorithms for motion compensation are introduced in order that motion can be exploited to enlarge the volume of the lesions treated, while still delivering a specified dose to the enlarged planning treatment volume.\nThe computation routines are written in python and fortran, with bindings performed with f2py, graphical user interface from wxpython and three-dimensional visualisation using mayavi. The numpy and mkl libraries are used extensively. Patient data can be provided as DICOM images for target identification, while registered and segmented anatomical data can be provided as an .stl or .vtk dataset. Settings are outputted as .xml data in .html files for ease in a human readable format, as well as text files which can be passed directly to a phased-array drive system. The system has been designed and implemented on an in-house high-performance computing facility, and is designed to be flexible for a variety of transducer designs.\n","date":1397433600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1397433600,"objectID":"767479e93a6a4daeca0b772de33d3945","permalink":"https://djps.github.io/abstracts/vegas/","publishdate":"2014-04-14T00:00:00Z","relpermalink":"/abstracts/vegas/","section":"abstracts","summary":"Abstract for International Symposium on Therapeutic Ultrasound (ISTU), Las Vegas 2014","tags":["HIFU","Thermal Ablation","Treatment Planning"],"title":"Transcostal High-Intensity Focused Ultrasound: Planning Treatment Delivery for Phased Arrays","type":"abstracts"},{"authors":["David Sinden","John Civale","Victoria Bull","Ian Rivens","David Holroyd","Nader Saffari","Gail ter Haar"],"categories":["Conference","Abstract"],"content":" While there is a vast literature on heat deposition by high-intensity focused ultrasound, much of the work tends to focus on a single approach. This talk provides a unified analysis of the heating due to HIFU, combing experimental, analytical and numerical approaches. One of the aims of this study is to ascertain the validity of the governing equations using clinically relevant experimental data.\nSix freshly excised porcine livers were repeatedly exposed to five seconds of continuous high-intensity focused ultrasound at 1.7 MHz by a single element transducer at a variety of intensities, in a variety of locations beneath the surface of the liver, while being perfused an by a OrganOx Metra, an prototype adaptive-perfusion system. Using ultrasound imaging to identify significant objects around the focal region, exposures were either performed at as distance close as possible to a thermally significant vessel and at a distance of approximately 0.5 cm from the vessel wall. The measured vessel diameter was 4 mm and the flow within the vessel was measured using Doppler ultrasound to be approximately 9 cm/s. A fibre-optic hydrophone measured the acoustic and thermal fields at the focus, sampling at a rate of 200 Hz.\nFor a single element, unsteered transducer the Khokhlov-Zabolotskaya-Kuznetsov (KZK) equation, modelling nonlinear wave propagation through both water and soft-tissue, was coupled to the bioheat transfer equation, from which the thermal-dose is calculated. Using the range of values from tissue characterisation measurements, distributions were found which matched the spread of data for various acoustic and thermal properties. Hence empirically derived confidence intervals for various material parameters were formulated. A sensitivity analysis was performed on the governing equations to show the relative importance of each of the acoustic and thermal properties. Simulations are found to be in good agreement with experimental data away from the vessels within the range of uncertainties of tissue properties.\nResults from numerical and experiment data are presented, compared and discussed in the context of the implications for therapeutic ultrasound treatment planning, specifically for the effects of cavitation enhanced heating and the viability of treatment next to thermally significant large vessels.\n","date":1353888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1353888000,"objectID":"ed9a3a5f13c474f5efa2af4fc4e768b0","permalink":"https://djps.github.io/abstracts/npl/","publishdate":"2012-11-26T00:00:00Z","relpermalink":"/abstracts/npl/","section":"abstracts","summary":"Abstract: UK Therapeutic Interest Ultrasound Group (THUGS), National Physical Laboratory, Teddington, 2012","tags":["HIFU","Thermal Ablation","Perfusion"],"title":"A comparison between theory and experiment in thermal ablation of perfused livers","type":"abstracts"},{"authors":["David Sinden","Eleanor Stride","Nader Saffari"],"categories":null,"content":"","date":1331251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1331251200,"objectID":"a7ff15973d5c94bb36478eec3c6cb972","permalink":"https://djps.github.io/publication/sinden2012aae/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/sinden2012aae/","section":"publication","summary":"In this paper the effect of interaction on the expansion of a bubble in a regular monodisperse cluster is investigated. By a geometric construction a two-dimensional ordinary differential equation with an exact expression for first-order bubble interactions is derived for an n-bubble model. An approximate equation is derived for the rapid expansion of the bubble which can be solved yielding an analytic expression for the collapse of a bubble which undergoes inertial cavitation. It is then demonstrated that the maximum volume of a bubble in a cluster is considerably less than that of a single bubble. This result is of significance as typically the dispersion relationship, the wave speed and the co-efficient of attenuation are calculated using a single bubble model and summed for the total number of bubbles to yield the void fraction. Furthermore it is shown that the maximum radius of a bubble in the cluster is considerably smaller than that of a single bubble, yet the duration of the collapse phase is only weakly dependent on the number of bubbles. Hence, it is conjectured that the likelihood of fragmentation due to Rayleigh–Taylor instability is reduced. The results from the analysis are in good agreement with full numerical simulations of multi-bubble dynamics, as well as experimental observations","tags":[],"title":"Approximations for acoustically excited bubble cluster dynamics","type":"publication"},{"authors":["David Sinden","Gert van der Heijden"],"categories":null,"content":"","date":1311552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1311552000,"objectID":"cf3e6cdee5b2dd5473fc011c52012ea3","permalink":"https://djps.github.io/publication/sinden2011bmc/","publishdate":"2011-08-28T00:00:00Z","relpermalink":"/publication/sinden2011bmc/","section":"publication","summary":"Magneto-striction destroys integrability and leads to spatially chaotic and pulse-pulsed homoclinic solutions","tags":["Cosserat"],"title":"The buckling of magneto-strictive Cosserat rods","type":"publication"},{"authors":["Gert van der Heijden","David Sinden"],"categories":null,"content":"\r","date":1308528000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1308528000,"objectID":"0a7bc108b00b9625094555bea64dfcc7","permalink":"https://djps.github.io/publication/vanderheijden2011ltc/","publishdate":"2009-08-28T00:00:00Z","relpermalink":"/publication/vanderheijden2011ltc/","section":"publication","summary":"Using quaternions, the ten-dimensional equilibrium equations have a periodic solution but can undergo a co-dimension two Hamiltonian-Hopf-Hopf bifurctaion","tags":["Cosserat"],"title":"Localisation of a twisted conducting rod in a uniform magnetic field: the Hamiltonian-Hopf-Hopf bifurcation","type":"publication"},{"authors":["David Sinden","Eleanor Stride","Nader Saffari"],"categories":null,"content":"","date":1258934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1258934400,"objectID":"e89908d411b3e355f822e1df625e3706","permalink":"https://djps.github.io/publication/sinden2009enw/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/sinden2009enw/","section":"publication","summary":"In the context of forecasting temperature and pressure fields generated by high-intensity focussed ultrasound, the accuracy of predictive models is critical for the safety and efficacy of treatment. In such fields 'inertial' cavitation is often observed. Classically, estimations of cavitation thresholds have been based on the assumption that the incident wave at the surface of a bubble is the same as in the far-field, neglecting the effect of nonlinear wave propagation. By modelling the incident wave as a solution to Burgers' equation using weak shock theory, the effects of nonlinear wave propagation on inertial cavitation are investigated using both numerical and analytical techniques. From radius-time curves for a single bubble, it is observed that there is a reduction in the maximum size of a bubble undergoing inertial cavitation and that the inertial collapse occurs earlier in contrast with the classical case. Corresponding stability thresholds for a bubble whose initial radius is slightly below the critical Blake radius are calculated, providing a lower bound for the onset of instability. Bifurcation diagrams and frequency-response curves are presented associated with the loss of stability. The consequences and physical implications of the results are discussed with respect to the classical results.","tags":[],"title":"The effects of nonlinear wave propagation on the stability of inertial cavitation","type":"publication"},{"authors":["David Sinden"],"categories":null,"content":"","date":1245456000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1245456000,"objectID":"0f40cf545adf79432be77d9037b7794b","permalink":"https://djps.github.io/publication/thesis/","publishdate":"2009-03-31T00:00:00Z","relpermalink":"/publication/thesis/","section":"publication","summary":"A conducting rod in uniform magnetic field is super integrable, but extensibility can break the integrability and lead to spatially complex localisation.","tags":["Cosserat"],"title":"Integrability, Localisation and Bifurcation of an Elastic Conducting Rod in a Uniform Magnetic Field","type":"publication"},{"authors":["David Sinden","Gert van der Heijden"],"categories":null,"content":"A preprint can be found on the arxiv.\n","date":1245456000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1245456000,"objectID":"12840161988f7f3a8d16a0c9173bc2b3","permalink":"https://djps.github.io/publication/sinden2009sce/","publishdate":"2009-08-28T00:00:00Z","relpermalink":"/publication/sinden2009sce/","section":"publication","summary":"Extensibility destroy integrability for a conducting rod in a uniform magnetic field","tags":["Cosserat"],"title":"Spatial chaos of an extensible conducting rod in a uniform magnetic field","type":"publication"},{"authors":["David Sinden","Gert van der Heijden"],"categories":null,"content":"A preprint can be found on the arxiv.\n","date":1213920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1213920000,"objectID":"525d99267a5aa1dc638261b218be5065","permalink":"https://djps.github.io/publication/sinden2008ice/","publishdate":"2008-01-15T00:00:00Z","relpermalink":"/publication/sinden2008ice/","section":"publication","summary":"A conducting rod in uniform magnetic field is super integrable","tags":["Cosserat"],"title":"Integrability of a conducting elastic rod in a magnetic field","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8a07c3dae13f18d0c06cf5f9e482b173","permalink":"https://djps.github.io/hello/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/hello/","section":"","summary":"","tags":null,"title":"Resources_0","type":"landing"}]