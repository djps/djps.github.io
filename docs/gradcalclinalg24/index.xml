<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>MDE-MET-01: Calculus and Linear Algebra for Graduate Students | David Sinden</title><link>https://djps.github.io/docs/gradcalclinalg24/</link><atom:link href="https://djps.github.io/docs/gradcalclinalg24/index.xml" rel="self" type="application/rss+xml"/><description>MDE-MET-01: Calculus and Linear Algebra for Graduate Students</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 08 Aug 2024 00:00:00 +0000</lastBuildDate><image><url>https://djps.github.io/docs/gradcalclinalg24/featured.jpg</url><title>MDE-MET-01: Calculus and Linear Algebra for Graduate Students</title><link>https://djps.github.io/docs/gradcalclinalg24/</link></image><item><title>MDE-MET-01: Calculus and Linear Algebra for Graduate Students</title><link>https://djps.github.io/docs/gradcalclinalg24/intro/intro/</link><pubDate>Thu, 08 Aug 2024 00:00:00 +0000</pubDate><guid>https://djps.github.io/docs/gradcalclinalg24/intro/intro/</guid><description>&lt;p&gt;This site will be the primary source of information for this course.&lt;/p&gt;
&lt;div class="callout flex px-4 py-3 mb-6 rounded-md border-l-4 bg-blue-100 dark:bg-blue-900 border-blue-500"
data-callout="note"
data-callout-metadata=""&gt;
&lt;span class="callout-icon pr-3 pt-1 text-blue-600 dark:text-blue-300"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m16.862 4.487l1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L6.832 19.82a4.5 4.5 0 0 1-1.897 1.13l-2.685.8l.8-2.685a4.5 4.5 0 0 1 1.13-1.897zm0 0L19.5 7.125"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;div class="callout-content dark:text-neutral-300"&gt;
&lt;div class="callout-title font-semibold mb-1"&gt;Note&lt;/div&gt;
&lt;div class="callout-body"&gt;&lt;p&gt;This course was delivered in Autumn 2024.&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="lectures"&gt;Lectures&lt;/h2&gt;
&lt;p&gt;Lectures will take place in autumn 2024, on Wednesday mornings 9:45-11:00 and 11:15-12:30 at &lt;del&gt;EH-4 (East Hall Classroom 4)&lt;/del&gt; IRC Seminar Room III.&lt;/p&gt;
&lt;p&gt;An office hour can be arranged by appointment.&lt;/p&gt;
&lt;p&gt;Course notes can be found
.&lt;/p&gt;
&lt;h2 id="recommendations-for-preparation"&gt;Recommendations for Preparation&lt;/h2&gt;
&lt;p&gt;There are no recommendations beyond reading the syllabus and the outline of the course.&lt;/p&gt;
&lt;h2 id="content-and-educational-aims"&gt;Content and Educational Aims&lt;/h2&gt;
&lt;p&gt;This module offers a highly structured introduction to the fundamentals of two major pillars of mathematical modelling and analysis: single and multivariable calculus on the one hand and linear algebra on the other.&lt;/p&gt;
&lt;p&gt;It is a gateway for graduate students who have not been exposed to the topics so far, or who would benefit from a refresher.&lt;/p&gt;
&lt;p&gt;This course will focus on practical experience rather than on mathematical rigour.&lt;/p&gt;
&lt;h2 id="intended-learning-outcomes"&gt;Intended Learning Outcomes&lt;/h2&gt;
&lt;p&gt;Upon completion of this module, students will be able to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Apply the fundamental concepts of calculus and linear algebra in structured situations&lt;/li&gt;
&lt;li&gt;Understand and use vectors and matrices, calculate determinants, eigenvalues and eigenvectors in simple cases&lt;/li&gt;
&lt;li&gt;Calculate derivatives and simple integrals&lt;/li&gt;
&lt;li&gt;Explain the importance of the methods of calculus and linear algebra in problems arising from applications&lt;/li&gt;
&lt;li&gt;Understand the methods of calculus and linear algebra used in more advanced modules as well as in scientific literature&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The informal aim is to build intuition of some of the fundamental concepts in modern mathematics which are used in machine learning.&lt;/p&gt;
&lt;h2 id="indicative-literature"&gt;Indicative Literature&lt;/h2&gt;
&lt;p&gt;There is no primary or required course book, but there are many suitable books, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;&lt;em&gt;Introduction to Linear Algebra&lt;/em&gt;&amp;rdquo; - G. Strang (2016) Wellesley-Cambridge Press, 5&lt;sup&gt;th&lt;/sup&gt; edition ISBN: &lt;span class="font-mono"&gt;978-09802327-7-6&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other useful resources are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;&lt;em&gt;Linear Algebra and Learning from Data&lt;/em&gt;&amp;rdquo; - G. Strang (2019) Wellesley-Cambridge Press, ISBN: &lt;span class="font-mono"&gt;978-06921963-8-0&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;&lt;em&gt;Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares&lt;/em&gt;&amp;rdquo; - S. Boyd and L. Vandenberghe (2018) Cambridge University Press, ISBN: &lt;span class="font-mono"&gt;978-1316518960&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Knowledge, Abilities, or Skills
* Knowledge of Calculus: functions, inverse functions, sets, real numbers, sequences and limits, polynomials, rational functions, trigonometric functions, logarithm and exponential function, parametric equations, tangent lines, graphs, derivatives, anti-derivatives, elementary techniques for solving equations
* Knowledge of Linear Algebra: vectors, matrices, lines, planes, $n$-dimensional Euclidean vector space, rotation, translation, dot product (scalar product), cross product, normal vectors, eigenvalues, eigenvectors, elementary techniques for solving systems of linear equations
* Some examples will be presented as python notebooks, but no knowledge of python is required, nor will there be any assessment of ability to code.
* Some examples will be presented as [python notebooks](/docs/gradcalclinalg24/notebooks/), but no knowledge of python is required, nor will there be any assessment of ability to code.
--&gt;
&lt;h2 id="usability-and-relationship-to-other-modules"&gt;Usability and Relationship to other Modules&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The module is a non-mandatory remedial module of the Data Engineering MSc and an elective module Data Science for Society and Business MSc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This module introduces and refreshes the essential calculus and linear algebra required in most of the modules of the data engineering program.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There is a placement test offered in the orientation week before the start of the first semester to help all students to find out if they need to take this course.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="course-outline"&gt;Course Outline&lt;/h2&gt;
&lt;p&gt;The following topics will be covered:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vectors: addition of vectors and multiplication by a scalar, linear combinations, lengths and dot products&lt;/li&gt;
&lt;li&gt;Matrices&lt;/li&gt;
&lt;li&gt;Vector spaces, subspaces, column spaces, span, linear independence, basis. The nullspace of a matrix&lt;/li&gt;
&lt;li&gt;Basis of the nullspace and the column space via Gaussian elimination. Free columns and pivot columns. Solutions of systems of linear equations&lt;/li&gt;
&lt;li&gt;Rank of a matrix. Sum of the dimensions of the nullspace and the column space. Multiplication of matrices&lt;/li&gt;
&lt;li&gt;Inverse of a matrix. Finding the inverse using Gaussian elimination&lt;/li&gt;
&lt;li&gt;Orthogonality. Orthonormal bases, projections, and Gram-Schmidt&lt;/li&gt;
&lt;li&gt;Determinants&lt;/li&gt;
&lt;li&gt;Eigenvalues and eigenvectors&lt;/li&gt;
&lt;li&gt;Diagonalization of a matrix&lt;/li&gt;
&lt;li&gt;Symmetric matrices.&lt;/li&gt;
&lt;li&gt;Principal Component Analysis&lt;/li&gt;
&lt;li&gt;Derivatives, tangent lines, higher order derivatives, curve sketching.&lt;/li&gt;
&lt;li&gt;Taylor series&lt;/li&gt;
&lt;li&gt;Functions of several variables. Partial derivatives&lt;/li&gt;
&lt;li&gt;Optimization problems. Positive/negative definite matrices&lt;/li&gt;
&lt;li&gt;Jacobians and Hessians&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="itinerary"&gt;Itinerary&lt;/h3&gt;
&lt;!-- todayMarker stroke-width:3px, stroke:#164e63, opacity:0.5 --&gt;
&lt;p&gt;Provisionally, the course will run as follows&lt;/p&gt;
&lt;div class="mermaid"&gt;gantt
dateFormat MM-DD
axisFormat %e-%b
todayMarker off
title Provisional Schedule 2024
section Linear Algebra
Vectors :a1a, 09-02, 2w
Assignment 1 : milestone, done, m1, 09-18, 0w
Matrices :a1b, 09-16, 1w
Linear equations :a1c, 09-23, 1w
Assignment 2 : milestone, done, m2, 10-02, 0w
Vector Spaces :a1d, 09-30, 1w
Break :crit, 10-07, 1w
Vector Spaces :a1e, 10-14, 1w
Assignment 3 : milestone, m3, 10-21, 1d
Matrices &amp; Eigenvalues :a1f, 10-21, 3w
Assignment 4 : milestone, active, m4, 11-01, 0d
Assignment 5 : milestone, active, m5, 11-12, 0w
section Calculus
Single Variables :a2c, 11-11, 1w
Taylor Series :a2d, 11-18, 1w
Multiple Variables :a2e, 11-25, 1w
Assignment 6 : milestone, active, m6, 11-23, 0w
section Summary
Summary :active, a3a, 12-02, 1w
&lt;/div&gt;
&lt;h3 id="structure"&gt;Structure&lt;/h3&gt;
&lt;p&gt;The course content has the following structure:&lt;/p&gt;
&lt;div class="markmap" style="height: 500px;"&gt;
&lt;pre&gt;---
markmap:
zoom: false
pan: false
---
- Linear Algebra
- [Vectors](/docs/gradcalclinalg24/part1/vectors/)
- [Matrices](/docs/gradcalclinalg24/part1/matrices/)
- [Linear Equations](/docs/gradcalclinalg24/part2/linear-equations/)
- [Vector Spaces](/docs/gradcalclinalg24/part1/spaces/)
- [Matrices &amp; Eigenvalues](/docs/gradcalclinalg24/part1/eigen/)
- Calculus
- [Single Variable Calculus](/docs/gradcalclinalg24/part2/one-variable/)
- [Taylor Series](/docs/gradcalclinalg24/part2/taylor/)
- [Many Variable Calculus](/docs/gradcalclinalg24/part2/many-variables/)&lt;/pre&gt;
&lt;/div&gt;
&lt;h2 id="assessment"&gt;Assessment&lt;/h2&gt;
&lt;h3 id="examination-type"&gt;Examination Type&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Examination Type:&lt;/td&gt;
&lt;td&gt;Module examination&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Assessment Type:&lt;/td&gt;
&lt;td&gt;Written examination&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Scope:&lt;/td&gt;
&lt;td&gt;All intended learning outcomes of this module&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Duration:&lt;/td&gt;
&lt;td&gt;120 min&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Weight:&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You have three attempts to pass the module. Once you pass the module, no further retakes of the exam are possible. (See
for more details.) The exams in this course are offered twice per year: in December and January.&lt;/p&gt;
&lt;p&gt;No supplementary material can be brought to the exam. A calculator is necessary. Graphical and scientific calculators are permitted.&lt;/p&gt;
&lt;p&gt;The pass mark is 45%.&lt;/p&gt;
&lt;h3 id="assignments"&gt;Assignments&lt;/h3&gt;
&lt;p&gt;By submitting homework assignments, via &lt;em&gt;Teams&lt;/em&gt; as a single pdf, you can improve this grade by up to 0.66 points, as bonus achievements.&lt;/p&gt;
&lt;p&gt;Homework submission is voluntary although highly recommended. It is possible to get a 100% final grade without submitting homework or participating in quizzes.&lt;/p&gt;
&lt;p&gt;Homework will be assigned every two weeks. Homework assignments are posted on
and on Teams approximately ten days before the due date.&lt;/p&gt;
&lt;p&gt;You are encouraged to discuss homework between each other. However, the submitted assignments should be written individually. No copying is allowed!&lt;/p&gt;
&lt;p&gt;The two lowest homework scores will be discarded before the final homework score is calculated. This rule covers short illness, excursions, late joining of the course, and similar situations.&lt;/p&gt;
&lt;p&gt;Note that each homework assignment carries equal marks.&lt;/p&gt;
&lt;p&gt;The problems on the final exam will be similar to the ones from homework. So, by doing homework you prepare for the final exam - maths is not a spectator sport.&lt;/p&gt;
&lt;h2 id="academic-integrity"&gt;Academic Integrity&lt;/h2&gt;
&lt;p&gt;All involved parties (lecturers, instructors and students) are expected to abide by the word and spirit of the
. Violations of the Code should be brought to the attention of the Academic Integrity Committee.&lt;/p&gt;
&lt;h2 id="artifical-intelligence-use-policy"&gt;Artifical Intelligence Use Policy&lt;/h2&gt;
&lt;p&gt;This policy covers any generative AI tool, such as ChatGPT, Elicit, etc. This includes text, slides, artwork/graphics/video/audio and other products.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You are discouraged from using AI tools &lt;em&gt;unless&lt;/em&gt; under direct instruction from your instructor to do so. Please contact your instructor if you are unsure or have questions &lt;em&gt;before&lt;/em&gt; using AI for any assignment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note that the material generated by these programs may be inaccurate, incomplete, or otherwise problematic. Their use may also stifle your own independent thinking and creativity. Accordingly, reduction in the grade is likely when using AI.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If any part of this AI policy is confusing or uncertain, please reach out to your instructor for a conversation before submitting your work.&lt;/p&gt;</description></item><item><title>Vectors</title><link>https://djps.github.io/docs/gradcalclinalg24/part1/vectors/</link><pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate><guid>https://djps.github.io/docs/gradcalclinalg24/part1/vectors/</guid><description>&lt;h2 id="notation"&gt;Notation&lt;/h2&gt;
&lt;p&gt;The set of all real numbers is denoted as $\mathbb{R}$ and the set of all vectors with $N$ coordinates, whose entries are real numbers, is denoted by $\mathbb{R}^N$.&lt;/p&gt;
&lt;p&gt;Vectors are written as by $\boldsymbol{v}$ or $\vec{v}$.&lt;/p&gt;
&lt;p&gt;The vector whose every entry is zero is called a zero vector.&lt;/p&gt;
&lt;p&gt;But note that if $\boldsymbol{v} = \boldsymbol{0} \in \mathbb{R}^2$ and $\boldsymbol{u} = \boldsymbol{0} \in \mathbb{R}^3$, then $\boldsymbol{v} \neq \boldsymbol{u}$, nor can arithmetic operations, such as addition or subtraction be performed.&lt;/p&gt;
&lt;!--
&lt;div id="chart-936185247" class="chart"&gt;&lt;/div&gt;
&lt;script&gt;
async function fetchChartJSON() {
console.debug('Hugo Blox fetching chart JSON...')
const response = await fetch('\/plotly\/test.json');
return await response.json();
}
(function() {
let a = setInterval( function() {
if ( typeof window.Plotly === 'undefined' ) {
console.debug('Plotly not loaded yet...')
return;
}
clearInterval( a );
fetchChartJSON().then(chart =&gt; {
console.debug('Plotting chart...')
window.Plotly.newPlot('chart-936185247', chart.data, chart.layout, {responsive: true});
});
}, 500 );
})();
&lt;/script&gt;
&lt;figure&gt;&lt;img src="https://djps.github.io/img/test.svg"&gt;
&lt;/figure&gt;
--&gt;
&lt;p&gt;
&lt;figure &gt;
&lt;div class="flex justify-center "&gt;
&lt;div class="w-full" &gt;&lt;img src="https://djps.github.io/img/vector_addition.png" alt="png" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Linear Combinations&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;Given some $k$ vectors $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k \in \mathbb{R}^{n}$, then their &lt;strong&gt;linear combination&lt;/strong&gt; is an expression of the form
&lt;/p&gt;
$$
\begin{equation*}
a_1 \vec{v}_1 + a_2 \vec{v}_2 + \ldots + a_k \vec{v}_k
\end{equation*}
$$&lt;p&gt;
where $a_1, a_2, \ldots, a_k \in \mathbb{R}$, i.e. are scalars.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Dot Product&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;For two vectors $\boldsymbol{a}$, $\boldsymbol{b} \in \mathbb{R}^{n}$, where $\boldsymbol{a}= \left(a_1, a_2, \ldots, a_n \right)$, the &lt;strong&gt;dot product&lt;/strong&gt; of the two vectors is given by
&lt;/p&gt;
$$
\begin{equation*}
\boldsymbol{a} \cdot \boldsymbol{b} = a_1 b_1 + a_2 b_2 + \ldots a_n b_n.
\end{equation*}
$$&lt;p&gt;
The result is a scalar.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The dot product can be generalised for more general vectors, called an &lt;strong&gt;inner product&lt;/strong&gt;, written as $\langle \cdot, \cdot \rangle=c$, where $c$ is scalar.&lt;/p&gt;
&lt;p&gt;The length of a vector is denoted by $\left| \vec{v} \right| = \sqrt{\vec{v} \cdot \vec{v}}$.&lt;/p&gt;
&lt;p&gt;A unit vector has length one. A non-zero vector can be transformed into a unit vector by the transformation $\vec{v} \mapsto \vec{v} / \left| \vec{v} \right|$. This process is often called normalization, as the norm of a vector is often defined as $\sqrt{\langle \vec{v}, \vec{v} \rangle}$.&lt;/p&gt;
&lt;p&gt;One can show that, for vectors $\boldsymbol{a}, \boldsymbol{b} \in\mathbb{R}^2$, then
&lt;/p&gt;
$$
\begin{equation*}
\boldsymbol{a} \cdot \boldsymbol{b} = \left| \boldsymbol{a} \right| \left| \boldsymbol{b} \right| \cos\theta.
\end{equation*}
$$&lt;p&gt;
Thus, any two vectors, $\vec{v}$ and $\vec{u}$ are said to be &lt;strong&gt;orthogonal&lt;/strong&gt; (or perpendicular) if $\vec{v} \cdot \vec{u}=0$.&lt;/p&gt;
&lt;p&gt;Thus, the angle between two vectors is given by
&lt;/p&gt;
$$
\begin{equation*}
\cos\theta = \dfrac{\boldsymbol{a} \cdot \boldsymbol{b}}{\left| \boldsymbol{a} \right| \left| \boldsymbol{b} \right|}.
\end{equation*}
$$&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Cauchy Schwarz Inequality&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;The Cauchy-Schwarz inequality states that the magnitude of the between two vectors is always less than or equal to the product of the magnitudes of the two vectors, i.e.
&lt;/p&gt;
$$
\left| \vec{v} \cdot \vec{w} \right| \leq \left| \vec{v} \right| \left| \vec{w} \right|.
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Triangle Inequality&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;The triangle inequality states that the sum of the lengths of any two sides must be greater than or equal to the length of the remaining side. Formally, this is expressed as
&lt;/p&gt;
$$
\left| \vec{v} + \vec{w} \right| \leq \left| \vec{v} \right| + \left| \vec{w} \right|.
$$&lt;p&gt;
It can be derived from the Cauchy-Schwarz inequality.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;For the Cauchy-Schwarz inequality, when the vectors $\vec{v}$ and $\vec{w}$ lie on the same line, then $\left| \vec{v} \cdot \vec{w} \right| = \left| \vec{v} \right| \left| \vec{w} \right|$.&lt;/p&gt;
&lt;p&gt;For the triangle inequality, when the vectors point in the same direction, then the $\left| \vec{v} + \vec{w} \right| = \left| \vec{v} \right| + \left| \vec{w} \right|$.&lt;/p&gt;</description></item><item><title>Matrices</title><link>https://djps.github.io/docs/gradcalclinalg24/part1/matrices/</link><pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate><guid>https://djps.github.io/docs/gradcalclinalg24/part1/matrices/</guid><description>&lt;p&gt;A matrix is a rectangular array of numbers.&lt;/p&gt;
&lt;p&gt;Matrices are usually denoted by uppercase letters.&lt;/p&gt;
&lt;p&gt;A matrix, $A$, with $m$ rows and ${n}$ columns, is referred to as an ${m\times n}$ matrix.&lt;/p&gt;
&lt;p&gt;If all the entries are real numbers, the matrix is a member of the set of all real matrices with $m$ rows and ${n}$ columns, i.e. $A \in\mathbb{R}^{m \times n}$.&lt;/p&gt;
&lt;p&gt;The entry in the $i$&lt;sup&gt;th&lt;/sup&gt; row and $j$&lt;sup&gt;th&lt;/sup&gt; column of the matrix $A \in \mathrm{R}^{m \times n}$ is denoted by $a_{ij}$ or $a_{i,j}$. Hence, the matrix is written as&lt;/p&gt;
$$
\begin{align*}
A = \left(
\begin{array}{cccc}
a_{1,1} &amp; a_{1,2} &amp; \cdots &amp; a_{1,n} \\
a_{2,1} &amp; a_{2,2} &amp; \cdots &amp; a_{2,n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m,1} &amp; a_{m,2} &amp; \cdots &amp; a_{m,n}
\end{array}
\right).
\end{align*}
$$&lt;p&gt;A matrix is said to be a &lt;strong&gt;square matrix&lt;/strong&gt; if the number of rows is equal to the number of columns, i.e. $n=m$.&lt;/p&gt;
&lt;h2 id="matrix-operations"&gt;Matrix Operations&lt;/h2&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Matrix Matrix Multiplication&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;For two matrices $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{n \times l}$, the matrix product $C=AB$ is a $m \times l$ matrix whose entries are given by
&lt;/p&gt;
$$
c_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj}
$$&lt;p&gt;
for any $i=1, \ldots, m$ and $j =1, \ldots, l$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;If $A$ is an $m\times n$ matrix and $B$ is a $n\times l$ matrix, then $A$ has the same number of columns as $B$ has rows, i.e. $n$. Thus, each entry $c_{ij}$ is the dot product of the $i$&lt;sup&gt;th&lt;/sup&gt; row of $A$ with the $j$&lt;sup&gt;th&lt;/sup&gt; column of $B$, which both have length $n$.&lt;/p&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Matrix Vector Multiplication&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;For a matrix $A \in \mathbb{R}^{m \times n}$ and vector $\boldsymbol{x} \in \mathbb{R}^{n}$, the matrix-vector product $A\boldsymbol{x} =\boldsymbol{y}$ is an $m \times l$ vector whose entries are given by
&lt;/p&gt;
$$
y_{j} = \sum_{i=1}^{n} a_{ij} x_i .
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Row and Column Vectors&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
When matrix $A \in \mathbb{R}^{m \times 1}$ is comprised of a single column of $m$ entries, it is called a column vector. Similarly, a $1 \times n$ matrix, i.e. a single row of $n$ entries is called a row vector.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Thus $\boldsymbol{v}^T \boldsymbol{v}$ is a $1 \times 1$ matrix, and $\boldsymbol{v} \boldsymbol{v}^T$ is a $n \times n$ matrix.&lt;/p&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Transpose&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
For a matrix $A \in \mathbb{R}^{m \times n}$, the transpose of the matrix $A^T \in \mathbb{R}^{n \times m}$ where all elements are mapped to $a_{ij} = a_{ji}$.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Thus, if $\boldsymbol{x}$ is a column vector it is possible to perform $A\boldsymbol{x}=\boldsymbol{y}$, then the equivalent multiplication by a row vector is $\boldsymbol{x}^T A^T = \boldsymbol{y}^T$.&lt;/p&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Trace&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;For a square matrix $A \in \mathbb{R}^{n \times n}$, the trace is defined as the sum of the elements on the main diagonal.
&lt;/p&gt;
$$
\text{tr}\left(A \right) = \sum_{i=0}^{n} a_{ii}.
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id="properties-of-matrix-operations"&gt;Properties of Matrix Operations&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;$A+B = B+A$&lt;/li&gt;
&lt;li&gt;$c\left(A + B\right) = cA + cB$&lt;/li&gt;
&lt;li&gt;$C\left(A + B\right) = CA + CB$&lt;/li&gt;
&lt;li&gt;$\left(A + B\right)C = AC + BC$&lt;/li&gt;
&lt;li&gt;$AB \neq BA$&lt;/li&gt;
&lt;li&gt;$A + \left(B + C\right) = \left(A+ B\right) + C$&lt;/li&gt;
&lt;li&gt;$A\left( BC \right) = \left(AB\right)C$&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="details admonition Proposition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m16.862 4.487l1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L10.582 16.07a4.5 4.5 0 0 1-1.897 1.13L6 18l.8-2.685a4.5 4.5 0 0 1 1.13-1.897zm0 0L19.5 7.125M18 14v4.75A2.25 2.25 0 0 1 15.75 21H5.25A2.25 2.25 0 0 1 3 18.75V8.25A2.25 2.25 0 0 1 5.25 6H10"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Proposition: &lt;em&gt;&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
$\left( A B \right)^T = B^T A^T$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="useful-types-of-matrices"&gt;Useful Types of Matrices&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;identity matrix&lt;/strong&gt; is the square matrix $I$ such that $AI=A$ and $IA=A$, all entries on the main diagonal are one, all others are zero.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;diagonal matrix&lt;/strong&gt; is a matrix where all entries outside the main diagonal are all zero, i.e. $a_{ij} =0$ when $i \neq j$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;banded&lt;/strong&gt; matrix is a matrix whose non-zero entries are confined to a diagonal band, comprising the main diagonal and zero or more diagonals on either side.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;inverse matrix&lt;/strong&gt; of a square matrix is the matrix $A^{-1}$ such that $AA^{-1}~=~A^{-1}A~=~I.$ A matrix is &lt;strong&gt;non-singular&lt;/strong&gt; or &lt;strong&gt;invertible&lt;/strong&gt; if there exists an inverse matrix exists.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A square matrix has an inverse if and only if the rank of the matrix is equal to the number of columns (or rows), equivalently, the columns are linearly independent. Such matrices are said to have full rank.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A square matrix $A$ is &lt;strong&gt;symmetric&lt;/strong&gt; if ${A=A^T}$, that is, $a_{i,j}=a_{j,i}$ for all indices $i$ and $j$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A square matrix, with complex elements, is said to be &lt;strong&gt;Hermitian&lt;/strong&gt; if the matrix is equal to its &lt;em&gt;conjugate transpose&lt;/em&gt;, i.e. $a_{i,j}=\overline{a_{j,i}}$ for all indices $i$ and $j$. A Hermitian matrix is written as $A=A^H$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An &lt;strong&gt;orthogonal matrix&lt;/strong&gt; $Q$ is a matrix whose columns $\vec{q}_i$ are orthogonal to one another, that is $\vec{q}_i \cdot \vec{q}_j = 0$ for $i \neq j$ and have unit length, i.e. $\left\| \vec{q}_i \right\| = 1$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="details admonition Proposition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m16.862 4.487l1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L10.582 16.07a4.5 4.5 0 0 1-1.897 1.13L6 18l.8-2.685a4.5 4.5 0 0 1 1.13-1.897zm0 0L19.5 7.125M18 14v4.75A2.25 2.25 0 0 1 15.75 21H5.25A2.25 2.25 0 0 1 3 18.75V8.25A2.25 2.25 0 0 1 5.25 6H10"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Proposition: &lt;em&gt;&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
The inverse of an orthogonal matrix $Q$ is equal to its transpose, i.e. $Q^{-1} = Q^T$.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;Markov&lt;/strong&gt; matrix has positive entries and every column sums to one.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="details admonition Proposition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m16.862 4.487l1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L10.582 16.07a4.5 4.5 0 0 1-1.897 1.13L6 18l.8-2.685a4.5 4.5 0 0 1 1.13-1.897zm0 0L19.5 7.125M18 14v4.75A2.25 2.25 0 0 1 15.75 21H5.25A2.25 2.25 0 0 1 3 18.75V8.25A2.25 2.25 0 0 1 5.25 6H10"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Proposition: &lt;em&gt;&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
The largest &lt;a href="https://djps.github.io/docs/gradcalclinalg24/part1/eigen/#eigenvalues--eigenvectors)"&gt;eigenvalue&lt;/a&gt; of a Markov matrix is one.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;permutation matrix&lt;/strong&gt; is a square matrix that has exactly one entry of $1$ in each row and each column and all other entries $0$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Rotation matrices&lt;/strong&gt; describe rotations by an angle about the axes of a coordinate system. They can be denoted by $R_x ( \alpha )$, i.e. rotation by angle $\theta$ about the $x$ axis. The product of two rotation matrices is also a rotation matrix. Note that in general $R_x\left( \theta \right) R_y \left( \beta \right) \neq R_y \left( \beta \right) R_x\left( \theta \right)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reflection matrices&lt;/strong&gt;: $R = I - 2 \boldsymbol{u} \boldsymbol{u}^T$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A square matrix is said to be &lt;strong&gt;lower triangular matrix&lt;/strong&gt; if all the elements above the main diagonal are zero, i.e. $a_{ij}=0$ when $i &lt; j$. Similarly, a matrix is said to be &lt;strong&gt;upper triangular&lt;/strong&gt; if all the entries below the main diagonal are zero, that is $a_{ij}=0$ when $i~&gt;~j$. A matrix is said to be strictly upper triangular (or strictly lower triangle) if the main diagonal is also zero.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="determinant-of-a-matrix"&gt;Determinant of a Matrix&lt;/h2&gt;
&lt;p&gt;For a 2$\times$2 matrix &lt;/p&gt;
$$A= \left( \begin{array}{cc} a &amp; b \\ c &amp; d \end{array} \right)$$&lt;p&gt;, the determinant, denoted by $\left| A \right|$ or det$A$ is given by ${ad-bc}$.&lt;/p&gt;
&lt;p&gt;There are many methods to compute the determinant. the&lt;/p&gt;
&lt;p&gt;Leibniz Method: Consider all possible choices of $n$ elements from a matrix such that there is precisely one element chosen from each row and column. Let such a choice be denoted by $\sigma$ and let sgn$\sigma$ be $-1$ to the power of the number of row swaps required to turn the choice $\sigma$ into the diagonal. Then det$\sigma$ is the sum of the products of each $\sigma$ multiplied by its sign.&lt;/p&gt;
&lt;p&gt;Laplace expansion: delete the $i$&lt;sup&gt;th&lt;/sup&gt; row and the $j$&lt;sup&gt;th&lt;/sup&gt; column from a matrix to yield a $(n-1)\times(n-1)$ matrix, called a minor, denoted by $M_{ij}$, then the cofactor is
&lt;/p&gt;
$$
C_{ij} = \left( -1 \right)^{i+j} \text{det} M_{ij}
$$&lt;p&gt;&lt;br&gt;
and the determinant is given by
&lt;/p&gt;
$$
\text{det}{A} = a_{i1}C_{i1} + a_{i2}C_{i2} + \ldots + a_{in}C_{in}
$$&lt;p&gt;
which is the cofactor expansion along row $i$. The computation of the determinant can be performed for any row. The cofactor expansion can also be performed along any column, i.e.
&lt;/p&gt;
$$
\text{det}{A} = a_{1j}C_{1j} + a_{2j}C_{2j} + \ldots + a_{nj}C_{nj}.
$$&lt;p&gt;Cramers Rule: for $A\boldsymbol{x}=\boldsymbol{b}$, define the matrix $B_j$ as the matrix $A$ with the $j$&lt;sup&gt;th&lt;/sup&gt; column replaced by $\boldsymbol{b}$, then, if det$A \neq 0$,
&lt;/p&gt;
$$
x_j = \dfrac{\text{det} B_j}{\text{det} A}.
$$&lt;p&gt;The inverse of a matrix can be found via
&lt;/p&gt;
$$
\left( A^{-1} \right)_{ij} = \dfrac{ C_{ji} }{\text{det} A}.
$$&lt;h2 id="properties-of-nonsingular-matrices"&gt;Properties of Nonsingular Matrices&lt;/h2&gt;
&lt;p&gt;For a nonsingular matrix, the following &lt;em&gt;all&lt;/em&gt; hold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A nonsingular matrices has full rank, i.e. the rank is equal to the number of rows or columns&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A square matrix is nonsingular if and only if the determinant of the matrix is non-zero.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If a matrix is singular, both versions of Gaussian elimination (with and without pivoting) will fail due to division by zero, yielding a floating exception error. Another way to understand this is that the number of pivots is equal to the rank, so if the matrix does not have full rank, so there will not be enough pivots in order to transform the matrix in to row-echelon form.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Linear Equations</title><link>https://djps.github.io/docs/gradcalclinalg24/part1/linear-equations/</link><pubDate>Mon, 20 Nov 2023 00:00:00 +0000</pubDate><guid>https://djps.github.io/docs/gradcalclinalg24/part1/linear-equations/</guid><description>&lt;h2 id="linear-equations"&gt;Linear Equations&lt;/h2&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Systems of Linear Equations&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;A system of linear equations (or linear system) is a collection of one or more linear equations involving the same variables. If there are $m$ equations with $n$ unknown variables to solve for&lt;/p&gt;
$$
\begin{align*}
a_{1,1} x_1 + a_{1,2} x_2 + \ldots + a_{1,n} x_n &amp; = b_1 \\
a_{2,1} x_1 + a_{2,2} x_2 + \cdots + a_{2,n} x_n &amp; = b_2 \\
\vdots &amp; \\
a_{m,1} x_1 + a_{m,2} x_2 + \cdots + a_{m,n} x_n &amp; = b_m.
\end{align*}
$$&lt;p&gt;The system of linear equations can be written in matrix form ${A\boldsymbol{x}=\boldsymbol{b}}$, where&lt;/p&gt;
$$
\begin{align*}
A = \left(
\begin{array}{cccc}
a_{1,1} &amp; a_{1,2} &amp; \cdots &amp; a_{1,n} \\
a_{2,1} &amp; a_{2,2} &amp; \cdots &amp; a_{2,n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m,1} &amp; a_{m,2} &amp; \cdots &amp; a_{m,n}
\end{array}
\right), \quad x = \left(
\begin{array}{c}
x_1 \\\
x_2 \\\
\vdots \\\
x_n
\end{array}
\right), \quad b = \left(
\begin{array}{c}
b_1 \\
b_2 \\
\vdots \\
b_m
\end{array}
\right),
\end{align*}
$$&lt;p&gt;
so that $A \in \mathbb{R}^{m \times n}$, $\boldsymbol{x}= \in \mathbb{R}^n$ and $\boldsymbol{b}= \in \mathbb{R}^m$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!--
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
If $\tilde{x}$ is an approximate solution to the linear problem ${Ax=b}$, then the &lt;strong&gt;residual&lt;/strong&gt; is defined as ${r = A \tilde{x}-b}$.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
--&gt;
&lt;h3 id="direct-methods"&gt;Direct Methods&lt;/h3&gt;
&lt;div class="details admonition Algorithm open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Algorithm: &lt;em&gt;Gaussian Elimination&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;Gaussian elimination is a method to solve systems of linear equations based on forward elimination (a series of row-wise operations) to convert the matrix, $A$, to upper triangular form (echelon form), and then back substitution to solve the system. The row operations are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;row swapping&lt;/li&gt;
&lt;li&gt;row scaling, i.e. multiplying by a non-zero scalar&lt;/li&gt;
&lt;li&gt;row addition, i.e. adding a multiple of one row to another&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Forward elimination is written as&lt;/p&gt;
&lt;p&gt;&lt;small&gt;1 &lt;/small&gt; For $k=1$ to $n-1$: &lt;br&gt;
&lt;small&gt;2 &lt;/small&gt; &lt;span style="margin-left: 10%;"&gt; For $i = k + 1$ to $n$: &lt;/span&gt;&lt;br&gt;
&lt;small&gt;3 &lt;/small&gt; &lt;span style="margin-left: 20%;"&gt; For $j = k$ to $n$: &lt;/span&gt;&lt;br&gt;
&lt;small&gt;4 &lt;/small&gt; &lt;span style="margin-left: 30%;"&gt; $a_{i,j} = a_{i,j} - \dfrac{a_{i,k}}{a_{k,k}} a_{k,j}$ &lt;/span&gt;&lt;br&gt;
&lt;small&gt;5 &lt;/small&gt; &lt;span style="margin-left: 20%;"&gt; End &lt;/span&gt;&lt;br&gt;
&lt;small&gt;6 &lt;/small&gt; &lt;span style="margin-left: 10%;"&gt; $b_{i} = b_{i} - \dfrac{a_{i,k}}{a_{k,k}} b_{k}$ &lt;/span&gt;&lt;br&gt;
&lt;small&gt;7 &lt;/small&gt; End &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Then back substitute, starting with the last unknown first:&lt;/p&gt;
&lt;p&gt;&lt;small&gt;8 &lt;/small&gt; Set $x_n = \dfrac{b_n}{a_{n,n}}$ &lt;br&gt;
&lt;small&gt;9 &lt;/small&gt; For $i$ =$n-1$ to $1$: &lt;br&gt;
&lt;small&gt;10&lt;/small&gt; &lt;span style="margin-left: 10%;"&gt; $y = b_i$ &lt;/span&gt;&lt;br&gt;
&lt;small&gt;11&lt;/small&gt; &lt;span style="margin-left: 10%;"&gt; For $j = n$ to $i+1$: &lt;/span&gt;&lt;br&gt;
&lt;small&gt;12&lt;/small&gt; &lt;span style="margin-left: 20%;"&gt; $y = y - a_{i,j} x_j$ &lt;/span&gt;&lt;br&gt;
&lt;small&gt;13&lt;/small&gt; &lt;span style="margin-left: 10%;"&gt; End &lt;/span&gt;&lt;br&gt;
&lt;small&gt;14&lt;/small&gt; &lt;span style="margin-left: 10%;"&gt; $x_i = \dfrac{y}{a_{i,i}}$ &lt;/span&gt;&lt;br&gt;
&lt;small&gt;15&lt;/small&gt; End&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Algorithm open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Algorithm: &lt;em&gt;Gaussian Elimination with Scaled Partial Pivoting&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;A pivot element is the element of a matrix, $A$, which is selected to do certain calculations first. Pivoting helps reduce errors due to rounding.&lt;/p&gt;
&lt;p&gt;&lt;small&gt;1 &lt;/small&gt; Find maximal absolute values vector $\boldsymbol{s}$ with entries $s_i= \max_{j=1, \ldots, n} \left| a_{i,j} \right|$ &lt;br&gt;
&lt;small&gt;2 &lt;/small&gt; For $k=1$ to $n-1$: &lt;br&gt;
&lt;small&gt;3 &lt;/small&gt; &lt;span style="margin-left: 10%;"&gt; For $i = k$ to $n$: &lt;/span&gt; &lt;br&gt;
&lt;small&gt;4 &lt;/small&gt; &lt;span style="margin-left: 20%;"&gt; Compute $\left| a_{i,k} / s_i \right|$ &lt;/span&gt; &lt;br&gt;
&lt;small&gt;5 &lt;/small&gt; &lt;span style="margin-left: 10%;"&gt; End &lt;/span&gt; &lt;br&gt;
&lt;small&gt;6 &lt;/small&gt; &lt;span style="margin-left: 10%;"&gt; Find the row with the largest relative pivot element and denote this as row $j$ &lt;/span&gt; &lt;br&gt;
&lt;small&gt;7 &lt;/small&gt; &lt;span style="margin-left: 10%;"&gt; Swap rows $k$ and $j$ &lt;/span&gt; &lt;br&gt;
&lt;small&gt;8 &lt;/small&gt; &lt;span style="margin-left: 10%;"&gt; Swap entries $k$ and $j$ in vector $\boldsymbol{s}$ &lt;/span&gt; &lt;br&gt;
&lt;small&gt;9 &lt;/small&gt; &lt;span style="margin-left: 10%;"&gt; Do forward elimination on row $k$ &lt;/span&gt; &lt;br&gt;
&lt;small&gt;10&lt;/small&gt; End &lt;br&gt;&lt;/p&gt;
&lt;p&gt;The matrix will now be in row-echelon form, so that back substitution can be performed.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Theorem open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M8.25 6.75h12M8.25 12h12m-12 5.25h12M3.75 6.75h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0M3.75 12h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0m-.375 5.25h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Theorem: &lt;em&gt;$LU$-Decomposition&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;Let ${A \in \mathbb{R}^{n \times n}}$ be invertible. Then there exists a decomposition of $A$ such that ${A=LU}$, where $L$ is a lower triangular matrix and $U$ is an upper triangular matrix, And
&lt;/p&gt;
$$
L = U_1^{-1} U_2^{-1} \cdots U_{n-1}^{-1}
$$&lt;p&gt;
where each matrix $U_i$ is a matrix which describes the $i$&lt;sup&gt;th&lt;/sup&gt; step in forward elimination. The upper triangular matrix $U$ is given by
&lt;/p&gt;
$$
U = U_{n-1} \cdots U_2 U_1 A.
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!--
### Fundamental Theorem of Numerical Analysis
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Stable&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;A numerical method is said to be &lt;strong&gt;stable&lt;/strong&gt; if and only if any initial error $e_0$ is damped during the iterations, i.e. ${\left\| e_k \right\| &lt; \left\| e_0 \right\| }$.&lt;/p&gt;
&lt;p&gt;Note that $\Vert x \Vert $ is a &lt;em&gt;norm&lt;/em&gt; of a vector, such as ${\Vert x \Vert_2 = \sqrt{x_0^2 + x_1^2 + \ldots + x_n^2} }$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Consistent&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
A numerical method is said to be &lt;strong&gt;consistent&lt;/strong&gt; if any fixed point $x^{\ast}$ of the iteration is a solution to the problem being solved.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
For linear systems, a fixed point, $x^{\ast}$, fulfils
$$
x^{\ast} = \left( I - Q^{-1} A \right) x^{\ast} + Q^{-1} b \Leftrightarrow A x^{\ast} = b.
$$
If the iterative method for a linear system is stable then
$$
e_k = \left( I - Q^{-1} A \right)^k e_0,
$$
so then ${\Vert I - Q^{-1} A \Vert &lt; 1}$ for ${\Vert e_k \Vert &lt; \Vert e_0 \Vert}$, where ${\Vert I - Q^{-1} A \Vert}$ is a _matrix norm_.
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Convergent&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
A numerical method is said to be &lt;strong&gt;convergent&lt;/strong&gt; if ${x_k \rightarrow x^{\ast}}$ as ${k \rightarrow \infty}$ where $x^{\ast}$ is the exact solution.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Theorem open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M8.25 6.75h12M8.25 12h12m-12 5.25h12M3.75 6.75h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0M3.75 12h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0m-.375 5.25h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Theorem: &lt;em&gt;Fundamental Theorem of Numerical Analysis&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
A numerical method is convergent if and only if it is consistent and stable.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Example open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 15.75V18m-7.5-6.75h.008v.008H8.25zm0 2.25h.008v.008H8.25zm0 2.25h.008v.008H8.25zm0 2.25h.008v.008H8.25zm2.498-6.75h.007v.008h-.007zm0 2.25h.007v.008h-.007zm0 2.25h.007v.008h-.007zm0 2.25h.007v.008h-.007zm2.504-6.75h.008v.008h-.008zm0 2.25h.008v.008h-.008zm0 2.25h.008v.008h-.008zm0 2.25h.008v.008h-.008zm2.498-6.75h.008v.008h-.008zm0 2.25h.008v.008h-.008zM8.25 6h7.5v2.25h-7.5zM12 2.25c-1.892 0-3.758.11-5.593.322C5.307 2.7 4.5 3.65 4.5 4.757V19.5a2.25 2.25 0 0 0 2.25 2.25h10.5a2.25 2.25 0 0 0 2.25-2.25V4.757c0-1.108-.806-2.057-1.907-2.185A48.507 48.507 0 0 0 12 2.25"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Example: &lt;em&gt;&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;An &lt;span class="mycode"&gt;.ipynb&lt;/span&gt; notebook with an example of iterative solvers for linear systems can be accessed online &lt;a class="hover:red" href="https://djps.github.io/docs/numericalmethods/notebooks/iterative/" style="text-decoration: underline; font-weight: 500; text-decoration-color: #651fff;"&gt;[here]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a class="underline text-red-600 hover:text-red-800 visited:text-red-600 dark:text-red-300 dark:hover:text-red-200" href="https://djps.github.io/ipyth/iterative.py/" &gt;[here]&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href="https://djps.github.io/ipyth/iterative.ipynb"&gt;[here]&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
--&gt;</description></item><item><title>Vector Spaces</title><link>https://djps.github.io/docs/gradcalclinalg24/part1/spaces/</link><pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate><guid>https://djps.github.io/docs/gradcalclinalg24/part1/spaces/</guid><description>&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Linear Independence&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;A set of vectors, $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n$ are linearly independent if none of the vectors can be expressed as a linear combination of the remaining $n-1$ vectors.&lt;/p&gt;
&lt;p&gt;An alternative definition is that if $c_1 \vec{v}_1 + c_2 \vec{v}_2 + \ldots c_n \vec{v}_n = \vec{0}$, then the only set of values for $c_i$ which satisfies this, are $c_1 = c_2 = \ldots = c_n =0$. That is, if the column vectors of $A$ are linearly independent, then the only solution to $A\vec{x}=\vec{0}$ is $\vec{x} =\vec{0}$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Thus a matrix $A$ has linearly independent columns if and only if the equation $A\vec{x} = \vec{0}$ has exactly one solution.&lt;/p&gt;
&lt;p&gt;If the columns of $A$ are not linearly independent, then $A\vec{x} =\vec{0}$ will have infinitely many solutions.&lt;/p&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Vector Spaces&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;A vector space is a set $V$ with two operations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Addition of the elements of $V$, i.e. if $\vec{v}$, $\vec{u} \in V$, then $\vec{v} + \vec{u} \in V$&lt;/li&gt;
&lt;li&gt;Multiplication of the elements of $V$ by a scalar, i.e. if $\vec{v} \in V$, and $\alpha \in \mathbb{R}$ then $\alpha\vec{v}~\in~V$,&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;which satisfies the following conditions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\vec{v} + \vec{u} = \vec{u} + \vec{v}$&lt;/li&gt;
&lt;li&gt;$\vec{u} + \left(\vec{v} + \vec{w} \right) =\left(\vec{u} + \vec{v}\right) + \vec{w}$&lt;/li&gt;
&lt;li&gt;There exists a vector $\vec{0} \in V$ such that $ \vec{u} + \vec{0} = \vec{u}$ for all $\vec{u} \in V$&lt;/li&gt;
&lt;li&gt;There exists a vector $\vec{1} \in V$ such that $ \vec{u}\vec{1} = \vec{u}$ for all $\vec{u} \in V$&lt;/li&gt;
&lt;li&gt;For any vector $\vec{v} \in V$, there exists a vector $\vec{u} \in V$ such that $\vec{v} + \vec{u} = \vec{0}$, which is denoted as $\vec{u} = - \vec{v}$&lt;/li&gt;
&lt;li&gt;For any $a$, $b \in \mathbb{R}$ and $\vec{v} \in V$, then $a\left(b \vec{v} \right) = \left( a b\right)\vec{v}$&lt;/li&gt;
&lt;li&gt;$a\left( \vec{v} + \vec{u} \right) = a \vec{v} + a \vec{u}$&lt;/li&gt;
&lt;li&gt;$\left( a + b \right)\vec{v} = a \vec{v} + b \vec{v}$&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Subspaces&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
If $V$ is a vector space and $W \subset V$ and $W$ is also a vector space, then it is called a subspace of $V$.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Span&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
If $\mathcal{A} = \left\{ \boldsymbol{v}_1, \ldots, \boldsymbol{v}_k \right\}$ where each vector $\boldsymbol{v}_i \in\mathbb{R}^n$, then the span of $\mathcal{A}$ is the set of all possible linear combinations of the vectors in $\mathcal{A}$.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Basis&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
A basis of a vectors space is the maximal collection of linearly independent vectors from that vector space.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Thus, if you add another vector from the vectors space to the basis set, it will be a linear combination of the vectors from the basis.&lt;/p&gt;
&lt;p&gt;The number of vectors in the basis is the &lt;strong&gt;dimension of the vector space&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Column and Row Spaces&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
The column space of a matrix $A$ is the span of all columns of $A$. Similarly, the row space is the span of the rows of $A$ or the column space of $A^T$.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Null spaces&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
The null space of a matrix $A$ is the collection of all solutions to $A \boldsymbol{x} = \boldsymbol{0}$.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Rank&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
The rank of a matrix $A$ is the dimension of the column space of $A$.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The rank is also the number of pivots in $A$ when performing Gaussian elimination.&lt;/p&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Projections&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;The projection of a vector $\boldsymbol{v}$ onto a nonzero vector $\boldsymbol{u}$ is given by
&lt;/p&gt;
$$
\textrm{proj}_{\boldsymbol{u}} \left(\boldsymbol{v} \right) = \dfrac{\boldsymbol{v} \cdot \boldsymbol{u} }{\boldsymbol{u} \cdot \boldsymbol{u} }\boldsymbol{u}.
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Gram-Schmidt&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;Given $k$ vectors $\boldsymbol{v_1}, \ldots, \boldsymbol{v_k}$ the Gram–Schmidt process defines the vectors $\boldsymbol{u_1}, \ldots, \boldsymbol{u_k}$ as follows:
&lt;/p&gt;
$$
\begin{align*}
\boldsymbol{u_1} &amp; = \boldsymbol{v_1} \\
&amp; \vdots \\
\boldsymbol{u_k} &amp; = \boldsymbol{v_k} - \sum_{j=1}^{k-1} \textrm{proj}_{\boldsymbol{u_j}} \left(\boldsymbol{v_k} \right).
\end{align*}
$$&lt;p&gt;
The set of vectors $\boldsymbol{u_k}$ are orthogonal. Normalizing the vectors as $\boldsymbol{e_j} = \dfrac{\boldsymbol{u_j}}{ \left\| \boldsymbol{u_j} \right\|}$ is a set of orthornormal vectors.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Example open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 15.75V18m-7.5-6.75h.008v.008H8.25zm0 2.25h.008v.008H8.25zm0 2.25h.008v.008H8.25zm0 2.25h.008v.008H8.25zm2.498-6.75h.007v.008h-.007zm0 2.25h.007v.008h-.007zm0 2.25h.007v.008h-.007zm0 2.25h.007v.008h-.007zm2.504-6.75h.008v.008h-.008zm0 2.25h.008v.008h-.008zm0 2.25h.008v.008h-.008zm0 2.25h.008v.008h-.008zm2.498-6.75h.008v.008h-.008zm0 2.25h.008v.008h-.008zM8.25 6h7.5v2.25h-7.5zM12 2.25c-1.892 0-3.758.11-5.593.322C5.307 2.7 4.5 3.65 4.5 4.757V19.5a2.25 2.25 0 0 0 2.25 2.25h10.5a2.25 2.25 0 0 0 2.25-2.25V4.757c0-1.108-.806-2.057-1.907-2.185A48.507 48.507 0 0 0 12 2.25"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Example: &lt;em&gt;&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;An &lt;span class="mycode"&gt;.ipynb&lt;/span&gt; notebook with an example of the Gram-Schmidt process can be accessed online &lt;a href="https://djps.github.io/docs/gradcalclinalg24/notebooks/gram_schmidt/"&gt;[here]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a href="https://djps.github.io/ipyth/gram_schmidt.py/"&gt;[here]&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href="https://djps.github.io/ipyth/gram_schmidt.ipynb"&gt;[here]&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>Eigenvalues &amp; Principal Component Analysis</title><link>https://djps.github.io/docs/gradcalclinalg24/part1/eigen/</link><pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate><guid>https://djps.github.io/docs/gradcalclinalg24/part1/eigen/</guid><description>&lt;h2 id="eigenvalues--eigenvectors"&gt;Eigenvalues &amp;amp; Eigenvectors&lt;/h2&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Eigenvalues and Eigenvectors&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;In linear algebra, an eigenvector is a non-zero vector that has its direction unchanged by a given linear transformation. More precisely, an eigenvector, $\boldsymbol{v}$, of a linear transformation, $A$, is scaled by a constant factor, $\lambda$, when the linear transformation is applied to it:
&lt;/p&gt;
$$
A \boldsymbol {v} = \lambda \boldsymbol {v}.
$$&lt;p&gt;
then $\boldsymbol {v}$ is called an eigenvector of $A$, and $\lambda$ is the corresponding eigenvalue. Thus $A\boldsymbol{v}$ and $\boldsymbol{v}$ are &lt;em&gt;collinear&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
$$
A \boldsymbol {v} = \lambda \boldsymbol {v} \Rightarrow A \boldsymbol {v} - \lambda \boldsymbol {v} = \boldsymbol{0}
$$&lt;p&gt;
i.e.
&lt;/p&gt;
$$
B \boldsymbol {v} = \boldsymbol{0} \quad \textsf{where} \quad B = A - \lambda I.
$$&lt;p&gt;
As $\boldsymbol{v}$ is not a zero vector and $B \boldsymbol{v} = \boldsymbol{0}$ then the determinant of $B$ is zero. Finding the roots to $\left| A- \lambda I \right|$ yields the eigenvalues, whose eigenvectors are in the span of the nullspace of $B=A-\lambda I$.&lt;/p&gt;
$$
A \boldsymbol {v} = X^{-1} D X \boldsymbol {v}
$$&lt;p&gt;Thus powers of matrices, such as $A^k$ can be easily computed for any $k$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $A$ is triangular, its eigenvalues are the entries on the diagonal.&lt;/li&gt;
&lt;li&gt;For an arbitrary $n$ by $n$ matrix $A$, the product of the $n$ eigenvalues is equal to the determinant of $A$.&lt;/li&gt;
&lt;li&gt;The sum of the $n$ eigenvalues is equal to the trace of $A$.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="details admonition Theorem open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M8.25 6.75h12M8.25 12h12m-12 5.25h12M3.75 6.75h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0M3.75 12h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0m-.375 5.25h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Theorem: &lt;em&gt;Eigenvalues and Eigenvectors of Symmetric Matrices&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
All eigenvalues of a symmetric matrix are real and positive. Furthermore, the eigenvectors can be chosen to be pairwise orthogonal.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="principal-component-analysis"&gt;Principal Component Analysis&lt;/h2&gt;
&lt;p&gt;Principal Component Analysis is a linear transformation of a dataset, $Z$ onto a new coordinate system such that the directions (principal components) capturing the largest variation in the data.&lt;/p&gt;
&lt;p&gt;Shift so that the mean of each column is zero, i.e. subtract the mean of each column of $Z$ from itself, yielding a new matrix $X$.&lt;/p&gt;
&lt;p&gt;$X\boldsymbol{w}$ is the projection of each data row on the direction $w$.&lt;/p&gt;
&lt;p&gt;Given the mean of each column is zero, so the variance of the set of column vectors is given by
&lt;/p&gt;
$$
\begin{align*}
\text{var} \, X &amp; = \dfrac{1}{n-1} \left( x_{1}^{2} + \ldots + x_n^2 \right) \\
&amp; = \dfrac{1}{n-1} \left( X \boldsymbol{w} \right)^T \left( X \boldsymbol{w} \right) \\
&amp; = \dfrac{1}{n-1} \boldsymbol{w}^T X^T X \boldsymbol{w}.
\end{align*}
$$&lt;p&gt;
Find the vector $\boldsymbol{w}$ so that variance is maximal.&lt;/p&gt;
&lt;p&gt;Note that as ${A = X^T X}$ is symmetric, i.e. ${A^T = A}$, then all eigenvalues are real and there exists a orthonormal basis given by the eigenvectors, $\boldsymbol{q_i}$ of $A$. Let ${Q = \left( \boldsymbol{q_1} \, \boldsymbol{q_2} \cdots \boldsymbol{q_n} \right)}$, with ${Q^T = Q^{-1}}$ and ${D = \text{diag}\, {D} }$. Then $A$ can be expressed using the eigenvalues and eigenvectors, thus&lt;/p&gt;
$$
X^T X = A = Q D Q^T .
$$&lt;p&gt;Then, the expression for the variance is given by
&lt;/p&gt;
$$
\begin{align*}
\boldsymbol{w}^T X^T X \boldsymbol{w} &amp; = \boldsymbol{w}^T A \boldsymbol{w} \\
&amp; = \boldsymbol{w}^T Q D Q^T \boldsymbol{w} \\
&amp; = \left( \boldsymbol{w}^T Q \right) D \left( Q^T \boldsymbol{w} \right) \\
&amp; = \left( Q^T \boldsymbol{w} \right)^T D \left( Q^T \boldsymbol{w} \right), \quad \text{let} \quad \boldsymbol{y} = Q^T \boldsymbol{w} \\
&amp; = \boldsymbol{y}^T D \boldsymbol{y}.
\end{align*}
$$&lt;p&gt;
Since $\boldsymbol{w}$ is a unit vector and $Q$ is orthogonal, so $\boldsymbol{y}$ is also a unit vector. It can easily be shown that the vector ${\boldsymbol{y} = \left(1, 0 \ldots 0 \right)^T}$ maximizes the variance. Thus, the corresponding principal component $\boldsymbol{w}$ is recovered from ${\boldsymbol{y} = Q^T \boldsymbol{w}}$, i.e. ${\boldsymbol{w} = \left( Q^T\right)^{-1} \boldsymbol{y} = Q\boldsymbol{y}}$.&lt;/p&gt;
&lt;div class="details admonition Example open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 15.75V18m-7.5-6.75h.008v.008H8.25zm0 2.25h.008v.008H8.25zm0 2.25h.008v.008H8.25zm0 2.25h.008v.008H8.25zm2.498-6.75h.007v.008h-.007zm0 2.25h.007v.008h-.007zm0 2.25h.007v.008h-.007zm0 2.25h.007v.008h-.007zm2.504-6.75h.008v.008h-.008zm0 2.25h.008v.008h-.008zm0 2.25h.008v.008h-.008zm0 2.25h.008v.008h-.008zm2.498-6.75h.008v.008h-.008zm0 2.25h.008v.008h-.008zM8.25 6h7.5v2.25h-7.5zM12 2.25c-1.892 0-3.758.11-5.593.322C5.307 2.7 4.5 3.65 4.5 4.757V19.5a2.25 2.25 0 0 0 2.25 2.25h10.5a2.25 2.25 0 0 0 2.25-2.25V4.757c0-1.108-.806-2.057-1.907-2.185A48.507 48.507 0 0 0 12 2.25"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Example: &lt;em&gt;&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;An &lt;span class="mycode"&gt;.ipynb&lt;/span&gt; notebook with an example of principal component analysis can be accessed online &lt;a href="https://djps.github.io/docs/gradcalclinalg24/notebooks/pca/"&gt;[here]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a href="https://djps.github.io/ipyth/pca.py/"&gt;[here]&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href="https://djps.github.io/ipyth/pca.ipynb"&gt;[here]&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>Calculus of a Single Variable</title><link>https://djps.github.io/docs/gradcalclinalg24/part2/one-variable/</link><pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate><guid>https://djps.github.io/docs/gradcalclinalg24/part2/one-variable/</guid><description>&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Derivative of a function&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;The derivative of a function $f(x)$ is given by
&lt;/p&gt;
$$
f^{\prime}\left( x \right) = \lim_{h \rightarrow 0} \dfrac{f\left( x + h \right) - f\left(x\right)}{h}.
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Note the following&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$f\left(x\right) = c \quad \Rightarrow f^{\prime}\left( x \right) = 0$&lt;/li&gt;
&lt;li&gt;$f\left(x\right) = x^a \quad \Rightarrow f^{\prime}\left( x \right) = a x^{a-1}$&lt;/li&gt;
&lt;li&gt;$f\left(x\right) = a^x \quad \Rightarrow f^{\prime}\left( x \right) = a^x \ln a$&lt;/li&gt;
&lt;li&gt;$f\left(x\right) = \log_{b}x \quad \Rightarrow f^{\prime}\left( x \right) = \dfrac{1}{x \log_e b}$&lt;/li&gt;
&lt;li&gt;$f\left(x\right) = \sin\left(x\right) \quad \Rightarrow f^{\prime}\left( x \right) = \cos\left(x\right)$&lt;/li&gt;
&lt;li&gt;$f\left(x\right) = \cos\left(x\right) \quad \Rightarrow f^{\prime}\left( x \right) = -\sin\left(x\right)$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thus for $f(x)=a^x$, when $a=e$ then, $f=e^x$ and $f^{\prime}\left( x \right) = f\left( x \right) = e^x$.&lt;/p&gt;
&lt;p&gt;Similarly, for $f\left(x\right) = \log_{b}x$ when $b=e$, i.e. $f\left(x\right) = \log_e x = \ln x$, so $f^{\prime}\left( x \right) = \dfrac{1}{x}$.&lt;/p&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Summation Rule&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;For a function of the form $f = g+h$,
&lt;/p&gt;
$$
\dfrac{\mathrm{d}\left(h+g\right)}{\mathrm{d} x} = h^{\prime}\left( x \right) + g^{\prime}\left( x \right).
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Product Rule&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;For a function which is of the form $f = g h$
&lt;/p&gt;
$$
\dfrac{\mathrm{d}\left(hg\right)}{\mathrm{d} x} = h^{\prime}\left( x \right)g\left(x\right) + h\left(x\right)g^{\prime}\left( x \right).
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Quotient Rule&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;For a function which is of the form $f = g /h$
&lt;/p&gt;
$$
\dfrac{\mathrm{d}f\left(x\right)}{\mathrm{d} x} = \dfrac{h\left(x\right)g^{\prime}\left( x \right) - h^{\prime}\left( x \right)g\left(x\right)}{ \left( h^\prime \left(x\right) \right)^2 }.
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Chain Rule&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;The chain rule enables the derivative of a function which can be expressed as a composition of two differentiable functions. Thus, for a function which is of the from $f = g \left( h\left(x\right) \right)$
&lt;/p&gt;
$$
\dfrac{\mathrm{d}f\left(x\right)}{\mathrm{d} x} = g^{\prime}\left( h \right) h^{\prime}\left(x\right).
$$&lt;p&gt;
Another expression is
&lt;/p&gt;
$$
\dfrac{\mathrm{d}f\left(x\right)}{\mathrm{d} x} = \dfrac{\mathrm{d}g}{\mathrm{d} h} \dfrac{\mathrm{d}h}{\mathrm{d}x}.
$$&lt;p&gt;
This form can be understood as stating that if a function $f$ is written in terms of $g$, which itself depends on the variable $x$ (that is both $f$ and $g$ are dependent variables), then $f$ depends on $x$ as well, via the intermediate variable $g$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;For example, the function could be $\sin\left( x^2 \right)$, then write the function as $h(x) = f(g(x))$, where $f(x)=\sin(y(x))$ and $y(x)=x^2$, then the derivative is $f^{\prime} = 2x \cos\left( x^2\right)$.&lt;/p&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Critical Point of a Function&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
If $f^{\prime} \left( x_0 \right) = 0$ for some $x_0$, then this point is called a critical point of $f$.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Critical points are candidates for being local maxima or minima for the function.&lt;/p&gt;
&lt;h2 id="global-maximum"&gt;Global Maximum&lt;/h2&gt;
&lt;p&gt;If $f(x)$ is a continuous function of a closed, bounded interval, then it always attains a global maximum and global minimum on that interval.&lt;/p&gt;
&lt;h2 id="linear-approximations"&gt;Linear Approximations&lt;/h2&gt;
&lt;p&gt;The tangent line at a point $x_0$ is the linear approximation of $f(x)$ at the point $x_0$, that is
&lt;/p&gt;
$$
y = f\left(x_0 \right) + f^{\prime} \left(x_0 \right) \left( x - x_0 \right).
$$&lt;p&gt;
This is of the form $y = mx + b$ where the gradient is $m=\left(x_0 \right)$ and the intercept is given by $b= f\left(x_0 \right) - x_0 f^{\prime} \left(x_0 \right)$.&lt;/p&gt;
&lt;h2 id="second-derivatives"&gt;Second Derivatives&lt;/h2&gt;
&lt;p&gt;Assuming $x_0$ is a critical point of $f(x)$ then $f^{\prime}\left( x_0 \right)=0$, so the Taylor expansion about $x_0$ is&lt;/p&gt;
$$
f\left(x_0 + x \right) \approx f\left(x_0\right) + \dfrac{1}{2!} f^{\prime\prime}\left(x_0 \right) x^2 + \ldots
$$&lt;p&gt;Then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $f^{\prime\prime}\left(x_0 \right) &lt; 0$, then $x_0$ is a local maxima&lt;/li&gt;
&lt;li&gt;If $f^{\prime\prime}\left(x_0 \right) &gt; 0$, then $x_0$ is a local minima&lt;/li&gt;
&lt;li&gt;If $f^{\prime\prime}\left(x_0 \right) = 0$, then the test fails.&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Taylor Series</title><link>https://djps.github.io/docs/gradcalclinalg24/part2/taylor/</link><pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate><guid>https://djps.github.io/docs/gradcalclinalg24/part2/taylor/</guid><description>&lt;p&gt;The Taylor series, or Taylor expansion of a function, is defined as&lt;/p&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Taylor Series&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;For a function $f : \mathbb{R} \mapsto \mathbb{R}$ which is infinitely differentiable at a point $c$, the Taylor series of $f(c)$ is given by
&lt;/p&gt;
$$
\begin{equation*}
\sum\limits_{k=0}^{\infty} \dfrac{ f^{(k)} \left( c \right) }{k!} \left( x - c \right)^{k}.
\end{equation*}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This is a infinite series of powers of the variable $x$, which is convergent for some values of $x$ such that $\left| x - c \right| &lt; r $ where $r$ is the radius of convergence.&lt;/p&gt;
&lt;div class="details admonition Theorem open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M8.25 6.75h12M8.25 12h12m-12 5.25h12M3.75 6.75h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0M3.75 12h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0m-.375 5.25h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Theorem: &lt;em&gt;Taylor&amp;#39;s Theorem&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;For a function $f \in C^{n+1}\left([a, b]\right)$, i.e. $f$ is $(n+1)$-times continuously differentiable in the interval $[a, b]$, then for some $c$ in the interval, the function can be written as
&lt;/p&gt;
$$
\begin{equation*}
f\left( x \right) = \sum\limits_{k=0}^{n} \dfrac{f^{(k)} \left(c\right) }{k!} \left( x- c \right)^{k} + \dfrac{f^{(n+1)} \left( \xi \right) }{\left( n + 1 \right)!} \left( x - c \right)^{n+1}
\end{equation*}
$$&lt;p&gt;
for some value $\xi \in \left[ a, b \right]$ where
&lt;/p&gt;
$$
\begin{equation*}
\lim\limits_{\xi \rightarrow c} \dfrac{ f^{(n+1)} \left( \xi \right) }{ \left( n + 1 \right)!} \left( x - c \right)^{n+1} = 0.
\end{equation*}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Theorem open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M8.25 6.75h12M8.25 12h12m-12 5.25h12M3.75 6.75h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0M3.75 12h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0m-.375 5.25h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Theorem: &lt;em&gt;Taylor&amp;#39;s Theorem for Multivariate Functions&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;For a function $f \, : \, \mathbb{R}^n \mapsto \mathbb{R}$ which is differentiable around $\boldsymbol{a}$, then the Taylor expansion can be generalised as
&lt;/p&gt;
$$
\begin{equation*}
f\left( \boldsymbol{x} + \boldsymbol{a} \right) = f\left( \boldsymbol{x} \right) + \boldsymbol{a}\cdot J + \dfrac{1}{2} \boldsymbol{a}^T H \boldsymbol{a} + \ldots
\end{equation*}
$$&lt;p&gt;
where $J$ is the Jacobian and $H$ is the Hessian.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Example open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 15.75V18m-7.5-6.75h.008v.008H8.25zm0 2.25h.008v.008H8.25zm0 2.25h.008v.008H8.25zm0 2.25h.008v.008H8.25zm2.498-6.75h.007v.008h-.007zm0 2.25h.007v.008h-.007zm0 2.25h.007v.008h-.007zm0 2.25h.007v.008h-.007zm2.504-6.75h.008v.008h-.008zm0 2.25h.008v.008h-.008zm0 2.25h.008v.008h-.008zm0 2.25h.008v.008h-.008zm2.498-6.75h.008v.008h-.008zm0 2.25h.008v.008h-.008zM8.25 6h7.5v2.25h-7.5zM12 2.25c-1.892 0-3.758.11-5.593.322C5.307 2.7 4.5 3.65 4.5 4.757V19.5a2.25 2.25 0 0 0 2.25 2.25h10.5a2.25 2.25 0 0 0 2.25-2.25V4.757c0-1.108-.806-2.057-1.907-2.185A48.507 48.507 0 0 0 12 2.25"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Example: &lt;em&gt;&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;An &lt;span class="mycode"&gt;.ipynb&lt;/span&gt; notebook with an example of the Taylor series for $\sin\left(x\right)$ can be accessed online &lt;a href="https://djps.github.io/docs/gradcalclinalg24/notebooks/taylor_series/"&gt;[here]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a href="https://djps.github.io/ipyth/taylor_series.py/"&gt;[here]&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href="https://djps.github.io/ipyth/taylor_series.ipynb"&gt;[here]&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Theorem open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M8.25 6.75h12M8.25 12h12m-12 5.25h12M3.75 6.75h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0M3.75 12h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0m-.375 5.25h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Theorem: &lt;em&gt;Rolle&amp;#39;s Theorem&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;If a real-valued function $f$ is continuous on a proper closed interval $[a, b]$, differentiable on the open interval $(a, b)$, and has ${f (a) = f (b)}$, then there exists at least one $c$ in the open interval $(a, b)$ such that
&lt;/p&gt;
$$
f^\prime (c) = 0.
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Theorem open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M8.25 6.75h12M8.25 12h12m-12 5.25h12M3.75 6.75h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0M3.75 12h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0m-.375 5.25h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Theorem: &lt;em&gt;Mean Value Theorem&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;The theorem states that if $f$ is a continuous function on the closed interval $[a ,b]$ and differentiable on the open interval $(a, b)$, then there exists a point ${c \in (a, b)}$ such that the tangent at $c$ is parallel to the secant line through the endpoints ${\big(a, f(a) \big)}$ and ${\big(b, f(b) \big)}$, that is,
&lt;/p&gt;
$$
f^\prime (c) = \dfrac{f(b) - f(a)}{b - a}.
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!--
Note that differentiating a vector function which is a matrix vector product means $D(x^TAx) = D(A)x + A D(x)
--&gt;</description></item><item><title>Calculus of a Function of Several Variables</title><link>https://djps.github.io/docs/gradcalclinalg24/part2/many-variables/</link><pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate><guid>https://djps.github.io/docs/gradcalclinalg24/part2/many-variables/</guid><description>&lt;!-- Consider functions which take multiple input arguments, and can either output a scalar value or a vector, i.e. $f \, : \, \mathbb{R}^n \mapsto \mathbb{R^m}$ --&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Partial Derivatives&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;For a function with multiple input arguments, $z = f\left(x,y\right)$, the partial derivative of $f$ with respect to $x$ can be expressed as
&lt;/p&gt;
$$
\dfrac{\partial f}{\partial x} = \lim_{h \rightarrow 0} \dfrac{f\left(x+h,y\right) - f\left(x,y\right) }{h}
$$&lt;p&gt;
similarly, the partial derivative with respect to $y$ can be expressed as
&lt;/p&gt;
$$
\dfrac{\partial f}{\partial y} = \lim_{h \rightarrow 0} \dfrac{f\left(x,y+h\right) - f\left(x,y\right) }{h}.
$$&lt;p&gt;
Note that the partial derivative is often denoted as $\dfrac{\partial f}{\partial x} = f_x$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Gradient of a Function&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;The gradient of a function $z=f\left(x_1, x_2, \ldots, x_n \right)$ is the row vector
&lt;/p&gt;
$$
\nabla f = \left( \dfrac{\partial f}{\partial x_1}, \dfrac{\partial f}{\partial x_2}, \ldots, \dfrac{\partial f}{\partial x_n} \right).
$$&lt;p&gt;
As the gradient $\nabla f$ depends on the point at which it is evaluated, it is denoted by $\nabla f \left(x_1, x_2, \ldots, x_n \right)$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The gradient is the analogue to the derivative, but, as a vector, has a direction.&lt;/p&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Critical Point of a Function&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;If $f\left(x, y\right)$ has a local minima or maxima at $\left( x_0, y_0 \right)$, then $\nabla f\left( x_0, y_0 \right) = \vec{0}$.&lt;/p&gt;
&lt;p&gt;Such points are called &lt;strong&gt;critical points&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Note that not all critical points are either maxima or minima. The classification of the critical points high-order derivatives.&lt;/p&gt;
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Second Derivatives&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
&lt;p&gt;Each partial derivative can be differentiated again to yield a second-order partial derivative,
&lt;/p&gt;
$$
\dfrac{\partial^2 f}{\partial x_i \partial x_j} = \lim_{h \rightarrow 0} \dfrac{f_{x_i} \left(x_1, \ldots, x_j + h, \ldots, x_n \right) - f_{x_i}\left( x_1, \ldots, x_n \right)}{h}.
$$&lt;p&gt;
Thus, all second derivatives can be expressed using the &lt;strong&gt;Hessian matrix&lt;/strong&gt;
&lt;/p&gt;
$$
H\left( \vec{x} \right) = \left(
\begin{array}{cccc}
f_{x_1 x_1} &amp; f_{x_1 x_2} &amp; \cdots &amp; f_{x_1 x_n} \\
f_{x_1 x_1} &amp; f_{x_2 x_2} &amp; \cdots &amp; f_{x_2 x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
f_{x_n x_1} &amp; f_{x_n x_2} &amp; \cdots &amp; f_{x_1 x_n}
\end{array}
\right).
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;If the second-order partial derivatives are continuous, then the Hessian matrix $H$ is symmetric. Then, if symmetric all eigenvalues are real-valued.&lt;/p&gt;
&lt;p&gt;The Taylor expansion of a function of more than one variable about a critical point is given by
&lt;/p&gt;
$$
f\left(\vec{x} + \vec{a} \right) = f\left( \vec{a} \right) + \dfrac{1}{2} \vec{x}^T H\left( \vec{a} \right) \vec{x} + \ldots
$$&lt;p&gt;
Note that the Hessian matrix can be factorized as
&lt;/p&gt;
$$
H = Q D Q^{-1}
$$&lt;p&gt;
where $D$ is the diagonal matrix $\mathrm{diag}\left( \lambda_1, \ldots, \lambda_n \right)$ and $\lambda_i$ are the eigenvalues associated with eigenvector $\boldsymbol{q_i}$ of the Hessian matrix. The eigenvector $\boldsymbol{q_i}$ is the $i$&lt;sup&gt;th&lt;/sup&gt; column of the matrix $Q$. As the eigenvectors are orthogonal, so $Q$ is an orthogonal matrix, thus $Q^{-1} = Q^T$. Hence,
&lt;/p&gt;
$$
f\left(\vec{x} + \vec{a} \right) = f\left( \vec{a} \right) + \dfrac{1}{2} \vec{x}^T Q D Q^{T} \vec{x} + \ldots
$$&lt;p&gt;
Letting $\vec{y}=Q^T \vec{x}$, then
&lt;/p&gt;
$$
\begin{align*}
f\left(\vec{x} + \vec{a} \right) &amp; = f\left( \vec{a} \right) + \dfrac{1}{2} \vec{y}^T D \vec{y} + \ldots \\
&amp; = f\left( \vec{a} \right) + \dfrac{1}{2} \left( \lambda_1^{} y_1^2 + \ldots \lambda_n^{} y_n^2 \right) + \ldots
\end{align*}
$$&lt;p&gt;
So:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If all $\lambda_1$, $\lambda_2, \ldots, \lambda_n &gt; 0$, then the critical point is a &lt;em&gt;local minima&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;If all $\lambda_1$, $\lambda_2, \ldots, \lambda_n &lt; 0$, then the critical point is a &lt;em&gt;local maxima&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;If some $\lambda_j &lt; 0$, and some $\lambda_i &gt; 0$, then the critical point is neither a local minima nor maxima. If none of the eigenvalues are equal to zero, then the critical point is called a &lt;em&gt;saddle point&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;If all $\lambda_1$, $\lambda_2, \ldots, \lambda_n \geq 0$ and at least is zero, or $\lambda_1$, $\lambda_2, \ldots \lambda_n, \leq 0$ and at least one is zero then the test is inconclusive, as the classification of the point depends on higher derivatives.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="jacobians"&gt;Jacobians&lt;/h2&gt;
&lt;p&gt;If the function outputs a vector, i.e. $\vec{f} \, : \, \mathbb{R}^n \mapsto \mathbb{R}^{m}$, write each component of $\vec{f}=\left( f_1, \ldots, f_m \right)$ and the same procedures can be performed on each component of the vector-valued function. Thus, a gradient can be computed for each component $\nabla f_i$, critical points must satisfy the vector equation $\vec{f}\left(\vec{x}^{*} \right) = \vec{0}$.&lt;/p&gt;
&lt;p&gt;or for $f(u,v)$, let $u=g(x), v=h(x)$ then the function is a composition $F(x) = f\left( g(x),h(x) \right)$. Apply the chain rule to compute the gradient
&lt;/p&gt;
$$
\nabla F = \nabla \left( f (g(x)), f(h(x)) \right) = \nabla f \dfrac{\partial (g,h)}{\partial x}
$$&lt;p&gt;&lt;br&gt;
where
&lt;/p&gt;
$$
\nabla f \dfrac{\partial (g,h)}{\partial x} = \left(
\begin{array}{c}
\nabla g \\
\nabla h
\end{array} \right)
$$&lt;p&gt;
is the &lt;strong&gt;Jacobi matrix&lt;/strong&gt;. Each row of the Jacobi matrix is a gradient of $g$ or $h$.&lt;/p&gt;
&lt;p&gt;For a vector $\boldsymbol{w}$, the &lt;strong&gt;directional derivative&lt;/strong&gt; of $f(\boldsymbol{x})$ in the direction of $\boldsymbol{w}$ is given by
&lt;/p&gt;
$$
\nabla_{\boldsymbol{w}} f \left(\boldsymbol{x}\right) = \lim_{h \rightarrow 0} \dfrac{f(\boldsymbol{x} + h\boldsymbol{v}) - f(\boldsymbol{x})}{h}
$$&lt;p&gt;
If the function is differentiable, then
&lt;/p&gt;
$$
\nabla_{\boldsymbol{w}} f \left(\boldsymbol{x}\right) = \nabla f \left(\boldsymbol{x}\right) \cdot \boldsymbol{w}
$$&lt;p&gt;
As, by convention, $\nabla f \left(\boldsymbol{x}\right)$ is a column vector.&lt;/p&gt;
&lt;p&gt;The directional derivative can then be expressed as a matrix-vector product, specifically a Jacobian-vector product.&lt;/p&gt;
&lt;p&gt;By the Cauchy-Schwarz inequality, the largest value of the directional derivative is when $\nabla f$ and $\boldsymbol{w}$ are pointing in the same direction.&lt;/p&gt;
&lt;!--
&lt;div class="details admonition Definition open"&gt;
&lt;div class="details-summary admonition-title"&gt;
&lt;span class="flex pr-3 pt-1"&gt;
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"&gt;&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/&gt;&lt;/svg&gt;
&lt;/span&gt;
&lt;span class="dark:text-neutral-300"&gt;
Definition: &lt;em&gt;Chain Rule&lt;/em&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="details-content"&gt;
&lt;div class="admonition-content"&gt;
The chain rule enables the derivative of a function which can be expressed as a composition of two differentiable functions.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
Jacobi Matrix
Global Maximina
Newton Methods
--&gt;</description></item></channel></rss>