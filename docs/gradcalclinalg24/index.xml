<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MDE-MET-01: Calculus and Linear Algebra for Graduate Students | David Sinden</title>
    <link>https://djps.github.io/docs/gradcalclinalg24/</link>
      <atom:link href="https://djps.github.io/docs/gradcalclinalg24/index.xml" rel="self" type="application/rss+xml" />
    <description>MDE-MET-01: Calculus and Linear Algebra for Graduate Students</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 08 Aug 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://djps.github.io/docs/gradcalclinalg24/featured.jpg</url>
      <title>MDE-MET-01: Calculus and Linear Algebra for Graduate Students</title>
      <link>https://djps.github.io/docs/gradcalclinalg24/</link>
    </image>
    
    <item>
      <title>MDE-MET-01: Calculus and Linear Algebra for Graduate Students</title>
      <link>https://djps.github.io/docs/gradcalclinalg24/intro/intro/</link>
      <pubDate>Thu, 08 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/docs/gradcalclinalg24/intro/intro/</guid>
      <description>&lt;p&gt;This site will be the primary source of information for this course.&lt;/p&gt;
&lt;h2 id=&#34;lectures&#34;&gt;Lectures&lt;/h2&gt;
&lt;p&gt;Lectures will take place in autumn 2024, on Wednesday mornings 9:45-11:00 and 11:15-12:30 at &lt;del&gt;EH-4 (East Hall Classroom 4)&lt;/del&gt; IRC Seminar Room III.&lt;/p&gt;
&lt;p&gt;An office hour can be arranged by appointment.&lt;/p&gt;
&lt;p&gt;Course notes can be found &lt;a href=&#34;https://djps.github.io/docs/gradcalclinalg24/notes/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;recommendations-for-preparation&#34;&gt;Recommendations for Preparation&lt;/h2&gt;
&lt;p&gt;There are no recommendations beyond reading the syllabus and the outline of the course.&lt;/p&gt;
&lt;h2 id=&#34;content-and-educational-aims&#34;&gt;Content and Educational Aims&lt;/h2&gt;
&lt;p&gt;This module offers a highly structured introduction to the fundamentals of two major pillars of mathematical modelling and analysis: single and multivariable calculus on the one hand and linear algebra on the other.&lt;/p&gt;
&lt;p&gt;It is a gateway for graduate students who have not been exposed to the topics so far, or who would benefit from a refresher.&lt;/p&gt;
&lt;p&gt;This course will focus on practical experience rather than on mathematical rigour.&lt;/p&gt;
&lt;h2 id=&#34;intended-learning-outcomes&#34;&gt;Intended Learning Outcomes&lt;/h2&gt;
&lt;p&gt;Upon completion of this module, students will be able to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Apply the fundamental concepts of calculus and linear algebra in structured situations&lt;/li&gt;
&lt;li&gt;Understand and use vectors and matrices, calculate determinants, eigenvalues and eigenvectors in simple cases&lt;/li&gt;
&lt;li&gt;Calculate derivatives and simple integrals&lt;/li&gt;
&lt;li&gt;Explain the importance of the methods of calculus and linear algebra in problems arising from applications&lt;/li&gt;
&lt;li&gt;Understand the methods of calculus and linear algebra used in more advanced modules as well as in scientific literature&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The informal aim is to build intuition of some of the fundamental concepts in modern mathematics which are used in machine learning.&lt;/p&gt;
&lt;h2 id=&#34;indicative-literature&#34;&gt;Indicative Literature&lt;/h2&gt;
&lt;p&gt;There is no primary or required course book, but there are many suitable books, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;&lt;em&gt;Introduction to Linear Algebra&lt;/em&gt;&amp;rdquo; - G. Strang (2016) Wellesley-Cambridge Press, 5&lt;sup&gt;th&lt;/sup&gt; edition ISBN: &lt;span class=&#34;font-mono&#34;&gt;978-09802327-7-6&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other useful resources are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;&lt;em&gt;Linear Algebra and Learning from Data&lt;/em&gt;&amp;rdquo; - G. Strang (2019) Wellesley-Cambridge Press, ISBN: &lt;span class=&#34;font-mono&#34;&gt;978-06921963-8-0&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;&lt;em&gt;Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares&lt;/em&gt;&amp;rdquo; - S. Boyd and L. Vandenberghe (2018) Cambridge University Press, ISBN: &lt;span class=&#34;font-mono&#34;&gt;978-1316518960&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Knowledge, Abilities, or Skills

  * Knowledge of Calculus: functions, inverse functions, sets, real numbers, sequences and limits, polynomials, rational functions, trigonometric functions, logarithm and exponential function, parametric equations, tangent lines, graphs, derivatives, anti-derivatives, elementary techniques for solving equations

  * Knowledge of Linear Algebra: vectors, matrices, lines, planes, $n$-dimensional Euclidean vector space, rotation, translation, dot product (scalar product), cross product, normal vectors, eigenvalues, eigenvectors, elementary techniques for solving systems of linear equations

  * Some examples will be presented as python notebooks, but no knowledge of python is required, nor will there be any assessment of ability to code.

  * Some examples will be presented as [python notebooks](/docs/gradcalclinalg24/notebooks/), but no knowledge of python is required, nor will there be any assessment of ability to code.
--&gt;
&lt;h2 id=&#34;usability-and-relationship-to-other-modules&#34;&gt;Usability and Relationship to other Modules&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The module is a non-mandatory remedial module of the Data Engineering MSc and an elective module Data Science for Society and Business MSc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This module introduces and refreshes the essential calculus and linear algebra required in most of the modules of the data engineering program.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There is a placement test offered in the orientation week before the start of the first semester to help all students to find out if they need to take this course.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;course-outline&#34;&gt;Course Outline&lt;/h2&gt;
&lt;p&gt;The following topics will be covered:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vectors: addition of vectors and multiplication by a scalar, linear combinations, lengths and dot products&lt;/li&gt;
&lt;li&gt;Matrices&lt;/li&gt;
&lt;li&gt;Vector spaces, subspaces, column spaces, span, linear independence, basis. The nullspace of a matrix&lt;/li&gt;
&lt;li&gt;Basis of the nullspace and the column space via Gaussian elimination. Free columns and pivot columns. Solutions of systems of linear equations&lt;/li&gt;
&lt;li&gt;Rank of a matrix. Sum of the dimensions of the nullspace and the column space. Multiplication of matrices&lt;/li&gt;
&lt;li&gt;Inverse of a matrix. Finding the inverse using Gaussian elimination&lt;/li&gt;
&lt;li&gt;Orthogonality. Orthonormal bases, projections, and Gram-Schmidt&lt;/li&gt;
&lt;li&gt;Determinants&lt;/li&gt;
&lt;li&gt;Eigenvalues and eigenvectors&lt;/li&gt;
&lt;li&gt;Diagonalization of a matrix&lt;/li&gt;
&lt;li&gt;Symmetric matrices.&lt;/li&gt;
&lt;li&gt;Principal Component Analysis&lt;/li&gt;
&lt;li&gt;Derivatives, tangent lines, higher order derivatives, curve sketching.&lt;/li&gt;
&lt;li&gt;Taylor series&lt;/li&gt;
&lt;li&gt;Functions of several variables. Partial derivatives&lt;/li&gt;
&lt;li&gt;Optimization problems. Positive/negative definite matrices&lt;/li&gt;
&lt;li&gt;Jacobians and Hessians&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;itinerary&#34;&gt;Itinerary&lt;/h3&gt;
&lt;!-- todayMarker  stroke-width:3px, stroke:#164e63, opacity:0.5 --&gt;
&lt;p&gt;Provisionally, the course will run as follows&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;gantt
  dateFormat   MM-DD
  axisFormat   %e-%b

  todayMarker  off

  title Provisional Schedule 2024

  section  Linear Algebra
  Vectors                          :a1a,  09-02, 2w
  Assignment 1 : milestone, done, m1, 09-18, 0w
  Matrices                         :a1b,  09-16, 1w
  Linear equations                 :a1c,  09-23, 1w
  Assignment 2 : milestone, done, m2, 10-02, 0w
  Vector Spaces                    :a1d,  09-30, 1w
  Break                            :crit, 10-07, 1w
  Vector Spaces                    :a1e,  10-14, 1w
  Assignment 3 : milestone, m3, 10-21, 1d
  Matrices &amp; Eigenvalues           :a1f,  10-21, 3w
  Assignment 4 : milestone, active, m4, 11-01, 0d
  Assignment 5 : milestone, active, m5, 11-12, 0w

  section  Calculus
  Single Variables                 :a2c,  11-11, 1w
  Taylor Series                    :a2d,  11-18, 1w
  Multiple Variables               :a2e,  11-25, 1w
  Assignment 6 : milestone, active, m6, 11-23, 0w

  section  Summary
  Summary                          :active, a3a,  12-02, 1w
&lt;/div&gt;
&lt;h3 id=&#34;structure&#34;&gt;Structure&lt;/h3&gt;
&lt;p&gt;The course content has the following structure:&lt;/p&gt;
&lt;div class=&#34;markmap&#34; style=&#34;height: 500px;&#34;&gt;

&lt;pre&gt;---
markmap:
  zoom: false
  pan: false
---
- Linear Algebra
  - [Vectors](/docs/gradcalclinalg24/part1/vectors/)
  - [Matrices](/docs/gradcalclinalg24/part1/matrices/)
  - [Linear Equations](/docs/gradcalclinalg24/part2/linear-equations/)
  - [Vector Spaces](/docs/gradcalclinalg24/part1/spaces/)
  - [Matrices &amp; Eigenvalues](/docs/gradcalclinalg24/part1/eigen/)
- Calculus
  - [Single Variable Calculus](/docs/gradcalclinalg24/part2/one-variable/)
  - [Taylor Series](/docs/gradcalclinalg24/part2/taylor/)
  - [Many Variable Calculus](/docs/gradcalclinalg24/part2/many-variables/)&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&#34;assessment&#34;&gt;Assessment&lt;/h2&gt;
&lt;h3 id=&#34;examination-type&#34;&gt;Examination Type&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Examination Type:&lt;/td&gt;
          &lt;td&gt;Module examination&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Assessment Type:&lt;/td&gt;
          &lt;td&gt;Written examination&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Scope:&lt;/td&gt;
          &lt;td&gt;All intended learning outcomes of this module&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Duration:&lt;/td&gt;
          &lt;td&gt;120 min&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Weight:&lt;/td&gt;
          &lt;td&gt;100%&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You have three attempts to pass the module. Once you pass the module, no further retakes of the exam are possible. (See &lt;a href=&#34;https://constructor.university/student-life/student-services/university-policies/academic-policies&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic Policies&lt;/a&gt; for more details.) The exams in this course are offered twice per year: in December and January.&lt;/p&gt;
&lt;!-- 




  
    
  

&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;The exam will take place on &lt;strong&gt;Wednesday 11 January between 12:30 and 14:30 at East Hall 8&lt;/strong&gt;&lt;/span&gt;
&lt;/div&gt; --&gt;
&lt;p&gt;No supplementary material can be brought to the exam. A calculator is necessary. Graphical and scientific calculators are permitted.&lt;/p&gt;
&lt;p&gt;The pass mark is 45%.&lt;/p&gt;
&lt;h3 id=&#34;assignments&#34;&gt;Assignments&lt;/h3&gt;
&lt;p&gt;By submitting homework assignments, via &lt;em&gt;Teams&lt;/em&gt; as a single pdf, you can improve this grade by up to 0.66 points, as bonus achievements.&lt;/p&gt;
&lt;p&gt;Homework submission is voluntary although highly recommended. It is possible to get a 100% final grade without submitting homework or participating in quizzes.&lt;/p&gt;
&lt;p&gt;Homework will be assigned every two weeks. Homework assignments are posted on &lt;a href=&#34;https://djps.github.io/docs/gradcalclinalg24/assignments/&#34;&gt;here&lt;/a&gt; and on Teams approximately ten days before the due date.&lt;/p&gt;
&lt;p&gt;You are encouraged to discuss homework between each other. However, the submitted assignments should be written individually. No copying is allowed!&lt;/p&gt;
&lt;p&gt;The two lowest homework scores will be discarded before the final homework score is calculated. This rule covers short illness, excursions, late joining of the course, and similar situations.&lt;/p&gt;
&lt;p&gt;Note that each homework assignment carries equal marks.&lt;/p&gt;
&lt;p&gt;The problems on the final exam will be similar to the ones from homework. So, by doing homework you prepare for the final exam - maths is not a spectator sport.&lt;/p&gt;
&lt;h2 id=&#34;academic-integrity&#34;&gt;Academic Integrity&lt;/h2&gt;
&lt;p&gt;All involved parties (lecturers, instructors and students) are expected to abide by the word and spirit of the &lt;a href=&#34;https://constructor.university/student-life/student-services/university-policies/academic-policies/code-of-academic-integrity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Code of Academic Integrity&amp;rdquo;&lt;/a&gt;. Violations of the Code should be brought to the attention of the Academic Integrity Committee.&lt;/p&gt;
&lt;h2 id=&#34;artifical-intelligence-use-policy&#34;&gt;Artifical Intelligence Use Policy&lt;/h2&gt;
&lt;p&gt;This policy covers any generative AI tool, such as ChatGPT, Elicit, etc. This includes text, slides, artwork/graphics/video/audio and other products.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You are discouraged from using AI tools &lt;em&gt;unless&lt;/em&gt; under direct instruction from your instructor to do so. Please contact your instructor if you are unsure or have questions &lt;em&gt;before&lt;/em&gt; using AI for any assignment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note that the material generated by these programs may be inaccurate, incomplete, or otherwise problematic.  Their use may also stifle your own independent thinking and creativity. Accordingly, reduction in the grade is likely when using AI.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If any part of this AI policy is confusing or uncertain, please reach out to your instructor for a conversation before submitting your work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vectors</title>
      <link>https://djps.github.io/docs/gradcalclinalg24/part1/vectors/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/docs/gradcalclinalg24/part1/vectors/</guid>
      <description>&lt;h2 id=&#34;notation&#34;&gt;Notation&lt;/h2&gt;
&lt;p&gt;The set of all real numbers is denoted as $\mathbb{R}$ and the set of all vectors with $N$ coordinates, whose entries are real numbers, is denoted by $\mathbb{R}^N$.&lt;/p&gt;
&lt;p&gt;Vectors are written as by $\boldsymbol{v}$ or $\vec{v}$.&lt;/p&gt;
&lt;p&gt;The vector whose every entry is zero is called a zero vector.&lt;/p&gt;
&lt;p&gt;But note that if $\boldsymbol{v} = \boldsymbol{0} \in \mathbb{R}^2$ and $\boldsymbol{u} = \boldsymbol{0} \in \mathbb{R}^3$, then $\boldsymbol{v} \neq \boldsymbol{u}$, nor can arithmetic operations, such as addition or subtraction be performed.&lt;/p&gt;
&lt;!--




&lt;div id=&#34;chart-683524791&#34; class=&#34;chart&#34;&gt;&lt;/div&gt;
&lt;script&gt;
  async function fetchChartJSON() {
    console.debug(&#39;Hugo Blox fetching chart JSON...&#39;)
    const response = await fetch(&#39;\/plotly\/test.json&#39;);
    return await response.json();
  }

  (function() {
    let a = setInterval( function() {
      if ( typeof window.Plotly === &#39;undefined&#39; ) {
        console.debug(&#39;Plotly not loaded yet...&#39;)
        return;
      }
      clearInterval( a );

      fetchChartJSON().then(chart =&gt; {
        console.debug(&#39;Plotting chart...&#39;)
        window.Plotly.newPlot(&#39;chart-683524791&#39;, chart.data, chart.layout, {responsive: true});
      });
    }, 500 );
  })();
&lt;/script&gt;


&lt;figure&gt;&lt;img src=&#34;https://djps.github.io/img/test.svg&#34;&gt;
&lt;/figure&gt;

--&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://djps.github.io/img/vector_addition.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Linear Combinations&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
\begin{equation*}
a_1 \vec{v}_1 + a_2 \vec{v}_2 + \ldots + a_k \vec{v}_k
\end{equation*}
$$&lt;p&gt;
where $a_1, a_2, \ldots, a_k \in \mathbb{R}$, i.e. are scalars.&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Dot Product&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
\begin{equation*}
\boldsymbol{a} \cdot \boldsymbol{b} = a_1 b_1 + a_2 b_2 + \ldots a_n b_n.
\end{equation*}
$$&lt;p&gt;
The result is a scalar.&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;p&gt;The dot product can be generalised for more general vectors, called an &lt;strong&gt;inner product&lt;/strong&gt;, written as $\langle \cdot, \cdot \rangle=c$, where $c$ is scalar.&lt;/p&gt;
&lt;p&gt;The length of a vector is denoted by $\left| \vec{v} \right| = \sqrt{\vec{v} \cdot \vec{v}}$.&lt;/p&gt;
&lt;p&gt;A unit vector has length one. A non-zero vector can be transformed into a unit vector by the transformation $\vec{v}  \mapsto \vec{v} / \left| \vec{v} \right|$. This process is often called normalization, as the norm of a vector is often defined as $\sqrt{\langle \vec{v}, \vec{v} \rangle}$.&lt;/p&gt;
$$
\begin{equation*}
\boldsymbol{a} \cdot \boldsymbol{b} = \left| \boldsymbol{a} \right|  \left| \boldsymbol{b} \right|  \cos\theta.
\end{equation*}
$$&lt;p&gt;
Thus, any two vectors, $\vec{v}$ and $\vec{u}$ are said to be &lt;strong&gt;orthogonal&lt;/strong&gt; (or perpendicular) if $\vec{v} \cdot \vec{u}=0$.&lt;/p&gt;
$$
\begin{equation*}
 \cos\theta = \dfrac{\boldsymbol{a} \cdot \boldsymbol{b}}{\left| \boldsymbol{a} \right|  \left| \boldsymbol{b} \right|}.
\end{equation*}
$$&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Cauchy Schwarz Inequality&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
\left| \vec{v} \cdot \vec{w} \right| \leq \left| \vec{v} \right| \left| \vec{w} \right|.
$$
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Triangle Inequality&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
\left| \vec{v} + \vec{w} \right| \leq \left| \vec{v} \right| + \left| \vec{w} \right|.
$$&lt;p&gt;
It can be derived from the Cauchy-Schwarz inequality.&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;p&gt;For the Cauchy-Schwarz inequality, when the vectors $\vec{v}$ and $\vec{w}$ lie on the same line, then $\left| \vec{v} \cdot \vec{w} \right| = \left| \vec{v} \right| \left| \vec{w} \right|$.&lt;/p&gt;
&lt;p&gt;For the triangle inequality, when the vectors point in the same direction, then the $\left| \vec{v} + \vec{w} \right| = \left| \vec{v} \right| + \left| \vec{w} \right|$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Matrices</title>
      <link>https://djps.github.io/docs/gradcalclinalg24/part1/matrices/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/docs/gradcalclinalg24/part1/matrices/</guid>
      <description>&lt;p&gt;A matrix is a rectangular array of numbers.&lt;/p&gt;
&lt;p&gt;Matrices are usually denoted by uppercase letters.&lt;/p&gt;
&lt;p&gt;A matrix, $A$, with $m$ rows and ${n}$ columns, is referred to as an ${m\times n}$ matrix.&lt;/p&gt;
&lt;p&gt;If all the entries are real numbers, the matrix is a member of the set of all real matrices with  $m$ rows and ${n}$ columns, i.e. $A \in\mathbb{R}^{m \times n}$.&lt;/p&gt;
&lt;p&gt;The entry in the $i$&lt;sup&gt;th&lt;/sup&gt; row and $j$&lt;sup&gt;th&lt;/sup&gt; column of the matrix $A \in \mathrm{R}^{m \times n}$ is denoted by $a_{ij}$ or $a_{i,j}$. Hence, the matrix is written as&lt;/p&gt;
$$
\begin{align*}
A = \left(
  \begin{array}{cccc}
  a_{1,1} &amp; a_{1,2} &amp; \cdots &amp; a_{1,n} \\
  a_{2,1} &amp; a_{2,2} &amp; \cdots &amp; a_{2,n} \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  a_{m,1} &amp; a_{m,2} &amp; \cdots &amp; a_{m,n}
  \end{array}
  \right).
\end{align*}
$$&lt;p&gt;A matrix is said to be a &lt;strong&gt;square matrix&lt;/strong&gt; if the number of rows is equal to the number of columns, i.e. $n=m$.&lt;/p&gt;
&lt;h2 id=&#34;matrix-operations&#34;&gt;Matrix Operations&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Matrix Matrix Multiplication&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
c_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj}
$$&lt;p&gt;
for any $i=1, \ldots, m$ and $j =1, \ldots, l$.&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;p&gt;If $A$ is an $m\times n$ matrix and $B$ is a $n\times l$ matrix, then $A$ has the same number of columns as $B$ has rows, i.e. $n$. Thus, each entry $c_{ij}$ is the dot product of the $i$&lt;sup&gt;th&lt;/sup&gt; row of $A$ with the $j$&lt;sup&gt;th&lt;/sup&gt; column of $B$, which both have length $n$.&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Matrix Vector Multiplication&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
y_{j} = \sum_{i=1}^{n} a_{ij} x_i .
$$
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Row and Column Vectors&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            When matrix $A \in \mathbb{R}^{m \times 1}$ is comprised of a single column of $m$ entries, it is called a column vector. Similarly, a $1 \times n$ matrix, i.e. a single row of $n$ entries is called a row vector.
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;p&gt;Thus $\boldsymbol{v}^T \boldsymbol{v}$ is a $1 \times 1$ matrix, and $\boldsymbol{v} \boldsymbol{v}^T$ is a $n \times n$ matrix.&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Transpose&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            For a matrix $A \in \mathbb{R}^{m \times n}$, the transpose of the matrix $A^T \in \mathbb{R}^{n \times m}$ where all elements are mapped to $a_{ij} = a_{ji}$.
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;p&gt;Thus, if $\boldsymbol{x}$ is a column vector it is possible to perform $A\boldsymbol{x}=\boldsymbol{y}$, then the equivalent multiplication by a row vector is $\boldsymbol{x}^T A^T = \boldsymbol{y}^T$.&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Trace&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
\text{tr}\left(A \right) = \sum_{i=0}^{n} a_{ii}.
$$
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;h3 id=&#34;properties-of-matrix-operations&#34;&gt;Properties of Matrix Operations&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;$A+B = B+A$&lt;/li&gt;
&lt;li&gt;$c\left(A + B\right) = cA + cB$&lt;/li&gt;
&lt;li&gt;$C\left(A + B\right) = CA + CB$&lt;/li&gt;
&lt;li&gt;$\left(A + B\right)C = AC + BC$&lt;/li&gt;
&lt;li&gt;$AB \neq BA$&lt;/li&gt;
&lt;li&gt;$A + \left(B + C\right) = \left(A+ B\right) + C$&lt;/li&gt;
&lt;li&gt;$A\left( BC \right) = \left(AB\right)C$&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;details admonition Proposition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m16.862 4.487l1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L10.582 16.07a4.5 4.5 0 0 1-1.897 1.13L6 18l.8-2.685a4.5 4.5 0 0 1 1.13-1.897zm0 0L19.5 7.125M18 14v4.75A2.25 2.25 0 0 1 15.75 21H5.25A2.25 2.25 0 0 1 3 18.75V8.25A2.25 2.25 0 0 1 5.25 6H10&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Proposition: &lt;em&gt;&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $\left( A B \right)^T = B^T A^T$
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;h2 id=&#34;useful-types-of-matrices&#34;&gt;Useful Types of Matrices&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;identity matrix&lt;/strong&gt; is the square matrix $I$ such that $AI=A$ and $IA=A$, all entries on the main diagonal are one, all others are zero.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;diagonal matrix&lt;/strong&gt; is a matrix where all entries outside the main diagonal are all zero, i.e. $a_{ij} =0$ when $i \neq j$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;banded&lt;/strong&gt; matrix is a matrix whose non-zero entries are confined to a diagonal band, comprising the main diagonal and zero or more diagonals on either side.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;inverse matrix&lt;/strong&gt; of a square matrix is the matrix $A^{-1}$ such that $AA^{-1}~=~A^{-1}A~=~I.$  A matrix is &lt;strong&gt;non-singular&lt;/strong&gt; or &lt;strong&gt;invertible&lt;/strong&gt; if there exists an inverse matrix exists.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A square matrix has an inverse if and only if the rank of the matrix is equal to the number of columns (or rows), equivalently, the columns are linearly independent. Such matrices are said to have full rank.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A square matrix $A$ is &lt;strong&gt;symmetric&lt;/strong&gt; if ${A=A^T}$, that is, $a_{i,j}=a_{j,i}$ for all indices $i$ and $j$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A square matrix, with complex elements, is said to be &lt;strong&gt;Hermitian&lt;/strong&gt; if the matrix is equal to its &lt;em&gt;conjugate transpose&lt;/em&gt;, i.e. $a_{i,j}=\overline{a_{j,i}}$ for all indices $i$ and $j$. A Hermitian matrix is written as $A=A^H$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An &lt;strong&gt;orthogonal matrix&lt;/strong&gt; $Q$ is a matrix whose columns $\vec{q}_i$ are orthogonal to one another, that is $\vec{q}_i \cdot \vec{q}_j = 0$ for $i \neq j$ and have unit length, i.e. $\left\| \vec{q}_i \right\| = 1$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;details admonition Proposition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m16.862 4.487l1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L10.582 16.07a4.5 4.5 0 0 1-1.897 1.13L6 18l.8-2.685a4.5 4.5 0 0 1 1.13-1.897zm0 0L19.5 7.125M18 14v4.75A2.25 2.25 0 0 1 15.75 21H5.25A2.25 2.25 0 0 1 3 18.75V8.25A2.25 2.25 0 0 1 5.25 6H10&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Proposition: &lt;em&gt;&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            The inverse of an orthogonal matrix $Q$ is equal to its transpose, i.e. $Q^{-1} = Q^T$.
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;Markov&lt;/strong&gt; matrix has positive entries and every column sums to one.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;details admonition Proposition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m16.862 4.487l1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L10.582 16.07a4.5 4.5 0 0 1-1.897 1.13L6 18l.8-2.685a4.5 4.5 0 0 1 1.13-1.897zm0 0L19.5 7.125M18 14v4.75A2.25 2.25 0 0 1 15.75 21H5.25A2.25 2.25 0 0 1 3 18.75V8.25A2.25 2.25 0 0 1 5.25 6H10&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Proposition: &lt;em&gt;&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            The largest &lt;a href=&#34;https://djps.github.io/docs/gradcalclinalg24/part1/eigen/#eigenvalues--eigenvectors)&#34;&gt;eigenvalue&lt;/a&gt; of a Markov matrix is one.
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;permutation matrix&lt;/strong&gt; is a square matrix that has exactly one entry of $1$ in each row and each column and all other entries $0$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Rotation matrices&lt;/strong&gt; describe rotations by an angle about the axes of a coordinate system. They can be denoted by $R_x ( \alpha )$, i.e. rotation by angle $\theta$ about the $x$ axis. The product of two rotation matrices is also a rotation matrix. Note that in general $R_x\left( \theta \right) R_y \left( \beta \right) \neq R_y \left( \beta \right) R_x\left( \theta \right)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reflection matrices&lt;/strong&gt;: $R = I - 2 \boldsymbol{u} \boldsymbol{u}^T$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A square matrix is said to be &lt;strong&gt;lower triangular matrix&lt;/strong&gt; if all the elements above the main diagonal are zero, i.e. $a_{ij}=0$ when $i &lt; j$. Similarly, a matrix is said to be &lt;strong&gt;upper triangular&lt;/strong&gt; if all the entries below the main diagonal are zero, that is $a_{ij}=0$ when $i~&gt;~j$. A matrix is said to be strictly upper triangular (or strictly lower triangle) if the main diagonal is also zero.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;determinant-of-a-matrix&#34;&gt;Determinant of a Matrix&lt;/h2&gt;
$$A= \left( \begin{array}{cc} a &amp; b \\ c &amp; d \end{array} \right)$$&lt;p&gt;, the determinant, denoted by $\left| A \right|$ or det$A$ is given by ${ad-bc}$.&lt;/p&gt;
&lt;p&gt;There are many methods to compute the determinant. the&lt;/p&gt;
&lt;p&gt;Leibniz Method: Consider all possible choices of $n$ elements from a matrix such that there is precisely one element chosen from each row and column. Let such a choice be denoted by $\sigma$ and let sgn$\sigma$ be $-1$ to the power of the number of row swaps required to turn the choice $\sigma$ into the diagonal. Then det$\sigma$ is the sum of the products of each $\sigma$ multiplied by its sign.&lt;/p&gt;
$$
C_{ij} = \left( -1 \right)^{i+j} \text{det} M_{ij}
$$$$
\text{det}{A} = a_{i1}C_{i1} + a_{i2}C_{i2} + \ldots + a_{in}C_{in}
$$$$
\text{det}{A} = a_{1j}C_{1j} + a_{2j}C_{2j} + \ldots + a_{nj}C_{nj}.
$$$$
x_j = \dfrac{\text{det} B_j}{\text{det} A}.
$$$$
\left( A^{-1} \right)_{ij} = \dfrac{ C_{ji} }{\text{det} A}.
$$&lt;h2 id=&#34;properties-of-nonsingular-matrices&#34;&gt;Properties of Nonsingular Matrices&lt;/h2&gt;
&lt;p&gt;For a nonsingular matrix, the following &lt;em&gt;all&lt;/em&gt; hold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A nonsingular matrices has full rank, i.e. the rank is equal to the number of rows or columns&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A square matrix is nonsingular if and only if the determinant of the matrix is non-zero.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If a matrix is singular, both versions of Gaussian elimination (with and without pivoting) will fail due to division by zero, yielding a floating exception error. Another way to understand this is that the number of pivots is equal to the rank, so if the matrix does not have full rank, so there will not be enough pivots in order to transform the matrix in to row-echelon form.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Linear Equations</title>
      <link>https://djps.github.io/docs/gradcalclinalg24/part1/linear-equations/</link>
      <pubDate>Mon, 20 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/docs/gradcalclinalg24/part1/linear-equations/</guid>
      <description>&lt;h2 id=&#34;linear-equations&#34;&gt;Linear Equations&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Systems of Linear Equations&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            &lt;p&gt;A system of linear equations (or linear system) is a collection of one or more linear equations involving the same variables. If there are $m$ equations with $n$ unknown variables to solve for&lt;/p&gt;
$$
\begin{align*}
a_{1,1} x_1 + a_{1,2} x_2 + \ldots + a_{1,n} x_n &amp; = b_1 \\
a_{2,1} x_1 + a_{2,2} x_2 + \cdots + a_{2,n} x_n &amp; = b_2 \\
\vdots &amp;  \\
a_{m,1} x_1 + a_{m,2} x_2 + \cdots + a_{m,n} x_n &amp; = b_m.
\end{align*}
$$&lt;p&gt;The system of linear equations can be written in matrix form ${A\boldsymbol{x}=\boldsymbol{b}}$, where&lt;/p&gt;
$$
\begin{align*}
A = \left(
  \begin{array}{cccc}
  a_{1,1} &amp; a_{1,2} &amp; \cdots &amp; a_{1,n} \\
  a_{2,1} &amp; a_{2,2} &amp; \cdots &amp; a_{2,n} \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  a_{m,1} &amp; a_{m,2} &amp; \cdots &amp; a_{m,n}
  \end{array}
  \right), \quad x = \left(
    \begin{array}{c}
    x_1 \\\
    x_2 \\\
    \vdots \\\
    x_n
    \end{array}
    \right), \quad b = \left(
      \begin{array}{c}
      b_1 \\
      b_2 \\
      \vdots \\
      b_m
      \end{array}
      \right),
\end{align*}
$$&lt;p&gt;
so that $A \in \mathbb{R}^{m \times n}$, $\boldsymbol{x}= \in \mathbb{R}^n$ and $\boldsymbol{b}= \in \mathbb{R}^m$.&lt;/p&gt;

        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;!--
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            If $\tilde{x}$ is an approximate solution to the linear problem ${Ax=b}$, then the &lt;strong&gt;residual&lt;/strong&gt; is defined as ${r = A \tilde{x}-b}$.
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




--&gt;
&lt;h3 id=&#34;direct-methods&#34;&gt;Direct Methods&lt;/h3&gt;
&lt;div class=&#34;details admonition Algorithm open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 640 512&#34;&gt;&lt;path stroke=&#34;currentColor&#34; d=&#34;M64 96c0-35.3 28.7-64 64-64H512c35.3 0 64 28.7 64 64V352H512V96H128V352H64V96zM0 403.2C0 392.6 8.6 384 19.2 384H620.8c10.6 0 19.2 8.6 19.2 19.2c0 42.4-34.4 76.8-76.8 76.8H76.8C34.4 480 0 445.6 0 403.2zM281 209l-31 31 31 31c9.4 9.4 9.4 24.6 0 33.9s-24.6 9.4-33.9 0l-48-48c-9.4-9.4-9.4-24.6 0-33.9l48-48c9.4-9.4 24.6-9.4 33.9 0s9.4 24.6 0 33.9zM393 175l48 48c9.4 9.4 9.4 24.6 0 33.9l-48 48c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l31-31-31-31c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0z&#34;/&gt;&lt;/svg&gt;

        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Algorithm: &lt;em&gt;Gaussian Elimination&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            &lt;p&gt;Gaussian elimination is a method to solve systems of linear equations based on forward elimination (a series of row-wise operations) to convert the matrix, $A$, to upper triangular form (echelon form), and then back substitution to solve the system. The row operations are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;row swapping&lt;/li&gt;
&lt;li&gt;row scaling, i.e. multiplying by a non-zero scalar&lt;/li&gt;
&lt;li&gt;row addition, i.e. adding a multiple of one row to another&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Forward elimination is written as&lt;/p&gt;
&lt;p&gt;&lt;small&gt;1 &lt;/small&gt; For $k=1$ to $n-1$: &lt;br&gt;
&lt;small&gt;2 &lt;/small&gt; &lt;span style=&#34;margin-left: 10%;&#34;&gt; For $i = k + 1$ to $n$: &lt;/span&gt;&lt;br&gt;
&lt;small&gt;3 &lt;/small&gt; &lt;span style=&#34;margin-left: 20%;&#34;&gt; For $j = k$ to $n$: &lt;/span&gt;&lt;br&gt;
&lt;small&gt;4 &lt;/small&gt; &lt;span style=&#34;margin-left: 30%;&#34;&gt; $a_{i,j} = a_{i,j} - \dfrac{a_{i,k}}{a_{k,k}} a_{k,j}$ &lt;/span&gt;&lt;br&gt;
&lt;small&gt;5 &lt;/small&gt; &lt;span style=&#34;margin-left: 20%;&#34;&gt; End &lt;/span&gt;&lt;br&gt;
&lt;small&gt;6 &lt;/small&gt; &lt;span style=&#34;margin-left: 10%;&#34;&gt; $b_{i} = b_{i} - \dfrac{a_{i,k}}{a_{k,k}} b_{k}$ &lt;/span&gt;&lt;br&gt;
&lt;small&gt;7 &lt;/small&gt; End &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Then back substitute, starting with the last unknown first:&lt;/p&gt;
&lt;p&gt;&lt;small&gt;8 &lt;/small&gt; Set $x_n = \dfrac{b_n}{a_{n,n}}$ &lt;br&gt;
&lt;small&gt;9 &lt;/small&gt; For $i$ =$n-1$ to $1$: &lt;br&gt;
&lt;small&gt;10&lt;/small&gt; &lt;span style=&#34;margin-left: 10%;&#34;&gt; $y = b_i$ &lt;/span&gt;&lt;br&gt;
&lt;small&gt;11&lt;/small&gt; &lt;span style=&#34;margin-left: 10%;&#34;&gt; For $j = n$ to $i+1$: &lt;/span&gt;&lt;br&gt;
&lt;small&gt;12&lt;/small&gt; &lt;span style=&#34;margin-left: 20%;&#34;&gt; $y = y - a_{i,j} x_j$ &lt;/span&gt;&lt;br&gt;
&lt;small&gt;13&lt;/small&gt; &lt;span style=&#34;margin-left: 10%;&#34;&gt; End &lt;/span&gt;&lt;br&gt;
&lt;small&gt;14&lt;/small&gt; &lt;span style=&#34;margin-left: 10%;&#34;&gt; $x_i = \dfrac{y}{a_{i,i}}$ &lt;/span&gt;&lt;br&gt;
&lt;small&gt;15&lt;/small&gt; End&lt;/p&gt;

        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Algorithm open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 640 512&#34;&gt;&lt;path stroke=&#34;currentColor&#34; d=&#34;M64 96c0-35.3 28.7-64 64-64H512c35.3 0 64 28.7 64 64V352H512V96H128V352H64V96zM0 403.2C0 392.6 8.6 384 19.2 384H620.8c10.6 0 19.2 8.6 19.2 19.2c0 42.4-34.4 76.8-76.8 76.8H76.8C34.4 480 0 445.6 0 403.2zM281 209l-31 31 31 31c9.4 9.4 9.4 24.6 0 33.9s-24.6 9.4-33.9 0l-48-48c-9.4-9.4-9.4-24.6 0-33.9l48-48c9.4-9.4 24.6-9.4 33.9 0s9.4 24.6 0 33.9zM393 175l48 48c9.4 9.4 9.4 24.6 0 33.9l-48 48c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l31-31-31-31c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0z&#34;/&gt;&lt;/svg&gt;

        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Algorithm: &lt;em&gt;Gaussian Elimination with Scaled Partial Pivoting&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            &lt;p&gt;A pivot element is the element of a matrix, $A$, which is selected to do certain calculations first. Pivoting helps reduce errors due to rounding.&lt;/p&gt;
&lt;p&gt;&lt;small&gt;1 &lt;/small&gt; Find maximal absolute values vector $\boldsymbol{s}$ with entries $s_i= \max_{j=1, \ldots, n} \left| a_{i,j} \right|$ &lt;br&gt;
&lt;small&gt;2 &lt;/small&gt; For $k=1$ to $n-1$: &lt;br&gt;
&lt;small&gt;3 &lt;/small&gt; &lt;span style=&#34;margin-left: 10%;&#34;&gt; For $i = k$ to $n$: &lt;/span&gt; &lt;br&gt;
&lt;small&gt;4 &lt;/small&gt; &lt;span style=&#34;margin-left: 20%;&#34;&gt; Compute $\left| a_{i,k} / s_i \right|$ &lt;/span&gt; &lt;br&gt;
&lt;small&gt;5 &lt;/small&gt; &lt;span style=&#34;margin-left: 10%;&#34;&gt; End &lt;/span&gt; &lt;br&gt;
&lt;small&gt;6 &lt;/small&gt; &lt;span style=&#34;margin-left: 10%;&#34;&gt; Find the row with the largest relative pivot element and denote this as row $j$ &lt;/span&gt; &lt;br&gt;
&lt;small&gt;7 &lt;/small&gt; &lt;span style=&#34;margin-left: 10%;&#34;&gt; Swap rows $k$ and $j$ &lt;/span&gt; &lt;br&gt;
&lt;small&gt;8 &lt;/small&gt; &lt;span style=&#34;margin-left: 10%;&#34;&gt; Swap entries $k$ and $j$ in vector $\boldsymbol{s}$ &lt;/span&gt; &lt;br&gt;
&lt;small&gt;9 &lt;/small&gt; &lt;span style=&#34;margin-left: 10%;&#34;&gt; Do forward elimination on row $k$ &lt;/span&gt; &lt;br&gt;
&lt;small&gt;10&lt;/small&gt; End &lt;br&gt;&lt;/p&gt;
&lt;p&gt;The matrix will now be in row-echelon form, so that back substitution can be performed.&lt;/p&gt;

        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Theorem open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M8.25 6.75h12M8.25 12h12m-12 5.25h12M3.75 6.75h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0M3.75 12h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0m-.375 5.25h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Theorem: &lt;em&gt;$LU$-Decomposition&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
L = U_1^{-1} U_2^{-1} \cdots U_{n-1}^{-1}
$$$$
U = U_{n-1} \cdots U_2 U_1 A.
$$
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;!--

### Fundamental Theorem of Numerical Analysis


&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Stable&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            &lt;p&gt;A numerical method is said to be &lt;strong&gt;stable&lt;/strong&gt; if and only if any initial error $e_0$ is damped during the iterations, i.e. ${\left\| e_k \right\| &lt; \left\| e_0 \right\| }$.&lt;/p&gt;
&lt;p&gt;Note that $\Vert x \Vert $ is a &lt;em&gt;norm&lt;/em&gt; of a vector, such as ${\Vert x \Vert_2 = \sqrt{x_0^2 + x_1^2 + \ldots + x_n^2} }$.&lt;/p&gt;

        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;






&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Consistent&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            A numerical method is said to be &lt;strong&gt;consistent&lt;/strong&gt; if any fixed point $x^{\ast}$ of the iteration is a solution to the problem being solved.
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;






For linear systems, a fixed point, $x^{\ast}$, fulfils
$$
x^{\ast} = \left( I - Q^{-1} A \right) x^{\ast} + Q^{-1} b \Leftrightarrow A x^{\ast} = b.
$$

If the iterative method for a linear system is stable then

$$
e_k = \left( I - Q^{-1} A \right)^k e_0,
$$
so then ${\Vert I - Q^{-1} A \Vert &lt; 1}$ for ${\Vert e_k \Vert &lt; \Vert e_0 \Vert}$, where ${\Vert I - Q^{-1} A \Vert}$ is a _matrix norm_.


&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Convergent&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            A numerical method is said to be &lt;strong&gt;convergent&lt;/strong&gt; if ${x_k \rightarrow x^{\ast}}$ as ${k \rightarrow \infty}$ where $x^{\ast}$ is the exact solution.
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;






&lt;div class=&#34;details admonition Theorem open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M8.25 6.75h12M8.25 12h12m-12 5.25h12M3.75 6.75h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0M3.75 12h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0m-.375 5.25h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Theorem: &lt;em&gt;Fundamental Theorem of Numerical Analysis&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            A numerical method is convergent if and only if it is consistent and stable.
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;






&lt;div class=&#34;details admonition Example open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M15.75 15.75V18m-7.5-6.75h.008v.008H8.25zm0 2.25h.008v.008H8.25zm0 2.25h.008v.008H8.25zm0 2.25h.008v.008H8.25zm2.498-6.75h.007v.008h-.007zm0 2.25h.007v.008h-.007zm0 2.25h.007v.008h-.007zm0 2.25h.007v.008h-.007zm2.504-6.75h.008v.008h-.008zm0 2.25h.008v.008h-.008zm0 2.25h.008v.008h-.008zm0 2.25h.008v.008h-.008zm2.498-6.75h.008v.008h-.008zm0 2.25h.008v.008h-.008zM8.25 6h7.5v2.25h-7.5zM12 2.25c-1.892 0-3.758.11-5.593.322C5.307 2.7 4.5 3.65 4.5 4.757V19.5a2.25 2.25 0 0 0 2.25 2.25h10.5a2.25 2.25 0 0 0 2.25-2.25V4.757c0-1.108-.806-2.057-1.907-2.185A48.507 48.507 0 0 0 12 2.25&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Example: &lt;em&gt;&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            &lt;p&gt;An &lt;span class=&#34;mycode&#34;&gt;.ipynb&lt;/span&gt; notebook with an example of iterative solvers for linear systems can be accessed online &lt;a class=&#34;hover:red&#34; href=&#34;https://djps.github.io/docs/numericalmethods/notebooks/iterative/&#34; style=&#34;text-decoration: underline; font-weight: 500; text-decoration-color: #651fff;&#34;&gt;[here]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a class=&#34;underline text-red-600 hover:text-red-800 visited:text-red-600 dark:text-red-300 dark:hover:text-red-200&#34; href=&#34;https://djps.github.io/ipyth/iterative.py/&#34; &gt;[here]&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href=&#34;https://djps.github.io/ipyth/iterative.ipynb&#34;&gt;[here]&lt;/a&gt;.&lt;/p&gt;

        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




--&gt;
</description>
    </item>
    
    <item>
      <title>Vector Spaces</title>
      <link>https://djps.github.io/docs/gradcalclinalg24/part1/spaces/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/docs/gradcalclinalg24/part1/spaces/</guid>
      <description>&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Linear Independence&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            &lt;p&gt;A set of vectors, $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n$ are linearly independent if none of the vectors can be expressed as a linear combination of the remaining $n-1$ vectors.&lt;/p&gt;
&lt;p&gt;An alternative definition is that if $c_1 \vec{v}_1 + c_2 \vec{v}_2  + \ldots c_n \vec{v}_n = \vec{0}$, then the only set of values for $c_i$ which satisfies this, are $c_1 = c_2 = \ldots = c_n =0$. That is, if the column vectors of $A$ are linearly independent, then the only solution to $A\vec{x}=\vec{0}$ is $\vec{x} =\vec{0}$.&lt;/p&gt;

        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;p&gt;Thus a matrix $A$ has linearly independent columns if and only if the equation $A\vec{x} = \vec{0}$ has exactly one solution.&lt;/p&gt;
&lt;p&gt;If the columns of $A$ are not linearly independent, then $A\vec{x} =\vec{0}$ will have infinitely many solutions.&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Vector Spaces&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            &lt;p&gt;A vector space is a set $V$ with two operations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Addition of the elements of $V$, i.e. if $\vec{v}$, $\vec{u} \in V$, then $\vec{v} + \vec{u} \in V$&lt;/li&gt;
&lt;li&gt;Multiplication of the elements of $V$ by a scalar, i.e. if $\vec{v} \in V$, and $\alpha \in \mathbb{R}$ then $\alpha\vec{v}~\in~V$,&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;which satisfies the following conditions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\vec{v} + \vec{u} = \vec{u} + \vec{v}$&lt;/li&gt;
&lt;li&gt;$\vec{u} + \left(\vec{v} +  \vec{w} \right) =\left(\vec{u} + \vec{v}\right) + \vec{w}$&lt;/li&gt;
&lt;li&gt;There exists a vector $\vec{0} \in V$ such that $ \vec{u} + \vec{0} = \vec{u}$ for all $\vec{u} \in V$&lt;/li&gt;
&lt;li&gt;There exists a vector $\vec{1} \in V$ such that $ \vec{u}\vec{1} = \vec{u}$ for all $\vec{u} \in V$&lt;/li&gt;
&lt;li&gt;For any vector $\vec{v} \in V$, there exists a vector $\vec{u} \in V$ such that $\vec{v} + \vec{u} = \vec{0}$, which is denoted as $\vec{u} = - \vec{v}$&lt;/li&gt;
&lt;li&gt;For any $a$, $b \in \mathbb{R}$ and $\vec{v} \in V$,  then $a\left(b \vec{v} \right) = \left( a b\right)\vec{v}$&lt;/li&gt;
&lt;li&gt;$a\left( \vec{v} + \vec{u} \right) = a \vec{v} + a \vec{u}$&lt;/li&gt;
&lt;li&gt;$\left( a + b \right)\vec{v} = a \vec{v} + b \vec{v}$&lt;/li&gt;
&lt;/ol&gt;

        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Subspaces&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            If $V$ is a vector space and $W \subset V$ and $W$ is also a vector space, then it is called a subspace of $V$.
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Span&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            If $\mathcal{A} = \left\{ \boldsymbol{v}_1, \ldots, \boldsymbol{v}_k \right\}$ where each vector $\boldsymbol{v}_i \in\mathbb{R}^n$, then the span of $\mathcal{A}$ is the set of all possible linear combinations of the vectors in $\mathcal{A}$.
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Basis&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            A basis of a vectors space is the maximal collection of linearly independent vectors from that vector space.
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;p&gt;Thus, if you add another vector from the vectors space to the basis set, it will be a linear combination of the vectors from the basis.&lt;/p&gt;
&lt;p&gt;The number of vectors in the basis is the &lt;strong&gt;dimension of the vector space&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Column and Row Spaces&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            The column space of a matrix $A$ is the span of all columns of $A$. Similarly, the row space is the span of the rows of $A$ or the column space of $A^T$.
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Null spaces&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            The null space of a matrix $A$ is the collection of all solutions to $A \boldsymbol{x} = \boldsymbol{0}$.
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Rank&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            The rank of a matrix $A$ is the dimension of the column space of $A$.
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;p&gt;The rank is also the number of pivots in $A$ when performing Gaussian elimination.&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Projections&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
\textrm{proj}_{\boldsymbol{u}} \left(\boldsymbol{v} \right) = \dfrac{\boldsymbol{v} \cdot \boldsymbol{u} }{\boldsymbol{u} \cdot \boldsymbol{u} }\boldsymbol{u}.
$$
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Gram-Schmidt&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
\begin{align*}
\boldsymbol{u_1} &amp; = \boldsymbol{v_1} \\
 &amp; \vdots \\
\boldsymbol{u_k} &amp; = \boldsymbol{v_k} - \sum_{j=1}^{k-1} \textrm{proj}_{\boldsymbol{u_j}} \left(\boldsymbol{v_k} \right).
\end{align*}
$$&lt;p&gt;
The set of vectors $\boldsymbol{u_k}$ are orthogonal. Normalizing the vectors as $\boldsymbol{e_j} = \dfrac{\boldsymbol{u_j}}{ \left\| \boldsymbol{u_j} \right\|}$ is a set of orthornormal vectors.&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Example open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M15.75 15.75V18m-7.5-6.75h.008v.008H8.25zm0 2.25h.008v.008H8.25zm0 2.25h.008v.008H8.25zm0 2.25h.008v.008H8.25zm2.498-6.75h.007v.008h-.007zm0 2.25h.007v.008h-.007zm0 2.25h.007v.008h-.007zm0 2.25h.007v.008h-.007zm2.504-6.75h.008v.008h-.008zm0 2.25h.008v.008h-.008zm0 2.25h.008v.008h-.008zm0 2.25h.008v.008h-.008zm2.498-6.75h.008v.008h-.008zm0 2.25h.008v.008h-.008zM8.25 6h7.5v2.25h-7.5zM12 2.25c-1.892 0-3.758.11-5.593.322C5.307 2.7 4.5 3.65 4.5 4.757V19.5a2.25 2.25 0 0 0 2.25 2.25h10.5a2.25 2.25 0 0 0 2.25-2.25V4.757c0-1.108-.806-2.057-1.907-2.185A48.507 48.507 0 0 0 12 2.25&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Example: &lt;em&gt;&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            &lt;p&gt;An &lt;span class=&#34;mycode&#34;&gt;.ipynb&lt;/span&gt; notebook with an example of the Gram-Schmidt process can be accessed online &lt;a href=&#34;https://djps.github.io/docs/gradcalclinalg24/notebooks/gram_schmidt/&#34;&gt;[here]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/gram_schmidt.py/&#34;&gt;[here]&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href=&#34;https://djps.github.io/ipyth/gram_schmidt.ipynb&#34;&gt;[here]&lt;/a&gt;.&lt;/p&gt;

        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




</description>
    </item>
    
    <item>
      <title>Eigenvalues &amp; Principal Component Analysis</title>
      <link>https://djps.github.io/docs/gradcalclinalg24/part1/eigen/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/docs/gradcalclinalg24/part1/eigen/</guid>
      <description>&lt;h2 id=&#34;eigenvalues--eigenvectors&#34;&gt;Eigenvalues &amp;amp; Eigenvectors&lt;/h2&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Eigenvalues and Eigenvectors&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
A \boldsymbol {v} = \lambda \boldsymbol {v}.
$$&lt;p&gt;
then $\boldsymbol {v}$ is called an eigenvector of $A$, and $\lambda$ is the corresponding eigenvalue. Thus $A\boldsymbol{v}$ and $\boldsymbol{v}$ are &lt;em&gt;collinear&lt;/em&gt;.&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




$$
A \boldsymbol {v} = \lambda \boldsymbol {v} \Rightarrow A \boldsymbol {v} - \lambda \boldsymbol {v} = \boldsymbol{0}
$$$$
B \boldsymbol {v} = \boldsymbol{0} \quad \textsf{where} \quad B = A - \lambda I.
$$&lt;p&gt;
As $\boldsymbol{v}$ is not a zero vector and $B \boldsymbol{v} = \boldsymbol{0}$ then the determinant of $B$ is zero. Finding the roots to $\left| A- \lambda I \right|$ yields the eigenvalues, whose eigenvectors are in the span of the nullspace of $B=A-\lambda I$.&lt;/p&gt;
$$
A \boldsymbol {v} = X^{-1} D X \boldsymbol {v}
$$&lt;p&gt;Thus powers of matrices, such as $A^k$ can be easily computed for any $k$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $A$ is triangular, its eigenvalues are the entries on the diagonal.&lt;/li&gt;
&lt;li&gt;For an arbitrary $n$ by $n$ matrix $A$, the product of the $n$ eigenvalues is equal to the determinant of $A$.&lt;/li&gt;
&lt;li&gt;The sum of the $n$ eigenvalues is equal to the trace of $A$.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M8.25 6.75h12M8.25 12h12m-12 5.25h12M3.75 6.75h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0M3.75 12h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0m-.375 5.25h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Theorem: &lt;em&gt;Eigenvalues and Eigenvectors of Symmetric Matrices&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            All eigenvalues of a symmetric matrix are real and positive. Furthermore, the eigenvectors can be chosen to be pairwise orthogonal.
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;h2 id=&#34;principal-component-analysis&#34;&gt;Principal Component Analysis&lt;/h2&gt;
&lt;p&gt;Principal Component Analysis is a linear transformation of a dataset, $Z$ onto a new coordinate system such that the directions (principal components) capturing the largest variation in the data.&lt;/p&gt;
&lt;p&gt;Shift so that the mean of each column is zero, i.e. subtract the mean of each column of $Z$ from itself, yielding a new matrix $X$.&lt;/p&gt;
&lt;p&gt;$X\boldsymbol{w}$ is the projection of each data row on the direction $w$.&lt;/p&gt;
$$
\begin{align*}
\text{var} \, X &amp; = \dfrac{1}{n-1} \left( x_{1}^{2} + \ldots + x_n^2 \right) \\
&amp; = \dfrac{1}{n-1} \left( X \boldsymbol{w} \right)^T \left( X \boldsymbol{w} \right) \\
&amp; = \dfrac{1}{n-1}  \boldsymbol{w}^T X^T X \boldsymbol{w}.
\end{align*}
$$&lt;p&gt;
Find the vector $\boldsymbol{w}$ so that variance is maximal.&lt;/p&gt;
&lt;p&gt;Note that as ${A = X^T X}$ is symmetric, i.e. ${A^T = A}$, then all eigenvalues are real and there exists a orthonormal basis given by the eigenvectors, $\boldsymbol{q_i}$ of $A$. Let ${Q = \left( \boldsymbol{q_1} \, \boldsymbol{q_2} \cdots \boldsymbol{q_n} \right)}$, with ${Q^T = Q^{-1}}$ and ${D = \text{diag}\, {D} }$. Then $A$ can be expressed using the eigenvalues and eigenvectors, thus&lt;/p&gt;
$$
X^T X = A = Q D Q^T .
$$$$
\begin{align*}
\boldsymbol{w}^T X^T X \boldsymbol{w} &amp; = \boldsymbol{w}^T A \boldsymbol{w} \\
&amp; = \boldsymbol{w}^T Q D Q^T \boldsymbol{w} \\
&amp; = \left( \boldsymbol{w}^T Q \right) D \left( Q^T \boldsymbol{w} \right)  \\
&amp; = \left( Q^T \boldsymbol{w} \right)^T D \left( Q^T \boldsymbol{w} \right), \quad \text{let} \quad \boldsymbol{y} = Q^T \boldsymbol{w} \\
&amp; = \boldsymbol{y}^T D \boldsymbol{y}.
\end{align*}
$$&lt;p&gt;
Since $\boldsymbol{w}$ is a unit vector and $Q$ is orthogonal, so $\boldsymbol{y}$ is also a unit vector. It can easily be shown that the vector ${\boldsymbol{y} = \left(1, 0 \ldots 0 \right)^T}$ maximizes the variance. Thus, the corresponding principal component $\boldsymbol{w}$ is recovered from ${\boldsymbol{y} = Q^T \boldsymbol{w}}$, i.e. ${\boldsymbol{w} =  \left( Q^T\right)^{-1} \boldsymbol{y} = Q\boldsymbol{y}}$.&lt;/p&gt;
&lt;div class=&#34;details admonition Example open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M15.75 15.75V18m-7.5-6.75h.008v.008H8.25zm0 2.25h.008v.008H8.25zm0 2.25h.008v.008H8.25zm0 2.25h.008v.008H8.25zm2.498-6.75h.007v.008h-.007zm0 2.25h.007v.008h-.007zm0 2.25h.007v.008h-.007zm0 2.25h.007v.008h-.007zm2.504-6.75h.008v.008h-.008zm0 2.25h.008v.008h-.008zm0 2.25h.008v.008h-.008zm0 2.25h.008v.008h-.008zm2.498-6.75h.008v.008h-.008zm0 2.25h.008v.008h-.008zM8.25 6h7.5v2.25h-7.5zM12 2.25c-1.892 0-3.758.11-5.593.322C5.307 2.7 4.5 3.65 4.5 4.757V19.5a2.25 2.25 0 0 0 2.25 2.25h10.5a2.25 2.25 0 0 0 2.25-2.25V4.757c0-1.108-.806-2.057-1.907-2.185A48.507 48.507 0 0 0 12 2.25&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Example: &lt;em&gt;&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            &lt;p&gt;An &lt;span class=&#34;mycode&#34;&gt;.ipynb&lt;/span&gt; notebook with an example of principal component analysis can be accessed online &lt;a href=&#34;https://djps.github.io/docs/gradcalclinalg24/notebooks/pca/&#34;&gt;[here]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/pca.py/&#34;&gt;[here]&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href=&#34;https://djps.github.io/ipyth/pca.ipynb&#34;&gt;[here]&lt;/a&gt;.&lt;/p&gt;

        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




</description>
    </item>
    
    <item>
      <title>Calculus of a Single Variable</title>
      <link>https://djps.github.io/docs/gradcalclinalg24/part2/one-variable/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/docs/gradcalclinalg24/part2/one-variable/</guid>
      <description>&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Derivative of a function&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
f^{\prime}\left( x \right) = \lim_{h \rightarrow 0} \dfrac{f\left( x + h \right) - f\left(x\right)}{h}.
$$
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;p&gt;Note the following&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$f\left(x\right) = c \quad \Rightarrow f^{\prime}\left( x \right)  = 0$&lt;/li&gt;
&lt;li&gt;$f\left(x\right) = x^a \quad \Rightarrow f^{\prime}\left( x \right)  = a x^{a-1}$&lt;/li&gt;
&lt;li&gt;$f\left(x\right) = a^x \quad \Rightarrow f^{\prime}\left( x \right)  = a^x \ln a$&lt;/li&gt;
&lt;li&gt;$f\left(x\right) = \log_{b}x \quad \Rightarrow f^{\prime}\left( x \right)  = \dfrac{1}{x \log_e b}$&lt;/li&gt;
&lt;li&gt;$f\left(x\right) = \sin\left(x\right) \quad \Rightarrow f^{\prime}\left( x \right)  = \cos\left(x\right)$&lt;/li&gt;
&lt;li&gt;$f\left(x\right) = \cos\left(x\right) \quad \Rightarrow f^{\prime}\left( x \right)  = -\sin\left(x\right)$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thus for $f(x)=a^x$, when $a=e$ then, $f=e^x$ and $f^{\prime}\left( x \right) = f\left( x \right) = e^x$.&lt;/p&gt;
&lt;p&gt;Similarly, for $f\left(x\right) = \log_{b}x$ when $b=e$, i.e. $f\left(x\right) = \log_e x = \ln x$, so $f^{\prime}\left( x \right) = \dfrac{1}{x}$.&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Summation Rule&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
\dfrac{\mathrm{d}\left(h+g\right)}{\mathrm{d} x} = h^{\prime}\left( x \right) + g^{\prime}\left( x \right).
$$
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Product Rule&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
\dfrac{\mathrm{d}\left(hg\right)}{\mathrm{d} x} = h^{\prime}\left( x \right)g\left(x\right) + h\left(x\right)g^{\prime}\left( x \right).
$$
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Quotient Rule&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
\dfrac{\mathrm{d}f\left(x\right)}{\mathrm{d} x} = \dfrac{h\left(x\right)g^{\prime}\left( x \right) - h^{\prime}\left( x \right)g\left(x\right)}{ \left( h^\prime \left(x\right) \right)^2 }.
$$
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Chain Rule&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
\dfrac{\mathrm{d}f\left(x\right)}{\mathrm{d} x} = g^{\prime}\left( h \right) h^{\prime}\left(x\right).
$$$$
\dfrac{\mathrm{d}f\left(x\right)}{\mathrm{d} x} = \dfrac{\mathrm{d}g}{\mathrm{d} h} \dfrac{\mathrm{d}h}{\mathrm{d}x}.
$$&lt;p&gt;
This form can be understood as stating that if a function $f$ is written in terms of $g$, which itself depends on the variable $x$ (that is both $f$ and $g$ are dependent variables), then $f$ depends on $x$ as well, via the intermediate variable $g$.&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;p&gt;For example, the function could be $\sin\left( x^2 \right)$, then write the function as $h(x) = f(g(x))$, where $f(x)=\sin(y(x))$ and $y(x)=x^2$, then the derivative is $f^{\prime} = 2x \cos\left( x^2\right)$.&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Critical Point of a Function&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            If $f^{\prime} \left( x_0 \right) = 0$ for some $x_0$, then this point is called a critical point of $f$.
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;p&gt;Critical points are candidates for being local maxima or minima for the function.&lt;/p&gt;
&lt;h2 id=&#34;global-maximum&#34;&gt;Global Maximum&lt;/h2&gt;
&lt;p&gt;If $f(x)$ is a continuous function of a closed, bounded interval, then it always attains a global maximum and global minimum on that interval.&lt;/p&gt;
&lt;h2 id=&#34;linear-approximations&#34;&gt;Linear Approximations&lt;/h2&gt;
$$
y = f\left(x_0 \right) + f^{\prime} \left(x_0 \right) \left( x - x_0 \right).
$$&lt;p&gt;
This is of the form  $y = mx + b$ where the gradient is $m=\left(x_0 \right)$ and the intercept is given by $b= f\left(x_0 \right) -  x_0 f^{\prime} \left(x_0 \right)$.&lt;/p&gt;
&lt;h2 id=&#34;second-derivatives&#34;&gt;Second Derivatives&lt;/h2&gt;
&lt;p&gt;Assuming $x_0$ is a critical point of $f(x)$ then $f^{\prime}\left( x_0 \right)=0$, so the Taylor expansion about $x_0$ is&lt;/p&gt;
$$
f\left(x_0 + x \right) \approx f\left(x_0\right) + \dfrac{1}{2!} f^{\prime\prime}\left(x_0 \right) x^2 + \ldots
$$&lt;p&gt;Then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $f^{\prime\prime}\left(x_0 \right) &lt; 0$, then $x_0$ is a local maxima&lt;/li&gt;
&lt;li&gt;If $f^{\prime\prime}\left(x_0 \right) &gt; 0$, then $x_0$ is a local minima&lt;/li&gt;
&lt;li&gt;If $f^{\prime\prime}\left(x_0 \right) = 0$, then the test fails.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Taylor Series</title>
      <link>https://djps.github.io/docs/gradcalclinalg24/part2/taylor/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/docs/gradcalclinalg24/part2/taylor/</guid>
      <description>&lt;p&gt;The Taylor series, or Taylor expansion of a function, is defined as&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Taylor Series&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
\begin{equation*}
\sum\limits_{k=0}^{\infty} \dfrac{ f^{(k)} \left( c \right) }{k!} \left( x - c \right)^{k}.
\end{equation*}
$$
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;p&gt;This is a infinite series of powers of the variable $x$, which is convergent for some values of $x$ such that $\left| x - c \right| &lt; r $ where $r$ is the radius of convergence.&lt;/p&gt;
&lt;div class=&#34;details admonition Theorem open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M8.25 6.75h12M8.25 12h12m-12 5.25h12M3.75 6.75h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0M3.75 12h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0m-.375 5.25h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Theorem: &lt;em&gt;Taylor&amp;#39;s Theorem&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
\begin{equation*}
f\left( x \right) = \sum\limits_{k=0}^{n} \dfrac{f^{(k)} \left(c\right) }{k!} \left( x- c \right)^{k} + \dfrac{f^{(n+1)} \left( \xi \right) }{\left( n + 1 \right)!} \left( x - c \right)^{n+1}
\end{equation*}
$$$$
\begin{equation*}
\lim\limits_{\xi \rightarrow c} \dfrac{ f^{(n+1)} \left( \xi \right) }{ \left( n + 1 \right)!} \left( x - c \right)^{n+1} = 0.
\end{equation*}
$$
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Theorem open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M8.25 6.75h12M8.25 12h12m-12 5.25h12M3.75 6.75h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0M3.75 12h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0m-.375 5.25h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Theorem: &lt;em&gt;Taylor&amp;#39;s Theorem for Multivariate Functions&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
\begin{equation*}
f\left( \boldsymbol{x} + \boldsymbol{a} \right) = f\left( \boldsymbol{x} \right)  + \boldsymbol{a}\cdot J + \dfrac{1}{2} \boldsymbol{a}^T H \boldsymbol{a} + \ldots
\end{equation*}
$$&lt;p&gt;
where $J$ is the Jacobian and $H$ is the Hessian.&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Example open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M15.75 15.75V18m-7.5-6.75h.008v.008H8.25zm0 2.25h.008v.008H8.25zm0 2.25h.008v.008H8.25zm0 2.25h.008v.008H8.25zm2.498-6.75h.007v.008h-.007zm0 2.25h.007v.008h-.007zm0 2.25h.007v.008h-.007zm0 2.25h.007v.008h-.007zm2.504-6.75h.008v.008h-.008zm0 2.25h.008v.008h-.008zm0 2.25h.008v.008h-.008zm0 2.25h.008v.008h-.008zm2.498-6.75h.008v.008h-.008zm0 2.25h.008v.008h-.008zM8.25 6h7.5v2.25h-7.5zM12 2.25c-1.892 0-3.758.11-5.593.322C5.307 2.7 4.5 3.65 4.5 4.757V19.5a2.25 2.25 0 0 0 2.25 2.25h10.5a2.25 2.25 0 0 0 2.25-2.25V4.757c0-1.108-.806-2.057-1.907-2.185A48.507 48.507 0 0 0 12 2.25&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Example: &lt;em&gt;&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            &lt;p&gt;An &lt;span class=&#34;mycode&#34;&gt;.ipynb&lt;/span&gt; notebook with an example of the Taylor series for $\sin\left(x\right)$ can be accessed online &lt;a href=&#34;https://djps.github.io/docs/gradcalclinalg24/notebooks/taylor_series/&#34;&gt;[here]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It can be downloaded from &lt;a href=&#34;https://djps.github.io/ipyth/taylor_series.py/&#34;&gt;[here]&lt;/a&gt; as a python file or downloaded as a notebook from &lt;a href=&#34;https://djps.github.io/ipyth/taylor_series.ipynb&#34;&gt;[here]&lt;/a&gt;.&lt;/p&gt;

        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Theorem open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M8.25 6.75h12M8.25 12h12m-12 5.25h12M3.75 6.75h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0M3.75 12h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0m-.375 5.25h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Theorem: &lt;em&gt;Rolle&amp;#39;s Theorem&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
f^\prime (c) = 0.
$$
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Theorem open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M8.25 6.75h12M8.25 12h12m-12 5.25h12M3.75 6.75h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0M3.75 12h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0m-.375 5.25h.007v.008H3.75zm.375 0a.375.375 0 1 1-.75 0a.375.375 0 0 1 .75 0&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Theorem: &lt;em&gt;Mean Value Theorem&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
f^\prime (c) = \dfrac{f(b) - f(a)}{b - a}.
$$
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;!--
Note that differentiating a vector function which is a matrix vector product means $D(x^TAx) = D(A)x + A D(x)


--&gt;
</description>
    </item>
    
    <item>
      <title>Calculus of a Function of Several Variables</title>
      <link>https://djps.github.io/docs/gradcalclinalg24/part2/many-variables/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://djps.github.io/docs/gradcalclinalg24/part2/many-variables/</guid>
      <description>&lt;!-- Consider functions which take multiple input arguments, and can either output a scalar value or a vector, i.e. $f \, : \, \mathbb{R}^n \mapsto \mathbb{R^m}$  --&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Partial Derivatives&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
\dfrac{\partial f}{\partial x} = \lim_{h \rightarrow 0} \dfrac{f\left(x+h,y\right) - f\left(x,y\right) }{h}
$$$$
\dfrac{\partial f}{\partial y} = \lim_{h \rightarrow 0} \dfrac{f\left(x,y+h\right) - f\left(x,y\right) }{h}.
$$&lt;p&gt;
Note that the partial derivative is often denoted as $\dfrac{\partial f}{\partial x} = f_x$.&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Gradient of a Function&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
\nabla f = \left( \dfrac{\partial f}{\partial x_1}, \dfrac{\partial f}{\partial x_2}, \ldots, \dfrac{\partial f}{\partial x_n} \right).
$$&lt;p&gt;
As the gradient $\nabla f$ depends on the point at which it is evaluated, it is denoted by $\nabla f \left(x_1, x_2, \ldots, x_n \right)$.&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;p&gt;The gradient is the analogue to the derivative, but, as a vector, has a direction.&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Critical Point of a Function&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            &lt;p&gt;If $f\left(x, y\right)$ has a local minima or maxima at $\left( x_0, y_0 \right)$, then $\nabla f\left( x_0, y_0 \right) = \vec{0}$.&lt;/p&gt;
&lt;p&gt;Such points are called &lt;strong&gt;critical points&lt;/strong&gt;.&lt;/p&gt;

        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;p&gt;Note that not all critical points are either maxima or minima. The classification of the critical points high-order derivatives.&lt;/p&gt;
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Second Derivatives&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            $$
\dfrac{\partial^2 f}{\partial x_i \partial x_j} = \lim_{h \rightarrow 0} \dfrac{f_{x_i} \left(x_1, \ldots,  x_j + h, \ldots, x_n \right) - f_{x_i}\left( x_1, \ldots,  x_n \right)}{h}.
$$$$
H\left( \vec{x} \right) = \left(
\begin{array}{cccc}
f_{x_1 x_1} &amp; f_{x_1 x_2} &amp; \cdots &amp; f_{x_1 x_n} \\
f_{x_1 x_1} &amp; f_{x_2 x_2} &amp; \cdots &amp; f_{x_2 x_n} \\
\vdots      &amp; \vdots  &amp; \ddots &amp;  \vdots \\
f_{x_n x_1} &amp; f_{x_n x_2} &amp; \cdots &amp; f_{x_1 x_n}
\end{array}
\right).
$$
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;




&lt;p&gt;If the second-order partial derivatives are continuous, then the Hessian matrix $H$ is symmetric. Then, if symmetric all eigenvalues are real-valued.&lt;/p&gt;
$$
f\left(\vec{x} + \vec{a} \right) = f\left( \vec{a} \right)  + \dfrac{1}{2} \vec{x}^T H\left( \vec{a} \right) \vec{x} + \ldots
$$$$
H = Q D Q^{-1}
$$$$
f\left(\vec{x} + \vec{a} \right) = f\left( \vec{a} \right)  + \dfrac{1}{2} \vec{x}^T  Q D Q^{T} \vec{x} + \ldots
$$$$
\begin{align*}
f\left(\vec{x} + \vec{a} \right) &amp;  = f\left( \vec{a} \right)  + \dfrac{1}{2} \vec{y}^T D \vec{y} + \ldots \\
 &amp;  = f\left( \vec{a} \right)  + \dfrac{1}{2} \left( \lambda_1^{} y_1^2 + \ldots \lambda_n^{} y_n^2 \right) + \ldots
\end{align*}
$$&lt;p&gt;
So:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If all $\lambda_1$, $\lambda_2, \ldots, \lambda_n &gt; 0$, then the critical point is a &lt;em&gt;local minima&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;If all $\lambda_1$, $\lambda_2, \ldots, \lambda_n &lt; 0$, then the critical point is a &lt;em&gt;local maxima&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;If some $\lambda_j &lt; 0$, and some $\lambda_i &gt; 0$, then the critical point is neither a local minima nor maxima. If none of the eigenvalues are equal to zero, then the critical point is called a &lt;em&gt;saddle point&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;If all $\lambda_1$, $\lambda_2, \ldots, \lambda_n \geq 0$ and at least is zero, or $\lambda_1$, $\lambda_2, \ldots \lambda_n, \leq 0$ and at least one is zero then the test is inconclusive, as the classification of the point depends on higher derivatives.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;jacobians&#34;&gt;Jacobians&lt;/h2&gt;
&lt;p&gt;If the function outputs a vector, i.e. $\vec{f} \, : \, \mathbb{R}^n \mapsto \mathbb{R}^{m}$, write each component of $\vec{f}=\left( f_1, \ldots, f_m \right)$ and the same procedures can be performed on each component of the vector-valued function. Thus, a gradient can be computed for each component $\nabla f_i$, critical points must satisfy the vector equation $\vec{f}\left(\vec{x}^{*} \right) = \vec{0}$.&lt;/p&gt;
$$
\nabla F = \nabla \left( f (g(x)), f(h(x)) \right) = \nabla f \dfrac{\partial (g,h)}{\partial x}
$$$$
\nabla f \dfrac{\partial (g,h)}{\partial x} = \left(
\begin{array}{c}
\nabla g \\
\nabla h
\end{array} \right)
$$&lt;p&gt;
is the &lt;strong&gt;Jacobi matrix&lt;/strong&gt;. Each row of the Jacobi matrix is a gradient of $g$ or $h$.&lt;/p&gt;
$$
\nabla_{\boldsymbol{w}} f \left(\boldsymbol{x}\right) = \lim_{h \rightarrow 0} \dfrac{f(\boldsymbol{x} + h\boldsymbol{v}) - f(\boldsymbol{x})}{h}
$$$$
\nabla_{\boldsymbol{w}} f \left(\boldsymbol{x}\right) = \nabla f \left(\boldsymbol{x}\right) \cdot  \boldsymbol{w}
$$&lt;p&gt;
As, by convention, $\nabla f \left(\boldsymbol{x}\right)$ is a column vector.&lt;/p&gt;
&lt;p&gt;The directional derivative can then be expressed as a matrix-vector product, specifically a Jacobian-vector product.&lt;/p&gt;
&lt;p&gt;By the Cauchy-Schwarz inequality, the largest value of the directional derivative is when $\nabla f$ and $\boldsymbol{w}$ are pointing in the same direction.&lt;/p&gt;
&lt;!--
&lt;div class=&#34;details admonition Definition open&#34;&gt;
    &lt;div class=&#34;details-summary admonition-title&#34;&gt;
        
        &lt;span class=&#34;flex pr-3 pt-1&#34;&gt;
          &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
        &lt;/span&gt;
        &lt;span class=&#34;dark:text-neutral-300&#34;&gt;
          Definition: &lt;em&gt;Chain Rule&lt;/em&gt;
        &lt;/span&gt;
    &lt;/div&gt;
    &lt;div class=&#34;details-content&#34;&gt;
        &lt;div class=&#34;admonition-content&#34;&gt;
            The chain rule enables the derivative of a function which can be expressed as a composition of two differentiable functions.
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;





Jacobi Matrix

Global Maximina

Newton Methods
--&gt;
</description>
    </item>
    
  </channel>
</rss>
